24-04-02 01:04:17.756 - INFO: Namespace(experiment='mlicpp_mse_0250', dataset='/mnt/bn/jiangwei-lvc3/dataset/image', epochs=500, learning_rate=0.0001, num_workers=8, lmbda=0.025, metrics='mse', batch_size=4, test_batch_size=1, aux_learning_rate=0.001, patch_size=[384, 384], gpu_id=0, cuda=True, save=True, seed=1984.0, clip_max_norm=1.0, checkpoint='/mnt/bn/jiangwei-lvc3/work_space/MLICPlusPlus/playground/experiments/mlicpp_mse_0250/checkpoints', world_size=4, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
24-04-02 01:04:17.757 - INFO: {'N': 192, 'M': 320, 'enc_dims': [3, 192, 192, 192, 320], 'dec_dims': [320, 192, 192, 192, 16, 3], 'slice_num': 10, 'context_window': 5, 'slice_ch': [8, 8, 8, 8, 16, 16, 32, 32, 96, 96], 'max_support_slices': 5, 'quant': 'ste', 'lambda_list': [0.07, 0.08, 0.09], 'use_hyper_gain': False, 'interpolated_type': 'exponential', 'act': <class 'torch.nn.modules.activation.GELU'>, 'L': 10, 'target_bpp': [0.0761, 0.1854, 0.2752, 0.3652, 0.4282, 0.5238, 0.5653, 0.6334, 0.745], 'bpp_threshold': [0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.02, 0.02], 'min_lmbda': 0.001, 'init_lmbda': [0.001, 0.0018, 0.0035, 0.0035, 0.0067, 0.0067, 0.013, 0.013, 0.025, 0.0483], 'lower_bound': 1e-09, 'ki': 0.1, 'kp': 0.1}
24-04-02 01:04:17.757 - INFO: DistributedDataParallel(
  (module): MLICPlusPlus(
    (entropy_bottleneck): EntropyBottleneck(
      (likelihood_lower_bound): LowerBound()
    )
    (g_a): AnalysisTransform(
      (analysis_transform): Sequential(
        (0): ResidualBlockWithStride(
          (conv1): Conv2d(3, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (gdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (skip): Conv2d(3, 192, kernel_size=(1, 1), stride=(2, 2))
        )
        (1): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (2): ResidualBlockWithStride(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (gdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (skip): Conv2d(192, 192, kernel_size=(1, 1), stride=(2, 2))
        )
        (3): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (4): ResidualBlockWithStride(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (gdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (skip): Conv2d(192, 192, kernel_size=(1, 1), stride=(2, 2))
        )
        (5): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (6): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (g_s): SynthesisTransform(
      (synthesis_transform): Sequential(
        (0): ResidualBlock(
          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (1): ResidualBlockUpsample(
          (subpel_conv): Sequential(
            (0): Conv2d(320, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (act): GELU(approximate='none')
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (igdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (upsample): Sequential(
            (0): Conv2d(320, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
        )
        (2): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (3): ResidualBlockUpsample(
          (subpel_conv): Sequential(
            (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (act): GELU(approximate='none')
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (igdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (upsample): Sequential(
            (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
        )
        (4): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (5): ResidualBlockUpsample(
          (subpel_conv): Sequential(
            (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
          (act): GELU(approximate='none')
          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (igdn): GDN(
            (beta_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
            (gamma_reparam): NonNegativeParametrizer(
              (lower_bound): LowerBound()
            )
          )
          (upsample): Sequential(
            (0): Conv2d(192, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): PixelShuffle(upscale_factor=2)
          )
        )
        (6): ResidualBlock(
          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (act): GELU(approximate='none')
          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (7): Sequential(
          (0): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): PixelShuffle(upscale_factor=2)
        )
      )
    )
    (h_a): HyperAnalysis(
      (reduction): Sequential(
        (0): Conv2d(320, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): GELU(approximate='none')
        (4): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (5): GELU(approximate='none')
        (6): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): GELU(approximate='none')
        (8): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
    (h_s): HyperSynthesis(
      (increase): Sequential(
        (0): Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): GELU(approximate='none')
        (2): Sequential(
          (0): Conv2d(320, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): PixelShuffle(upscale_factor=2)
        )
        (3): GELU(approximate='none')
        (4): Conv2d(320, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): GELU(approximate='none')
        (6): Sequential(
          (0): Conv2d(480, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): PixelShuffle(upscale_factor=2)
        )
        (7): GELU(approximate='none')
        (8): Conv2d(480, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (gaussian_conditional): GaussianConditional(
      (likelihood_lower_bound): LowerBound()
      (lower_bound_scale): LowerBound()
    )
    (local_context): ModuleList(
      (0-9): 10 x LocalContext(
        (qkv_proj): Linear(in_features=32, out_features=96, bias=True)
        (unfold): Unfold(kernel_size=5, dilation=1, padding=2, stride=1)
        (softmax): Softmax(dim=-1)
        (proj): Linear(in_features=64, out_features=64, bias=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=128, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=128, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (fusion): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
      )
    )
    (channel_context): ModuleList(
      (0): None
      (1): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(32, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (4): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (5): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (6): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (7): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(224, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (8): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (9): ChannelContext(
        (fushion): Sequential(
          (0): Conv2d(288, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (global_inter_context): ModuleList(
      (0): None
      (1): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (queries): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (values): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (reprojection): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        )
        (queries): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        )
        (values): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        )
        (reprojection): Conv2d(64, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (3): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        )
        (queries): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        )
        (values): Sequential(
          (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        )
        (reprojection): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (4): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        )
        (queries): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        )
        (values): Sequential(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        )
        (reprojection): Conv2d(128, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (5): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)
        )
        (queries): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)
        )
        (values): Sequential(
          (0): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)
        )
        (reprojection): Conv2d(160, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (6): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
        (queries): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
        (values): Sequential(
          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        )
        (reprojection): Conv2d(192, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (7): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
        )
        (queries): Sequential(
          (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
        )
        (values): Sequential(
          (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
        )
        (reprojection): Conv2d(224, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (8): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        )
        (queries): Sequential(
          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        )
        (values): Sequential(
          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
        )
        (reprojection): Conv2d(256, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (9): LinearGlobalInterContext(
        (keys): Sequential(
          (0): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
        )
        (queries): Sequential(
          (0): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
        )
        (values): Sequential(
          (0): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
        )
        (reprojection): Conv2d(288, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
        (skip): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (global_intra_context): ModuleList(
      (0): None
      (1-9): 9 x LinearGlobalIntraContext(
        (keys): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (queries): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (values): Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        )
        (reprojection): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (mlp): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (3): GELU(approximate='none')
          (4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (entropy_parameters_anchor): ModuleList(
      (0): EntropyParameters(
        (fusion): Sequential(
          (0): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (5): GELU(approximate='none')
          (6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1-9): 9 x EntropyParameters(
        (fusion): Sequential(
          (0): Conv2d(832, 320, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (5): GELU(approximate='none')
          (6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (entropy_parameters_nonanchor): ModuleList(
      (0): EntropyParameters(
        (fusion): Sequential(
          (0): Conv2d(704, 320, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (5): GELU(approximate='none')
          (6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (1-9): 9 x EntropyParameters(
        (fusion): Sequential(
          (0): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          (5): GELU(approximate='none')
          (6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (lrp_anchor): ModuleList(
      (0): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(352, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (1): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(384, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(416, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (4): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(480, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (5): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(512, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (6): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(544, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (7): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(576, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (8): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(608, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (9): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(640, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (lrp_nonanchor): ModuleList(
      (0): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(352, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (1): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(384, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(416, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (4): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(480, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (5): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(512, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (6): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(544, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (7): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(576, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (8): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(608, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (9): LatentResidualPrediction(
        (lrp_transform): Sequential(
          (0): Conv2d(640, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(224, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): GELU(approximate='none')
          (4): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
  )
)
24-04-02 01:04:17.766 - INFO: Learning rate: 0.0001
24-04-02 01:39:32.718 - INFO: Learning rate: 0.0001
24-04-02 02:14:16.563 - INFO: Learning rate: 0.0001
24-04-02 02:49:14.659 - INFO: Learning rate: 0.0001
24-04-02 03:23:50.685 - INFO: Learning rate: 0.0001
24-04-02 03:58:16.368 - INFO: Learning rate: 0.0001
24-04-02 04:32:50.393 - INFO: Learning rate: 0.0001
24-04-02 05:07:30.307 - INFO: Learning rate: 0.0001
24-04-02 05:42:27.554 - INFO: Learning rate: 0.0001
24-04-02 06:17:03.671 - INFO: Learning rate: 0.0001
24-04-02 06:51:58.609 - INFO: Learning rate: 0.0001
24-04-02 07:27:03.531 - INFO: Learning rate: 0.0001
24-04-02 08:01:38.479 - INFO: Learning rate: 0.0001
24-04-02 08:36:10.176 - INFO: Learning rate: 0.0001
24-04-02 09:10:51.813 - INFO: Learning rate: 0.0001
24-04-02 09:45:10.490 - INFO: Learning rate: 0.0001
24-04-02 10:19:39.883 - INFO: Learning rate: 0.0001
24-04-02 10:54:12.086 - INFO: Learning rate: 0.0001
24-04-02 11:28:54.988 - INFO: Learning rate: 0.0001
.4278 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 91.85
24-04-02 01:07:58.515 - INFO: Train epoch 347: [ 9600/94637 (10%)] Step: [2048101] | Lr: 0.000100 | Loss: 0.9383 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 87.75
24-04-02 01:08:34.099 - INFO: Train epoch 347: [11200/94637 (12%)] Step: [2048201] | Lr: 0.000100 | Loss: 1.1756 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 86.94
24-04-02 01:09:10.415 - INFO: Train epoch 347: [12800/94637 (14%)] Step: [2048301] | Lr: 0.000100 | Loss: 0.9687 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 82.21
24-04-02 01:09:45.849 - INFO: Train epoch 347: [14400/94637 (15%)] Step: [2048401] | Lr: 0.000100 | Loss: 0.9808 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 92.19
24-04-02 01:10:22.567 - INFO: Train epoch 347: [16000/94637 (17%)] Step: [2048501] | Lr: 0.000100 | Loss: 1.6102 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 92.11
24-04-02 01:10:57.645 - INFO: Train epoch 347: [17600/94637 (19%)] Step: [2048601] | Lr: 0.000100 | Loss: 1.2918 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 86.65
24-04-02 01:11:32.805 - INFO: Train epoch 347: [19200/94637 (20%)] Step: [2048701] | Lr: 0.000100 | Loss: 0.8352 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 86.89
24-04-02 01:12:07.611 - INFO: Train epoch 347: [20800/94637 (22%)] Step: [2048801] | Lr: 0.000100 | Loss: 1.8027 | MSE loss: 0.0004 | Bpp loss: 1.11 | Aux loss: 87.26
24-04-02 01:12:43.395 - INFO: Train epoch 347: [22400/94637 (24%)] Step: [2048901] | Lr: 0.000100 | Loss: 1.5983 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 90.35
24-04-02 01:13:18.868 - INFO: Train epoch 347: [24000/94637 (25%)] Step: [2049001] | Lr: 0.000100 | Loss: 1.5221 | MSE loss: 0.0004 | Bpp loss: 0.95 | Aux loss: 89.68
24-04-02 01:13:54.225 - INFO: Train epoch 347: [25600/94637 (27%)] Step: [2049101] | Lr: 0.000100 | Loss: 1.2472 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 84.01
24-04-02 01:14:29.967 - INFO: Train epoch 347: [27200/94637 (29%)] Step: [2049201] | Lr: 0.000100 | Loss: 1.2800 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 94.86
24-04-02 01:15:05.866 - INFO: Train epoch 347: [28800/94637 (30%)] Step: [2049301] | Lr: 0.000100 | Loss: 0.8982 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 97.88
24-04-02 01:15:42.911 - INFO: Train epoch 347: [30400/94637 (32%)] Step: [2049401] | Lr: 0.000100 | Loss: 1.0638 | MSE loss: 0.0003 | Bpp loss: 0.64 | Aux loss: 81.27
24-04-02 01:16:19.170 - INFO: Train epoch 347: [32000/94637 (34%)] Step: [2049501] | Lr: 0.000100 | Loss: 0.7508 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 81.50
24-04-02 01:16:55.375 - INFO: Train epoch 347: [33600/94637 (36%)] Step: [2049601] | Lr: 0.000100 | Loss: 1.3129 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 87.79
24-04-02 01:17:31.181 - INFO: Train epoch 347: [35200/94637 (37%)] Step: [2049701] | Lr: 0.000100 | Loss: 1.0838 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 83.81
24-04-02 01:18:06.527 - INFO: Train epoch 347: [36800/94637 (39%)] Step: [2049801] | Lr: 0.000100 | Loss: 0.6260 | MSE loss: 0.0001 | Bpp loss: 0.42 | Aux loss: 92.17
24-04-02 01:18:42.689 - INFO: Train epoch 347: [38400/94637 (41%)] Step: [2049901] | Lr: 0.000100 | Loss: 1.4398 | MSE loss: 0.0004 | Bpp loss: 0.86 | Aux loss: 87.07
24-04-02 01:19:19.951 - INFO: Train epoch 347: [40000/94637 (42%)] Step: [2050001] | Lr: 0.000100 | Loss: 1.2303 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 89.40
24-04-02 01:19:56.457 - INFO: Train epoch 347: [41600/94637 (44%)] Step: [2050101] | Lr: 0.000100 | Loss: 1.3885 | MSE loss: 0.0004 | Bpp loss: 0.81 | Aux loss: 93.84
24-04-02 01:20:32.647 - INFO: Train epoch 347: [43200/94637 (46%)] Step: [2050201] | Lr: 0.000100 | Loss: 1.3030 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 86.93
24-04-02 01:21:08.438 - INFO: Train epoch 347: [44800/94637 (47%)] Step: [2050301] | Lr: 0.000100 | Loss: 1.2562 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 85.06
24-04-02 01:21:44.492 - INFO: Train epoch 347: [46400/94637 (49%)] Step: [2050401] | Lr: 0.000100 | Loss: 1.6103 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 86.97
24-04-02 01:22:21.089 - INFO: Train epoch 347: [48000/94637 (51%)] Step: [2050501] | Lr: 0.000100 | Loss: 1.3385 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 92.74
24-04-02 01:22:57.116 - INFO: Train epoch 347: [49600/94637 (52%)] Step: [2050601] | Lr: 0.000100 | Loss: 1.2456 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 89.77
24-04-02 01:23:31.556 - INFO: Train epoch 347: [51200/94637 (54%)] Step: [2050701] | Lr: 0.000100 | Loss: 1.5713 | MSE loss: 0.0004 | Bpp loss: 0.95 | Aux loss: 87.68
24-04-02 01:24:06.851 - INFO: Train epoch 347: [52800/94637 (56%)] Step: [2050801] | Lr: 0.000100 | Loss: 1.0529 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 87.68
24-04-02 01:24:41.015 - INFO: Train epoch 347: [54400/94637 (57%)] Step: [2050901] | Lr: 0.000100 | Loss: 0.8059 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 89.17
24-04-02 01:25:17.404 - INFO: Train epoch 347: [56000/94637 (59%)] Step: [2051001] | Lr: 0.000100 | Loss: 0.8697 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 88.43
24-04-02 01:25:53.780 - INFO: Train epoch 347: [57600/94637 (61%)] Step: [2051101] | Lr: 0.000100 | Loss: 0.8617 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 86.57
24-04-02 01:26:30.637 - INFO: Train epoch 347: [59200/94637 (63%)] Step: [2051201] | Lr: 0.000100 | Loss: 1.1196 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 89.82
24-04-02 01:27:05.718 - INFO: Train epoch 347: [60800/94637 (64%)] Step: [2051301] | Lr: 0.000100 | Loss: 1.2350 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 90.80
24-04-02 01:27:42.185 - INFO: Train epoch 347: [62400/94637 (66%)] Step: [2051401] | Lr: 0.000100 | Loss: 1.5157 | MSE loss: 0.0003 | Bpp loss: 0.96 | Aux loss: 91.56
24-04-02 01:28:18.828 - INFO: Train epoch 347: [64000/94637 (68%)] Step: [2051501] | Lr: 0.000100 | Loss: 1.0097 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 82.92
24-04-02 01:28:53.994 - INFO: Train epoch 347: [65600/94637 (69%)] Step: [2051601] | Lr: 0.000100 | Loss: 0.9864 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 107.34
24-04-02 01:29:28.516 - INFO: Train epoch 347: [67200/94637 (71%)] Step: [2051701] | Lr: 0.000100 | Loss: 0.8089 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 90.99
24-04-02 01:30:04.059 - INFO: Train epoch 347: [68800/94637 (73%)] Step: [2051801] | Lr: 0.000100 | Loss: 0.8593 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 87.37
24-04-02 01:30:39.715 - INFO: Train epoch 347: [70400/94637 (74%)] Step: [2051901] | Lr: 0.000100 | Loss: 1.3883 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 86.47
24-04-02 01:31:15.546 - INFO: Train epoch 347: [72000/94637 (76%)] Step: [2052001] | Lr: 0.000100 | Loss: 1.1503 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 81.48
24-04-02 01:31:48.600 - INFO: Train epoch 347: [73600/94637 (78%)] Step: [2052101] | Lr: 0.000100 | Loss: 0.9745 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 87.10
24-04-02 01:32:23.658 - INFO: Train epoch 347: [75200/94637 (79%)] Step: [2052201] | Lr: 0.000100 | Loss: 1.5892 | MSE loss: 0.0004 | Bpp loss: 1.00 | Aux loss: 90.96
24-04-02 01:32:57.626 - INFO: Train epoch 347: [76800/94637 (81%)] Step: [2052301] | Lr: 0.000100 | Loss: 0.7474 | MSE loss: 0.0001 | Bpp loss: 0.51 | Aux loss: 84.57
24-04-02 01:33:31.928 - INFO: Train epoch 347: [78400/94637 (83%)] Step: [2052401] | Lr: 0.000100 | Loss: 1.7655 | MSE loss: 0.0004 | Bpp loss: 1.06 | Aux loss: 89.23
24-04-02 01:34:09.512 - INFO: Train epoch 347: [80000/94637 (85%)] Step: [2052501] | Lr: 0.000100 | Loss: 1.2003 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 87.49
24-04-02 01:34:43.507 - INFO: Train epoch 347: [81600/94637 (86%)] Step: [2052601] | Lr: 0.000100 | Loss: 1.1560 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 87.47
24-04-02 01:35:18.540 - INFO: Train epoch 347: [83200/94637 (88%)] Step: [2052701] | Lr: 0.000100 | Loss: 0.8806 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 84.11
24-04-02 01:35:54.738 - INFO: Train epoch 347: [84800/94637 (90%)] Step: [2052801] | Lr: 0.000100 | Loss: 1.2348 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 82.94
24-04-02 01:36:29.556 - INFO: Train epoch 347: [86400/94637 (91%)] Step: [2052901] | Lr: 0.000100 | Loss: 0.9315 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 88.81
24-04-02 01:37:04.945 - INFO: Train epoch 347: [88000/94637 (93%)] Step: [2053001] | Lr: 0.000100 | Loss: 1.5422 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 85.83
24-04-02 01:37:40.439 - INFO: Train epoch 347: [89600/94637 (95%)] Step: [2053101] | Lr: 0.000100 | Loss: 1.6857 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 88.97
24-04-02 01:38:16.215 - INFO: Train epoch 347: [91200/94637 (96%)] Step: [2053201] | Lr: 0.000100 | Loss: 1.9074 | MSE loss: 0.0005 | Bpp loss: 1.10 | Aux loss: 87.85
24-04-02 01:38:51.913 - INFO: Train epoch 347: [92800/94637 (98%)] Step: [2053301] | Lr: 0.000100 | Loss: 1.5376 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 94.85
24-04-02 01:39:27.311 - INFO: Train epoch 347: [94400/94637 (100%)] Step: [2053401] | Lr: 0.000100 | Loss: 1.2911 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 86.17
24-04-02 01:39:52.006 - INFO: Learning rate: 0.0001
24-04-02 01:39:53.058 - INFO: Train epoch 348: [    0/94637 (0%)] Step: [2053416] | Lr: 0.000100 | Loss: 1.3938 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 88.45
24-04-02 01:40:27.671 - INFO: Train epoch 348: [ 1600/94637 (2%)] Step: [2053516] | Lr: 0.000100 | Loss: 1.1577 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 89.63
24-04-02 01:41:02.531 - INFO: Train epoch 348: [ 3200/94637 (3%)] Step: [2053616] | Lr: 0.000100 | Loss: 0.8820 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 82.18
24-04-02 01:41:37.269 - INFO: Train epoch 348: [ 4800/94637 (5%)] Step: [2053716] | Lr: 0.000100 | Loss: 1.8655 | MSE loss: 0.0005 | Bpp loss: 1.10 | Aux loss: 91.06
24-04-02 01:42:11.549 - INFO: Train epoch 348: [ 6400/94637 (7%)] Step: [2053816] | Lr: 0.000100 | Loss: 1.2218 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 82.66
24-04-02 01:42:45.566 - INFO: Train epoch 348: [ 8000/94637 (8%)] Step: [2053916] | Lr: 0.000100 | Loss: 1.0508 | MSE loss: 0.0002 | Bpp loss: 0.68 | Aux loss: 90.81
24-04-02 01:43:20.268 - INFO: Train epoch 348: [ 9600/94637 (10%)] Step: [2054016] | Lr: 0.000100 | Loss: 1.0662 | MSE loss: 0.0003 | Bpp loss: 0.63 | Aux loss: 87.87
24-04-02 01:43:53.689 - INFO: Train epoch 348: [11200/94637 (12%)] Step: [2054116] | Lr: 0.000100 | Loss: 1.1622 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 88.17
24-04-02 01:44:27.470 - INFO: Train epoch 348: [12800/94637 (14%)] Step: [2054216] | Lr: 0.000100 | Loss: 1.3851 | MSE loss: 0.0004 | Bpp loss: 0.74 | Aux loss: 84.10
24-04-02 01:45:02.048 - INFO: Train epoch 348: [14400/94637 (15%)] Step: [2054316] | Lr: 0.000100 | Loss: 0.4744 | MSE loss: 0.0001 | Bpp loss: 0.33 | Aux loss: 86.78
24-04-02 01:45:37.018 - INFO: Train epoch 348: [16000/94637 (17%)] Step: [2054416] | Lr: 0.000100 | Loss: 1.5565 | MSE loss: 0.0004 | Bpp loss: 0.97 | Aux loss: 91.40
24-04-02 01:46:12.366 - INFO: Train epoch 348: [17600/94637 (19%)] Step: [2054516] | Lr: 0.000100 | Loss: 1.3004 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 82.34
24-04-02 01:46:46.723 - INFO: Train epoch 348: [19200/94637 (20%)] Step: [2054616] | Lr: 0.000100 | Loss: 1.3070 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 85.32
24-04-02 01:47:21.934 - INFO: Train epoch 348: [20800/94637 (22%)] Step: [2054716] | Lr: 0.000100 | Loss: 1.4212 | MSE loss: 0.0004 | Bpp loss: 0.79 | Aux loss: 84.67
24-04-02 01:47:56.904 - INFO: Train epoch 348: [22400/94637 (24%)] Step: [2054816] | Lr: 0.000100 | Loss: 1.4303 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 90.69
24-04-02 01:48:31.890 - INFO: Train epoch 348: [24000/94637 (25%)] Step: [2054916] | Lr: 0.000100 | Loss: 1.6267 | MSE loss: 0.0004 | Bpp loss: 0.93 | Aux loss: 90.67
24-04-02 01:49:08.452 - INFO: Train epoch 348: [25600/94637 (27%)] Step: [2055016] | Lr: 0.000100 | Loss: 1.3546 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 84.42
24-04-02 01:49:42.485 - INFO: Train epoch 348: [27200/94637 (29%)] Step: [2055116] | Lr: 0.000100 | Loss: 1.2813 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 91.58
24-04-02 01:50:17.617 - INFO: Train epoch 348: [28800/94637 (30%)] Step: [2055216] | Lr: 0.000100 | Loss: 0.9594 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 93.16
24-04-02 01:50:52.604 - INFO: Train epoch 348: [30400/94637 (32%)] Step: [2055316] | Lr: 0.000100 | Loss: 1.9360 | MSE loss: 0.0004 | Bpp loss: 1.25 | Aux loss: 87.50
24-04-02 01:51:27.005 - INFO: Train epoch 348: [32000/94637 (34%)] Step: [2055416] | Lr: 0.000100 | Loss: 1.6020 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 88.91
24-04-02 01:52:01.995 - INFO: Train epoch 348: [33600/94637 (36%)] Step: [2055516] | Lr: 0.000100 | Loss: 1.0603 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 86.11
24-04-02 01:52:36.354 - INFO: Train epoch 348: [35200/94637 (37%)] Step: [2055616] | Lr: 0.000100 | Loss: 0.7119 | MSE loss: 0.0002 | Bpp loss: 0.47 | Aux loss: 85.94
24-04-02 01:53:11.736 - INFO: Train epoch 348: [36800/94637 (39%)] Step: [2055716] | Lr: 0.000100 | Loss: 1.2568 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 88.67
24-04-02 01:53:46.395 - INFO: Train epoch 348: [38400/94637 (41%)] Step: [2055816] | Lr: 0.000100 | Loss: 1.0370 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 92.39
24-04-02 01:54:21.815 - INFO: Train epoch 348: [40000/94637 (42%)] Step: [2055916] | Lr: 0.000100 | Loss: 1.2905 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 91.43
24-04-02 01:54:56.572 - INFO: Train epoch 348: [41600/94637 (44%)] Step: [2056016] | Lr: 0.000100 | Loss: 1.4828 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 94.11
24-04-02 01:55:31.928 - INFO: Train epoch 348: [43200/94637 (46%)] Step: [2056116] | Lr: 0.000100 | Loss: 1.3147 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 96.10
24-04-02 01:56:07.273 - INFO: Train epoch 348: [44800/94637 (47%)] Step: [2056216] | Lr: 0.000100 | Loss: 1.4219 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 89.13
24-04-02 01:56:42.346 - INFO: Train epoch 348: [46400/94637 (49%)] Step: [2056316] | Lr: 0.000100 | Loss: 1.4309 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 90.25
24-04-02 01:57:17.163 - INFO: Train epoch 348: [48000/94637 (51%)] Step: [2056416] | Lr: 0.000100 | Loss: 0.7878 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 86.70
24-04-02 01:57:52.601 - INFO: Train epoch 348: [49600/94637 (52%)] Step: [2056516] | Lr: 0.000100 | Loss: 1.6302 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 85.80
24-04-02 01:58:27.525 - INFO: Train epoch 348: [51200/94637 (54%)] Step: [2056616] | Lr: 0.000100 | Loss: 1.3773 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 88.09
24-04-02 01:59:02.157 - INFO: Train epoch 348: [52800/94637 (56%)] Step: [2056716] | Lr: 0.000100 | Loss: 1.2811 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 92.27
24-04-02 01:59:36.789 - INFO: Train epoch 348: [54400/94637 (57%)] Step: [2056816] | Lr: 0.000100 | Loss: 1.1619 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 89.78
24-04-02 02:00:11.323 - INFO: Train epoch 348: [56000/94637 (59%)] Step: [2056916] | Lr: 0.000100 | Loss: 1.3629 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 84.34
24-04-02 02:00:45.898 - INFO: Train epoch 348: [57600/94637 (61%)] Step: [2057016] | Lr: 0.000100 | Loss: 1.9679 | MSE loss: 0.0005 | Bpp loss: 1.14 | Aux loss: 83.82
24-04-02 02:01:20.821 - INFO: Train epoch 348: [59200/94637 (63%)] Step: [2057116] | Lr: 0.000100 | Loss: 1.4333 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 90.86
24-04-02 02:01:55.151 - INFO: Train epoch 348: [60800/94637 (64%)] Step: [2057216] | Lr: 0.000100 | Loss: 1.2877 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 79.64
24-04-02 02:02:30.090 - INFO: Train epoch 348: [62400/94637 (66%)] Step: [2057316] | Lr: 0.000100 | Loss: 1.3286 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 75.09
24-04-02 02:03:04.579 - INFO: Train epoch 348: [64000/94637 (68%)] Step: [2057416] | Lr: 0.000100 | Loss: 0.7666 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 79.62
24-04-02 02:03:41.150 - INFO: Train epoch 348: [65600/94637 (69%)] Step: [2057516] | Lr: 0.000100 | Loss: 1.1588 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 87.40
24-04-02 02:04:16.222 - INFO: Train epoch 348: [67200/94637 (71%)] Step: [2057616] | Lr: 0.000100 | Loss: 1.2393 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 88.45
24-04-02 02:04:51.798 - INFO: Train epoch 348: [68800/94637 (73%)] Step: [2057716] | Lr: 0.000100 | Loss: 1.2686 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 79.29
24-04-02 02:05:27.048 - INFO: Train epoch 348: [70400/94637 (74%)] Step: [2057816] | Lr: 0.000100 | Loss: 0.6683 | MSE loss: 0.0001 | Bpp loss: 0.44 | Aux loss: 91.47
24-04-02 02:06:02.796 - INFO: Train epoch 348: [72000/94637 (76%)] Step: [2057916] | Lr: 0.000100 | Loss: 1.4005 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 83.59
24-04-02 02:06:38.826 - INFO: Train epoch 348: [73600/94637 (78%)] Step: [2058016] | Lr: 0.000100 | Loss: 1.2715 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 84.87
24-04-02 02:07:14.187 - INFO: Train epoch 348: [75200/94637 (79%)] Step: [2058116] | Lr: 0.000100 | Loss: 1.1378 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 79.74
24-04-02 02:07:49.104 - INFO: Train epoch 348: [76800/94637 (81%)] Step: [2058216] | Lr: 0.000100 | Loss: 1.3311 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 89.55
24-04-02 02:08:24.475 - INFO: Train epoch 348: [78400/94637 (83%)] Step: [2058316] | Lr: 0.000100 | Loss: 1.1153 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 80.07
24-04-02 02:08:58.762 - INFO: Train epoch 348: [80000/94637 (85%)] Step: [2058416] | Lr: 0.000100 | Loss: 1.1906 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 85.56
24-04-02 02:09:33.467 - INFO: Train epoch 348: [81600/94637 (86%)] Step: [2058516] | Lr: 0.000100 | Loss: 0.8676 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 84.68
24-04-02 02:10:07.987 - INFO: Train epoch 348: [83200/94637 (88%)] Step: [2058616] | Lr: 0.000100 | Loss: 1.4143 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 81.28
24-04-02 02:10:42.981 - INFO: Train epoch 348: [84800/94637 (90%)] Step: [2058716] | Lr: 0.000100 | Loss: 0.9426 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 87.90
24-04-02 02:11:18.208 - INFO: Train epoch 348: [86400/94637 (91%)] Step: [2058816] | Lr: 0.000100 | Loss: 1.4220 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 87.62
24-04-02 02:11:53.098 - INFO: Train epoch 348: [88000/94637 (93%)] Step: [2058916] | Lr: 0.000100 | Loss: 1.1787 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 92.26
24-04-02 02:12:27.177 - INFO: Train epoch 348: [89600/94637 (95%)] Step: [2059016] | Lr: 0.000100 | Loss: 1.5790 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 83.11
24-04-02 02:13:01.788 - INFO: Train epoch 348: [91200/94637 (96%)] Step: [2059116] | Lr: 0.000100 | Loss: 1.0680 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 81.76
24-04-02 02:13:36.554 - INFO: Train epoch 348: [92800/94637 (98%)] Step: [2059216] | Lr: 0.000100 | Loss: 1.4806 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 87.06
24-04-02 02:14:11.613 - INFO: Train epoch 348: [94400/94637 (100%)] Step: [2059316] | Lr: 0.000100 | Loss: 1.3659 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 80.30
24-04-02 02:14:28.252 - INFO: Learning rate: 0.0001
24-04-02 02:14:29.080 - INFO: Train epoch 349: [    0/94637 (0%)] Step: [2059331] | Lr: 0.000100 | Loss: 1.2201 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 89.14
24-04-02 02:15:03.815 - INFO: Train epoch 349: [ 1600/94637 (2%)] Step: [2059431] | Lr: 0.000100 | Loss: 0.8433 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 88.26
24-04-02 02:15:39.026 - INFO: Train epoch 349: [ 3200/94637 (3%)] Step: [2059531] | Lr: 0.000100 | Loss: 1.3097 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 88.18
24-04-02 02:16:14.970 - INFO: Train epoch 349: [ 4800/94637 (5%)] Step: [2059631] | Lr: 0.000100 | Loss: 1.4915 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 87.65
24-04-02 02:16:50.505 - INFO: Train epoch 349: [ 6400/94637 (7%)] Step: [2059731] | Lr: 0.000100 | Loss: 1.2268 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 88.69
24-04-02 02:17:26.293 - INFO: Train epoch 349: [ 8000/94637 (8%)] Step: [2059831] | Lr: 0.000100 | Loss: 2.4534 | MSE loss: 0.0007 | Bpp loss: 1.39 | Aux loss: 88.10
24-04-02 02:18:01.931 - INFO: Train epoch 349: [ 9600/94637 (10%)] Step: [2059931] | Lr: 0.000100 | Loss: 1.5295 | MSE loss: 0.0004 | Bpp loss: 0.94 | Aux loss: 91.50
24-04-02 02:18:39.300 - INFO: Train epoch 349: [11200/94637 (12%)] Step: [2060031] | Lr: 0.000100 | Loss: 1.2738 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 89.17
24-04-02 02:19:14.960 - INFO: Train epoch 349: [12800/94637 (14%)] Step: [2060131] | Lr: 0.000100 | Loss: 0.7878 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 95.87
24-04-02 02:19:49.424 - INFO: Train epoch 349: [14400/94637 (15%)] Step: [2060231] | Lr: 0.000100 | Loss: 1.5125 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 96.37
24-04-02 02:20:24.560 - INFO: Train epoch 349: [16000/94637 (17%)] Step: [2060331] | Lr: 0.000100 | Loss: 1.1746 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 86.39
24-04-02 02:20:58.955 - INFO: Train epoch 349: [17600/94637 (19%)] Step: [2060431] | Lr: 0.000100 | Loss: 0.9414 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 86.55
24-04-02 02:21:34.675 - INFO: Train epoch 349: [19200/94637 (20%)] Step: [2060531] | Lr: 0.000100 | Loss: 1.1045 | MSE loss: 0.0003 | Bpp loss: 0.64 | Aux loss: 86.37
24-04-02 02:22:10.537 - INFO: Train epoch 349: [20800/94637 (22%)] Step: [2060631] | Lr: 0.000100 | Loss: 1.1265 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 87.11
24-04-02 02:22:46.058 - INFO: Train epoch 349: [22400/94637 (24%)] Step: [2060731] | Lr: 0.000100 | Loss: 1.1529 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 84.98
24-04-02 02:23:21.060 - INFO: Train epoch 349: [24000/94637 (25%)] Step: [2060831] | Lr: 0.000100 | Loss: 0.9352 | MSE loss: 0.0003 | Bpp loss: 0.51 | Aux loss: 87.07
24-04-02 02:23:56.071 - INFO: Train epoch 349: [25600/94637 (27%)] Step: [2060931] | Lr: 0.000100 | Loss: 1.2846 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 87.05
24-04-02 02:24:31.311 - INFO: Train epoch 349: [27200/94637 (29%)] Step: [2061031] | Lr: 0.000100 | Loss: 1.4297 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 91.11
24-04-02 02:25:06.444 - INFO: Train epoch 349: [28800/94637 (30%)] Step: [2061131] | Lr: 0.000100 | Loss: 1.0747 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 84.22
24-04-02 02:25:40.893 - INFO: Train epoch 349: [30400/94637 (32%)] Step: [2061231] | Lr: 0.000100 | Loss: 1.3422 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 86.51
24-04-02 02:26:14.863 - INFO: Train epoch 349: [32000/94637 (34%)] Step: [2061331] | Lr: 0.000100 | Loss: 1.5072 | MSE loss: 0.0003 | Bpp loss: 0.96 | Aux loss: 90.24
24-04-02 02:26:49.787 - INFO: Train epoch 349: [33600/94637 (36%)] Step: [2061431] | Lr: 0.000100 | Loss: 0.8714 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 83.76
24-04-02 02:27:24.869 - INFO: Train epoch 349: [35200/94637 (37%)] Step: [2061531] | Lr: 0.000100 | Loss: 1.6783 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 85.84
24-04-02 02:27:59.810 - INFO: Train epoch 349: [36800/94637 (39%)] Step: [2061631] | Lr: 0.000100 | Loss: 0.8613 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 91.98
24-04-02 02:28:33.872 - INFO: Train epoch 349: [38400/94637 (41%)] Step: [2061731] | Lr: 0.000100 | Loss: 1.0013 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 81.67
24-04-02 02:29:08.818 - INFO: Train epoch 349: [40000/94637 (42%)] Step: [2061831] | Lr: 0.000100 | Loss: 0.8987 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 88.86
24-04-02 02:29:43.817 - INFO: Train epoch 349: [41600/94637 (44%)] Step: [2061931] | Lr: 0.000100 | Loss: 1.4788 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 90.45
24-04-02 02:30:18.698 - INFO: Train epoch 349: [43200/94637 (46%)] Step: [2062031] | Lr: 0.000100 | Loss: 1.3306 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 88.94
24-04-02 02:30:53.522 - INFO: Train epoch 349: [44800/94637 (47%)] Step: [2062131] | Lr: 0.000100 | Loss: 0.9795 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 87.02
24-04-02 02:31:28.068 - INFO: Train epoch 349: [46400/94637 (49%)] Step: [2062231] | Lr: 0.000100 | Loss: 1.1842 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 84.01
24-04-02 02:32:02.394 - INFO: Train epoch 349: [48000/94637 (51%)] Step: [2062331] | Lr: 0.000100 | Loss: 1.2912 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.05
24-04-02 02:32:36.747 - INFO: Train epoch 349: [49600/94637 (52%)] Step: [2062431] | Lr: 0.000100 | Loss: 1.2211 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 90.58
24-04-02 02:33:13.656 - INFO: Train epoch 349: [51200/94637 (54%)] Step: [2062531] | Lr: 0.000100 | Loss: 1.4137 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 91.04
24-04-02 02:33:48.641 - INFO: Train epoch 349: [52800/94637 (56%)] Step: [2062631] | Lr: 0.000100 | Loss: 2.1413 | MSE loss: 0.0006 | Bpp loss: 1.11 | Aux loss: 84.52
24-04-02 02:34:22.880 - INFO: Train epoch 349: [54400/94637 (57%)] Step: [2062731] | Lr: 0.000100 | Loss: 1.2919 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 93.39
24-04-02 02:34:56.749 - INFO: Train epoch 349: [56000/94637 (59%)] Step: [2062831] | Lr: 0.000100 | Loss: 1.0926 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 84.06
24-04-02 02:35:31.747 - INFO: Train epoch 349: [57600/94637 (61%)] Step: [2062931] | Lr: 0.000100 | Loss: 0.5985 | MSE loss: 0.0002 | Bpp loss: 0.33 | Aux loss: 87.94
24-04-02 02:36:06.819 - INFO: Train epoch 349: [59200/94637 (63%)] Step: [2063031] | Lr: 0.000100 | Loss: 1.0998 | MSE loss: 0.0003 | Bpp loss: 0.64 | Aux loss: 81.43
24-04-02 02:36:42.255 - INFO: Train epoch 349: [60800/94637 (64%)] Step: [2063131] | Lr: 0.000100 | Loss: 1.2139 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 87.01
24-04-02 02:37:17.648 - INFO: Train epoch 349: [62400/94637 (66%)] Step: [2063231] | Lr: 0.000100 | Loss: 1.1509 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 86.69
24-04-02 02:37:53.413 - INFO: Train epoch 349: [64000/94637 (68%)] Step: [2063331] | Lr: 0.000100 | Loss: 1.4256 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 89.76
24-04-02 02:38:29.306 - INFO: Train epoch 349: [65600/94637 (69%)] Step: [2063431] | Lr: 0.000100 | Loss: 1.1836 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 87.23
24-04-02 02:39:04.443 - INFO: Train epoch 349: [67200/94637 (71%)] Step: [2063531] | Lr: 0.000100 | Loss: 1.3074 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 91.91
24-04-02 02:39:39.693 - INFO: Train epoch 349: [68800/94637 (73%)] Step: [2063631] | Lr: 0.000100 | Loss: 1.3061 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 91.66
24-04-02 02:40:15.326 - INFO: Train epoch 349: [70400/94637 (74%)] Step: [2063731] | Lr: 0.000100 | Loss: 2.7089 | MSE loss: 0.0008 | Bpp loss: 1.39 | Aux loss: 84.19
24-04-02 02:40:50.958 - INFO: Train epoch 349: [72000/94637 (76%)] Step: [2063831] | Lr: 0.000100 | Loss: 1.5937 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 83.14
24-04-02 02:41:26.230 - INFO: Train epoch 349: [73600/94637 (78%)] Step: [2063931] | Lr: 0.000100 | Loss: 1.1245 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 90.34
24-04-02 02:42:01.203 - INFO: Train epoch 349: [75200/94637 (79%)] Step: [2064031] | Lr: 0.000100 | Loss: 1.4381 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 88.37
24-04-02 02:42:36.319 - INFO: Train epoch 349: [76800/94637 (81%)] Step: [2064131] | Lr: 0.000100 | Loss: 0.9472 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 87.57
24-04-02 02:43:11.628 - INFO: Train epoch 349: [78400/94637 (83%)] Step: [2064231] | Lr: 0.000100 | Loss: 1.1559 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 87.66
24-04-02 02:43:47.731 - INFO: Train epoch 349: [80000/94637 (85%)] Step: [2064331] | Lr: 0.000100 | Loss: 1.6396 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 88.45
24-04-02 02:44:22.872 - INFO: Train epoch 349: [81600/94637 (86%)] Step: [2064431] | Lr: 0.000100 | Loss: 1.6063 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 89.39
24-04-02 02:44:58.374 - INFO: Train epoch 349: [83200/94637 (88%)] Step: [2064531] | Lr: 0.000100 | Loss: 1.0107 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 85.13
24-04-02 02:45:35.028 - INFO: Train epoch 349: [84800/94637 (90%)] Step: [2064631] | Lr: 0.000100 | Loss: 1.4412 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 86.83
24-04-02 02:46:10.244 - INFO: Train epoch 349: [86400/94637 (91%)] Step: [2064731] | Lr: 0.000100 | Loss: 1.0866 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 84.95
24-04-02 02:46:46.022 - INFO: Train epoch 349: [88000/94637 (93%)] Step: [2064831] | Lr: 0.000100 | Loss: 1.1119 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 84.36
24-04-02 02:47:20.685 - INFO: Train epoch 349: [89600/94637 (95%)] Step: [2064931] | Lr: 0.000100 | Loss: 1.7205 | MSE loss: 0.0004 | Bpp loss: 1.06 | Aux loss: 82.43
24-04-02 02:47:58.776 - INFO: Train epoch 349: [91200/94637 (96%)] Step: [2065031] | Lr: 0.000100 | Loss: 1.5762 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 85.09
24-04-02 02:48:34.947 - INFO: Train epoch 349: [92800/94637 (98%)] Step: [2065131] | Lr: 0.000100 | Loss: 1.3157 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 91.53
24-04-02 02:49:09.747 - INFO: Train epoch 349: [94400/94637 (100%)] Step: [2065231] | Lr: 0.000100 | Loss: 1.3297 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 82.29
24-04-02 02:49:26.463 - INFO: Learning rate: 0.0001
24-04-02 02:49:27.264 - INFO: Train epoch 350: [    0/94637 (0%)] Step: [2065246] | Lr: 0.000100 | Loss: 0.8895 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 85.05
24-04-02 02:50:01.373 - INFO: Train epoch 350: [ 1600/94637 (2%)] Step: [2065346] | Lr: 0.000100 | Loss: 1.4501 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 87.48
24-04-02 02:50:36.087 - INFO: Train epoch 350: [ 3200/94637 (3%)] Step: [2065446] | Lr: 0.000100 | Loss: 1.1432 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 82.07
24-04-02 02:51:10.528 - INFO: Train epoch 350: [ 4800/94637 (5%)] Step: [2065546] | Lr: 0.000100 | Loss: 1.0031 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 83.40
24-04-02 02:51:44.919 - INFO: Train epoch 350: [ 6400/94637 (7%)] Step: [2065646] | Lr: 0.000100 | Loss: 1.9534 | MSE loss: 0.0004 | Bpp loss: 1.23 | Aux loss: 84.14
24-04-02 02:52:19.683 - INFO: Train epoch 350: [ 8000/94637 (8%)] Step: [2065746] | Lr: 0.000100 | Loss: 0.9916 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 84.21
24-04-02 02:52:54.974 - INFO: Train epoch 350: [ 9600/94637 (10%)] Step: [2065846] | Lr: 0.000100 | Loss: 1.1902 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 88.02
24-04-02 02:53:29.755 - INFO: Train epoch 350: [11200/94637 (12%)] Step: [2065946] | Lr: 0.000100 | Loss: 1.0290 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 86.30
24-04-02 02:54:04.202 - INFO: Train epoch 350: [12800/94637 (14%)] Step: [2066046] | Lr: 0.000100 | Loss: 1.5284 | MSE loss: 0.0004 | Bpp loss: 0.87 | Aux loss: 88.39
24-04-02 02:54:38.103 - INFO: Train epoch 350: [14400/94637 (15%)] Step: [2066146] | Lr: 0.000100 | Loss: 1.8117 | MSE loss: 0.0005 | Bpp loss: 0.96 | Aux loss: 91.47
24-04-02 02:55:11.919 - INFO: Train epoch 350: [16000/94637 (17%)] Step: [2066246] | Lr: 0.000100 | Loss: 1.6935 | MSE loss: 0.0004 | Bpp loss: 1.08 | Aux loss: 84.11
24-04-02 02:55:45.624 - INFO: Train epoch 350: [17600/94637 (19%)] Step: [2066346] | Lr: 0.000100 | Loss: 1.2978 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.30
24-04-02 02:56:20.375 - INFO: Train epoch 350: [19200/94637 (20%)] Step: [2066446] | Lr: 0.000100 | Loss: 1.3366 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 86.72
24-04-02 02:56:54.237 - INFO: Train epoch 350: [20800/94637 (22%)] Step: [2066546] | Lr: 0.000100 | Loss: 0.8333 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 80.84
24-04-02 02:57:30.004 - INFO: Train epoch 350: [22400/94637 (24%)] Step: [2066646] | Lr: 0.000100 | Loss: 1.2881 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 85.82
24-04-02 02:58:04.044 - INFO: Train epoch 350: [24000/94637 (25%)] Step: [2066746] | Lr: 0.000100 | Loss: 1.2476 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 86.98
24-04-02 02:58:38.762 - INFO: Train epoch 350: [25600/94637 (27%)] Step: [2066846] | Lr: 0.000100 | Loss: 0.9977 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 87.70
24-04-02 02:59:13.105 - INFO: Train epoch 350: [27200/94637 (29%)] Step: [2066946] | Lr: 0.000100 | Loss: 1.4915 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 82.04
24-04-02 02:59:47.733 - INFO: Train epoch 350: [28800/94637 (30%)] Step: [2067046] | Lr: 0.000100 | Loss: 1.0931 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 81.41
24-04-02 03:00:23.512 - INFO: Train epoch 350: [30400/94637 (32%)] Step: [2067146] | Lr: 0.000100 | Loss: 1.0318 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 83.41
24-04-02 03:00:59.095 - INFO: Train epoch 350: [32000/94637 (34%)] Step: [2067246] | Lr: 0.000100 | Loss: 1.9270 | MSE loss: 0.0005 | Bpp loss: 1.07 | Aux loss: 84.82
24-04-02 03:01:34.503 - INFO: Train epoch 350: [33600/94637 (36%)] Step: [2067346] | Lr: 0.000100 | Loss: 1.0318 | MSE loss: 0.0002 | Bpp loss: 0.67 | Aux loss: 84.86
24-04-02 03:02:10.306 - INFO: Train epoch 350: [35200/94637 (37%)] Step: [2067446] | Lr: 0.000100 | Loss: 0.9826 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 94.68
24-04-02 03:02:46.793 - INFO: Train epoch 350: [36800/94637 (39%)] Step: [2067546] | Lr: 0.000100 | Loss: 1.6904 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 88.72
24-04-02 03:03:20.737 - INFO: Train epoch 350: [38400/94637 (41%)] Step: [2067646] | Lr: 0.000100 | Loss: 1.4962 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 85.03
24-04-02 03:03:55.079 - INFO: Train epoch 350: [40000/94637 (42%)] Step: [2067746] | Lr: 0.000100 | Loss: 1.3056 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 88.27
24-04-02 03:04:28.663 - INFO: Train epoch 350: [41600/94637 (44%)] Step: [2067846] | Lr: 0.000100 | Loss: 1.3506 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 86.05
24-04-02 03:05:02.568 - INFO: Train epoch 350: [43200/94637 (46%)] Step: [2067946] | Lr: 0.000100 | Loss: 1.1859 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 86.85
24-04-02 03:05:37.680 - INFO: Train epoch 350: [44800/94637 (47%)] Step: [2068046] | Lr: 0.000100 | Loss: 1.1106 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 91.98
24-04-02 03:06:14.105 - INFO: Train epoch 350: [46400/94637 (49%)] Step: [2068146] | Lr: 0.000100 | Loss: 1.6518 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 89.30
24-04-02 03:06:49.989 - INFO: Train epoch 350: [48000/94637 (51%)] Step: [2068246] | Lr: 0.000100 | Loss: 0.5406 | MSE loss: 0.0001 | Bpp loss: 0.38 | Aux loss: 90.45
24-04-02 03:07:25.711 - INFO: Train epoch 350: [49600/94637 (52%)] Step: [2068346] | Lr: 0.000100 | Loss: 1.3644 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 92.29
24-04-02 03:08:01.816 - INFO: Train epoch 350: [51200/94637 (54%)] Step: [2068446] | Lr: 0.000100 | Loss: 1.2131 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 84.77
24-04-02 03:08:36.439 - INFO: Train epoch 350: [52800/94637 (56%)] Step: [2068546] | Lr: 0.000100 | Loss: 1.3423 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 92.72
24-04-02 03:09:10.983 - INFO: Train epoch 350: [54400/94637 (57%)] Step: [2068646] | Lr: 0.000100 | Loss: 1.8899 | MSE loss: 0.0005 | Bpp loss: 1.11 | Aux loss: 83.24
24-04-02 03:09:45.802 - INFO: Train epoch 350: [56000/94637 (59%)] Step: [2068746] | Lr: 0.000100 | Loss: 2.5031 | MSE loss: 0.0007 | Bpp loss: 1.37 | Aux loss: 87.16
24-04-02 03:10:19.730 - INFO: Train epoch 350: [57600/94637 (61%)] Step: [2068846] | Lr: 0.000100 | Loss: 0.9844 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 80.86
24-04-02 03:10:54.684 - INFO: Train epoch 350: [59200/94637 (63%)] Step: [2068946] | Lr: 0.000100 | Loss: 1.2460 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 83.45
24-04-02 03:11:29.648 - INFO: Train epoch 350: [60800/94637 (64%)] Step: [2069046] | Lr: 0.000100 | Loss: 1.1728 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 80.36
24-04-02 03:12:04.594 - INFO: Train epoch 350: [62400/94637 (66%)] Step: [2069146] | Lr: 0.000100 | Loss: 0.8398 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 81.84
24-04-02 03:12:40.233 - INFO: Train epoch 350: [64000/94637 (68%)] Step: [2069246] | Lr: 0.000100 | Loss: 1.4230 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 82.49
24-04-02 03:13:14.887 - INFO: Train epoch 350: [65600/94637 (69%)] Step: [2069346] | Lr: 0.000100 | Loss: 1.2040 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 79.91
24-04-02 03:13:49.794 - INFO: Train epoch 350: [67200/94637 (71%)] Step: [2069446] | Lr: 0.000100 | Loss: 1.0636 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 81.70
24-04-02 03:14:24.485 - INFO: Train epoch 350: [68800/94637 (73%)] Step: [2069546] | Lr: 0.000100 | Loss: 0.9389 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 75.56
24-04-02 03:14:59.994 - INFO: Train epoch 350: [70400/94637 (74%)] Step: [2069646] | Lr: 0.000100 | Loss: 1.6591 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 83.77
24-04-02 03:15:35.276 - INFO: Train epoch 350: [72000/94637 (76%)] Step: [2069746] | Lr: 0.000100 | Loss: 1.6062 | MSE loss: 0.0004 | Bpp loss: 1.00 | Aux loss: 77.73
24-04-02 03:16:10.311 - INFO: Train epoch 350: [73600/94637 (78%)] Step: [2069846] | Lr: 0.000100 | Loss: 1.3142 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 81.26
24-04-02 03:16:45.753 - INFO: Train epoch 350: [75200/94637 (79%)] Step: [2069946] | Lr: 0.000100 | Loss: 1.0716 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 75.10
24-04-02 03:17:23.981 - INFO: Train epoch 350: [76800/94637 (81%)] Step: [2070046] | Lr: 0.000100 | Loss: 1.4593 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 78.90
24-04-02 03:17:59.483 - INFO: Train epoch 350: [78400/94637 (83%)] Step: [2070146] | Lr: 0.000100 | Loss: 1.2842 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 86.39
24-04-02 03:18:34.033 - INFO: Train epoch 350: [80000/94637 (85%)] Step: [2070246] | Lr: 0.000100 | Loss: 0.7271 | MSE loss: 0.0002 | Bpp loss: 0.44 | Aux loss: 83.37
24-04-02 03:19:09.790 - INFO: Train epoch 350: [81600/94637 (86%)] Step: [2070346] | Lr: 0.000100 | Loss: 1.3385 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 87.58
24-04-02 03:19:44.232 - INFO: Train epoch 350: [83200/94637 (88%)] Step: [2070446] | Lr: 0.000100 | Loss: 1.2140 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 80.80
24-04-02 03:20:18.729 - INFO: Train epoch 350: [84800/94637 (90%)] Step: [2070546] | Lr: 0.000100 | Loss: 1.4968 | MSE loss: 0.0005 | Bpp loss: 0.73 | Aux loss: 87.21
24-04-02 03:20:53.113 - INFO: Train epoch 350: [86400/94637 (91%)] Step: [2070646] | Lr: 0.000100 | Loss: 1.5841 | MSE loss: 0.0004 | Bpp loss: 0.91 | Aux loss: 82.54
24-04-02 03:21:27.884 - INFO: Train epoch 350: [88000/94637 (93%)] Step: [2070746] | Lr: 0.000100 | Loss: 1.2905 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 85.42
24-04-02 03:22:02.731 - INFO: Train epoch 350: [89600/94637 (95%)] Step: [2070846] | Lr: 0.000100 | Loss: 1.6252 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 86.35
24-04-02 03:22:36.768 - INFO: Train epoch 350: [91200/94637 (96%)] Step: [2070946] | Lr: 0.000100 | Loss: 0.8591 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 81.01
24-04-02 03:23:10.974 - INFO: Train epoch 350: [92800/94637 (98%)] Step: [2071046] | Lr: 0.000100 | Loss: 1.2612 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 87.80
24-04-02 03:23:45.602 - INFO: Train epoch 350: [94400/94637 (100%)] Step: [2071146] | Lr: 0.000100 | Loss: 0.8298 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 83.10
24-04-02 03:24:02.203 - INFO: Learning rate: 0.0001
24-04-02 03:24:03.375 - INFO: Train epoch 351: [    0/94637 (0%)] Step: [2071161] | Lr: 0.000100 | Loss: 1.0361 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 84.53
24-04-02 03:24:37.079 - INFO: Train epoch 351: [ 1600/94637 (2%)] Step: [2071261] | Lr: 0.000100 | Loss: 1.8165 | MSE loss: 0.0004 | Bpp loss: 1.13 | Aux loss: 89.67
24-04-02 03:25:10.586 - INFO: Train epoch 351: [ 3200/94637 (3%)] Step: [2071361] | Lr: 0.000100 | Loss: 0.9795 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 79.65
24-04-02 03:25:43.638 - INFO: Train epoch 351: [ 4800/94637 (5%)] Step: [2071461] | Lr: 0.000100 | Loss: 1.8515 | MSE loss: 0.0004 | Bpp loss: 1.18 | Aux loss: 89.25
24-04-02 03:26:16.561 - INFO: Train epoch 351: [ 6400/94637 (7%)] Step: [2071561] | Lr: 0.000100 | Loss: 1.1161 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 86.69
24-04-02 03:26:49.944 - INFO: Train epoch 351: [ 8000/94637 (8%)] Step: [2071661] | Lr: 0.000100 | Loss: 1.1228 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 86.39
24-04-02 03:27:23.352 - INFO: Train epoch 351: [ 9600/94637 (10%)] Step: [2071761] | Lr: 0.000100 | Loss: 1.6038 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 81.65
24-04-02 03:27:56.896 - INFO: Train epoch 351: [11200/94637 (12%)] Step: [2071861] | Lr: 0.000100 | Loss: 1.3562 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 84.73
24-04-02 03:28:31.679 - INFO: Train epoch 351: [12800/94637 (14%)] Step: [2071961] | Lr: 0.000100 | Loss: 2.0957 | MSE loss: 0.0005 | Bpp loss: 1.23 | Aux loss: 82.29
24-04-02 03:29:06.791 - INFO: Train epoch 351: [14400/94637 (15%)] Step: [2072061] | Lr: 0.000100 | Loss: 1.2677 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 84.63
24-04-02 03:29:42.457 - INFO: Train epoch 351: [16000/94637 (17%)] Step: [2072161] | Lr: 0.000100 | Loss: 2.0975 | MSE loss: 0.0006 | Bpp loss: 1.16 | Aux loss: 88.35
24-04-02 03:30:16.400 - INFO: Train epoch 351: [17600/94637 (19%)] Step: [2072261] | Lr: 0.000100 | Loss: 1.7717 | MSE loss: 0.0004 | Bpp loss: 1.06 | Aux loss: 82.55
24-04-02 03:30:50.455 - INFO: Train epoch 351: [19200/94637 (20%)] Step: [2072361] | Lr: 0.000100 | Loss: 1.1543 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 91.23
24-04-02 03:31:24.734 - INFO: Train epoch 351: [20800/94637 (22%)] Step: [2072461] | Lr: 0.000100 | Loss: 0.9935 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 80.80
24-04-02 03:32:01.129 - INFO: Train epoch 351: [22400/94637 (24%)] Step: [2072561] | Lr: 0.000100 | Loss: 1.4254 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 89.24
24-04-02 03:32:35.808 - INFO: Train epoch 351: [24000/94637 (25%)] Step: [2072661] | Lr: 0.000100 | Loss: 1.1453 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 91.75
24-04-02 03:33:10.339 - INFO: Train epoch 351: [25600/94637 (27%)] Step: [2072761] | Lr: 0.000100 | Loss: 1.1357 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 89.30
24-04-02 03:33:45.294 - INFO: Train epoch 351: [27200/94637 (29%)] Step: [2072861] | Lr: 0.000100 | Loss: 0.9466 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 96.18
24-04-02 03:34:18.802 - INFO: Train epoch 351: [28800/94637 (30%)] Step: [2072961] | Lr: 0.000100 | Loss: 1.6061 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 86.30
24-04-02 03:34:53.431 - INFO: Train epoch 351: [30400/94637 (32%)] Step: [2073061] | Lr: 0.000100 | Loss: 1.3155 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 82.16
24-04-02 03:35:27.757 - INFO: Train epoch 351: [32000/94637 (34%)] Step: [2073161] | Lr: 0.000100 | Loss: 1.4808 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 83.45
24-04-02 03:36:03.136 - INFO: Train epoch 351: [33600/94637 (36%)] Step: [2073261] | Lr: 0.000100 | Loss: 1.0185 | MSE loss: 0.0002 | Bpp loss: 0.68 | Aux loss: 83.97
24-04-02 03:36:38.490 - INFO: Train epoch 351: [35200/94637 (37%)] Step: [2073361] | Lr: 0.000100 | Loss: 1.6937 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 88.96
24-04-02 03:37:13.650 - INFO: Train epoch 351: [36800/94637 (39%)] Step: [2073461] | Lr: 0.000100 | Loss: 0.9078 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 82.87
24-04-02 03:37:48.414 - INFO: Train epoch 351: [38400/94637 (41%)] Step: [2073561] | Lr: 0.000100 | Loss: 1.5750 | MSE loss: 0.0004 | Bpp loss: 0.97 | Aux loss: 82.80
24-04-02 03:38:23.137 - INFO: Train epoch 351: [40000/94637 (42%)] Step: [2073661] | Lr: 0.000100 | Loss: 1.7474 | MSE loss: 0.0004 | Bpp loss: 1.15 | Aux loss: 85.06
24-04-02 03:38:57.970 - INFO: Train epoch 351: [41600/94637 (44%)] Step: [2073761] | Lr: 0.000100 | Loss: 1.2601 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 84.51
24-04-02 03:39:32.813 - INFO: Train epoch 351: [43200/94637 (46%)] Step: [2073861] | Lr: 0.000100 | Loss: 0.9315 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 85.64
24-04-02 03:40:07.158 - INFO: Train epoch 351: [44800/94637 (47%)] Step: [2073961] | Lr: 0.000100 | Loss: 1.2713 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 82.30
24-04-02 03:40:41.692 - INFO: Train epoch 351: [46400/94637 (49%)] Step: [2074061] | Lr: 0.000100 | Loss: 1.6259 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 83.17
24-04-02 03:41:16.880 - INFO: Train epoch 351: [48000/94637 (51%)] Step: [2074161] | Lr: 0.000100 | Loss: 1.2869 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 91.23
24-04-02 03:41:52.150 - INFO: Train epoch 351: [49600/94637 (52%)] Step: [2074261] | Lr: 0.000100 | Loss: 0.7272 | MSE loss: 0.0002 | Bpp loss: 0.46 | Aux loss: 85.64
24-04-02 03:42:27.274 - INFO: Train epoch 351: [51200/94637 (54%)] Step: [2074361] | Lr: 0.000100 | Loss: 0.7729 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 84.90
24-04-02 03:43:02.690 - INFO: Train epoch 351: [52800/94637 (56%)] Step: [2074461] | Lr: 0.000100 | Loss: 1.2030 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 86.52
24-04-02 03:43:37.149 - INFO: Train epoch 351: [54400/94637 (57%)] Step: [2074561] | Lr: 0.000100 | Loss: 0.9598 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 85.20
24-04-02 03:44:11.476 - INFO: Train epoch 351: [56000/94637 (59%)] Step: [2074661] | Lr: 0.000100 | Loss: 0.8064 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 86.39
24-04-02 03:44:45.798 - INFO: Train epoch 351: [57600/94637 (61%)] Step: [2074761] | Lr: 0.000100 | Loss: 1.2570 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 88.75
24-04-02 03:45:20.884 - INFO: Train epoch 351: [59200/94637 (63%)] Step: [2074861] | Lr: 0.000100 | Loss: 1.2839 | MSE loss: 0.0004 | Bpp loss: 0.69 | Aux loss: 91.13
24-04-02 03:45:55.810 - INFO: Train epoch 351: [60800/94637 (64%)] Step: [2074961] | Lr: 0.000100 | Loss: 1.0177 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 83.75
24-04-02 03:46:32.981 - INFO: Train epoch 351: [62400/94637 (66%)] Step: [2075061] | Lr: 0.000100 | Loss: 0.8066 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 84.34
24-04-02 03:47:07.666 - INFO: Train epoch 351: [64000/94637 (68%)] Step: [2075161] | Lr: 0.000100 | Loss: 1.1512 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 89.08
24-04-02 03:47:42.368 - INFO: Train epoch 351: [65600/94637 (69%)] Step: [2075261] | Lr: 0.000100 | Loss: 0.9757 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 93.30
24-04-02 03:48:16.950 - INFO: Train epoch 351: [67200/94637 (71%)] Step: [2075361] | Lr: 0.000100 | Loss: 1.2689 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 87.88
24-04-02 03:48:50.950 - INFO: Train epoch 351: [68800/94637 (73%)] Step: [2075461] | Lr: 0.000100 | Loss: 1.1363 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 89.95
24-04-02 03:49:24.417 - INFO: Train epoch 351: [70400/94637 (74%)] Step: [2075561] | Lr: 0.000100 | Loss: 1.1742 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 90.83
24-04-02 03:49:59.891 - INFO: Train epoch 351: [72000/94637 (76%)] Step: [2075661] | Lr: 0.000100 | Loss: 0.8277 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 87.46
24-04-02 03:50:34.253 - INFO: Train epoch 351: [73600/94637 (78%)] Step: [2075761] | Lr: 0.000100 | Loss: 1.0803 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 89.39
24-04-02 03:51:08.787 - INFO: Train epoch 351: [75200/94637 (79%)] Step: [2075861] | Lr: 0.000100 | Loss: 1.0827 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 89.71
24-04-02 03:51:44.954 - INFO: Train epoch 351: [76800/94637 (81%)] Step: [2075961] | Lr: 0.000100 | Loss: 1.1725 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 88.59
24-04-02 03:52:20.428 - INFO: Train epoch 351: [78400/94637 (83%)] Step: [2076061] | Lr: 0.000100 | Loss: 1.2092 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 89.38
24-04-02 03:52:55.748 - INFO: Train epoch 351: [80000/94637 (85%)] Step: [2076161] | Lr: 0.000100 | Loss: 2.1420 | MSE loss: 0.0006 | Bpp loss: 1.18 | Aux loss: 83.71
24-04-02 03:53:30.674 - INFO: Train epoch 351: [81600/94637 (86%)] Step: [2076261] | Lr: 0.000100 | Loss: 1.0930 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 84.57
24-04-02 03:54:05.960 - INFO: Train epoch 351: [83200/94637 (88%)] Step: [2076361] | Lr: 0.000100 | Loss: 1.2187 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 89.12
24-04-02 03:54:41.136 - INFO: Train epoch 351: [84800/94637 (90%)] Step: [2076461] | Lr: 0.000100 | Loss: 1.2408 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 91.27
24-04-02 03:55:16.171 - INFO: Train epoch 351: [86400/94637 (91%)] Step: [2076561] | Lr: 0.000100 | Loss: 1.1064 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 85.49
24-04-02 03:55:50.780 - INFO: Train epoch 351: [88000/94637 (93%)] Step: [2076661] | Lr: 0.000100 | Loss: 0.6789 | MSE loss: 0.0001 | Bpp loss: 0.45 | Aux loss: 85.70
24-04-02 03:56:25.118 - INFO: Train epoch 351: [89600/94637 (95%)] Step: [2076761] | Lr: 0.000100 | Loss: 0.9944 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 85.03
24-04-02 03:56:59.843 - INFO: Train epoch 351: [91200/94637 (96%)] Step: [2076861] | Lr: 0.000100 | Loss: 1.4282 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 86.69
24-04-02 03:57:35.467 - INFO: Train epoch 351: [92800/94637 (98%)] Step: [2076961] | Lr: 0.000100 | Loss: 1.8342 | MSE loss: 0.0004 | Bpp loss: 1.14 | Aux loss: 85.93
24-04-02 03:58:11.436 - INFO: Train epoch 351: [94400/94637 (100%)] Step: [2077061] | Lr: 0.000100 | Loss: 0.9506 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 89.51
24-04-02 03:58:33.609 - INFO: Learning rate: 0.0001
24-04-02 03:58:34.524 - INFO: Train epoch 352: [    0/94637 (0%)] Step: [2077076] | Lr: 0.000100 | Loss: 0.8299 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 82.87
24-04-02 03:59:09.190 - INFO: Train epoch 352: [ 1600/94637 (2%)] Step: [2077176] | Lr: 0.000100 | Loss: 0.8112 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 83.37
24-04-02 03:59:44.708 - INFO: Train epoch 352: [ 3200/94637 (3%)] Step: [2077276] | Lr: 0.000100 | Loss: 1.2730 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 80.79
24-04-02 04:00:19.103 - INFO: Train epoch 352: [ 4800/94637 (5%)] Step: [2077376] | Lr: 0.000100 | Loss: 0.9433 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 88.32
24-04-02 04:00:52.721 - INFO: Train epoch 352: [ 6400/94637 (7%)] Step: [2077476] | Lr: 0.000100 | Loss: 1.8032 | MSE loss: 0.0004 | Bpp loss: 1.08 | Aux loss: 84.82
24-04-02 04:01:28.754 - INFO: Train epoch 352: [ 8000/94637 (8%)] Step: [2077576] | Lr: 0.000100 | Loss: 1.1076 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 85.21
24-04-02 04:02:02.325 - INFO: Train epoch 352: [ 9600/94637 (10%)] Step: [2077676] | Lr: 0.000100 | Loss: 1.1578 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 81.93
24-04-02 04:02:35.854 - INFO: Train epoch 352: [11200/94637 (12%)] Step: [2077776] | Lr: 0.000100 | Loss: 1.1456 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 92.04
24-04-02 04:03:10.635 - INFO: Train epoch 352: [12800/94637 (14%)] Step: [2077876] | Lr: 0.000100 | Loss: 1.0827 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 92.87
24-04-02 04:03:44.738 - INFO: Train epoch 352: [14400/94637 (15%)] Step: [2077976] | Lr: 0.000100 | Loss: 1.9800 | MSE loss: 0.0005 | Bpp loss: 1.19 | Aux loss: 85.17
24-04-02 04:04:18.760 - INFO: Train epoch 352: [16000/94637 (17%)] Step: [2078076] | Lr: 0.000100 | Loss: 1.3799 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 86.93
24-04-02 04:04:52.866 - INFO: Train epoch 352: [17600/94637 (19%)] Step: [2078176] | Lr: 0.000100 | Loss: 0.8650 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 81.69
24-04-02 04:05:27.353 - INFO: Train epoch 352: [19200/94637 (20%)] Step: [2078276] | Lr: 0.000100 | Loss: 1.3567 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 88.42
24-04-02 04:06:02.349 - INFO: Train epoch 352: [20800/94637 (22%)] Step: [2078376] | Lr: 0.000100 | Loss: 1.4731 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 84.66
24-04-02 04:06:35.926 - INFO: Train epoch 352: [22400/94637 (24%)] Step: [2078476] | Lr: 0.000100 | Loss: 1.5492 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 90.72
24-04-02 04:07:10.416 - INFO: Train epoch 352: [24000/94637 (25%)] Step: [2078576] | Lr: 0.000100 | Loss: 1.1100 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 88.16
24-04-02 04:07:45.662 - INFO: Train epoch 352: [25600/94637 (27%)] Step: [2078676] | Lr: 0.000100 | Loss: 2.0483 | MSE loss: 0.0006 | Bpp loss: 1.12 | Aux loss: 88.82
24-04-02 04:08:21.535 - INFO: Train epoch 352: [27200/94637 (29%)] Step: [2078776] | Lr: 0.000100 | Loss: 1.5409 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 86.98
24-04-02 04:08:56.854 - INFO: Train epoch 352: [28800/94637 (30%)] Step: [2078876] | Lr: 0.000100 | Loss: 0.8095 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 85.20
24-04-02 04:09:31.530 - INFO: Train epoch 352: [30400/94637 (32%)] Step: [2078976] | Lr: 0.000100 | Loss: 1.1808 | MSE loss: 0.0002 | Bpp loss: 0.79 | Aux loss: 81.02
24-04-02 04:10:06.549 - INFO: Train epoch 352: [32000/94637 (34%)] Step: [2079076] | Lr: 0.000100 | Loss: 1.9984 | MSE loss: 0.0005 | Bpp loss: 1.21 | Aux loss: 86.49
24-04-02 04:10:40.623 - INFO: Train epoch 352: [33600/94637 (36%)] Step: [2079176] | Lr: 0.000100 | Loss: 1.6604 | MSE loss: 0.0004 | Bpp loss: 0.97 | Aux loss: 82.86
24-04-02 04:11:15.085 - INFO: Train epoch 352: [35200/94637 (37%)] Step: [2079276] | Lr: 0.000100 | Loss: 1.6027 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 84.36
24-04-02 04:11:50.515 - INFO: Train epoch 352: [36800/94637 (39%)] Step: [2079376] | Lr: 0.000100 | Loss: 1.5247 | MSE loss: 0.0004 | Bpp loss: 0.94 | Aux loss: 80.79
24-04-02 04:12:25.606 - INFO: Train epoch 352: [38400/94637 (41%)] Step: [2079476] | Lr: 0.000100 | Loss: 1.8362 | MSE loss: 0.0004 | Bpp loss: 1.17 | Aux loss: 87.90
24-04-02 04:13:00.903 - INFO: Train epoch 352: [40000/94637 (42%)] Step: [2079576] | Lr: 0.000100 | Loss: 1.1601 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 84.08
24-04-02 04:13:35.014 - INFO: Train epoch 352: [41600/94637 (44%)] Step: [2079676] | Lr: 0.000100 | Loss: 1.2102 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 91.72
24-04-02 04:14:09.878 - INFO: Train epoch 352: [43200/94637 (46%)] Step: [2079776] | Lr: 0.000100 | Loss: 1.7308 | MSE loss: 0.0005 | Bpp loss: 0.90 | Aux loss: 86.02
24-04-02 04:14:44.867 - INFO: Train epoch 352: [44800/94637 (47%)] Step: [2079876] | Lr: 0.000100 | Loss: 0.5747 | MSE loss: 0.0001 | Bpp loss: 0.38 | Aux loss: 86.19
24-04-02 04:15:19.730 - INFO: Train epoch 352: [46400/94637 (49%)] Step: [2079976] | Lr: 0.000100 | Loss: 1.0103 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 86.52
24-04-02 04:15:57.324 - INFO: Train epoch 352: [48000/94637 (51%)] Step: [2080076] | Lr: 0.000100 | Loss: 1.3413 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 91.58
24-04-02 04:16:32.381 - INFO: Train epoch 352: [49600/94637 (52%)] Step: [2080176] | Lr: 0.000100 | Loss: 1.4281 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 86.83
24-04-02 04:17:06.752 - INFO: Train epoch 352: [51200/94637 (54%)] Step: [2080276] | Lr: 0.000100 | Loss: 1.2990 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 91.14
24-04-02 04:17:42.071 - INFO: Train epoch 352: [52800/94637 (56%)] Step: [2080376] | Lr: 0.000100 | Loss: 1.1750 | MSE loss: 0.0002 | Bpp loss: 0.77 | Aux loss: 91.56
24-04-02 04:18:18.471 - INFO: Train epoch 352: [54400/94637 (57%)] Step: [2080476] | Lr: 0.000100 | Loss: 1.9548 | MSE loss: 0.0005 | Bpp loss: 1.13 | Aux loss: 88.48
24-04-02 04:18:53.891 - INFO: Train epoch 352: [56000/94637 (59%)] Step: [2080576] | Lr: 0.000100 | Loss: 1.3968 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 86.44
24-04-02 04:19:29.170 - INFO: Train epoch 352: [57600/94637 (61%)] Step: [2080676] | Lr: 0.000100 | Loss: 1.5852 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 95.12
24-04-02 04:20:04.322 - INFO: Train epoch 352: [59200/94637 (63%)] Step: [2080776] | Lr: 0.000100 | Loss: 1.1558 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 86.56
24-04-02 04:20:38.677 - INFO: Train epoch 352: [60800/94637 (64%)] Step: [2080876] | Lr: 0.000100 | Loss: 1.3028 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 85.28
24-04-02 04:21:12.947 - INFO: Train epoch 352: [62400/94637 (66%)] Step: [2080976] | Lr: 0.000100 | Loss: 1.3052 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 84.05
24-04-02 04:21:47.522 - INFO: Train epoch 352: [64000/94637 (68%)] Step: [2081076] | Lr: 0.000100 | Loss: 0.8141 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 86.05
24-04-02 04:22:22.148 - INFO: Train epoch 352: [65600/94637 (69%)] Step: [2081176] | Lr: 0.000100 | Loss: 1.5363 | MSE loss: 0.0004 | Bpp loss: 0.86 | Aux loss: 85.54
24-04-02 04:22:56.912 - INFO: Train epoch 352: [67200/94637 (71%)] Step: [2081276] | Lr: 0.000100 | Loss: 1.0815 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 84.37
24-04-02 04:23:31.513 - INFO: Train epoch 352: [68800/94637 (73%)] Step: [2081376] | Lr: 0.000100 | Loss: 1.4053 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 83.26
24-04-02 04:24:06.570 - INFO: Train epoch 352: [70400/94637 (74%)] Step: [2081476] | Lr: 0.000100 | Loss: 1.0564 | MSE loss: 0.0003 | Bpp loss: 0.63 | Aux loss: 87.39
24-04-02 04:24:41.788 - INFO: Train epoch 352: [72000/94637 (76%)] Step: [2081576] | Lr: 0.000100 | Loss: 1.1767 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 83.68
24-04-02 04:25:16.785 - INFO: Train epoch 352: [73600/94637 (78%)] Step: [2081676] | Lr: 0.000100 | Loss: 0.9839 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 82.50
24-04-02 04:25:52.156 - INFO: Train epoch 352: [75200/94637 (79%)] Step: [2081776] | Lr: 0.000100 | Loss: 0.9805 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 86.55
24-04-02 04:26:26.946 - INFO: Train epoch 352: [76800/94637 (81%)] Step: [2081876] | Lr: 0.000100 | Loss: 1.2609 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 84.73
24-04-02 04:27:02.007 - INFO: Train epoch 352: [78400/94637 (83%)] Step: [2081976] | Lr: 0.000100 | Loss: 1.3907 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 89.89
24-04-02 04:27:37.341 - INFO: Train epoch 352: [80000/94637 (85%)] Step: [2082076] | Lr: 0.000100 | Loss: 0.7691 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 87.42
24-04-02 04:28:11.781 - INFO: Train epoch 352: [81600/94637 (86%)] Step: [2082176] | Lr: 0.000100 | Loss: 0.8859 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 87.38
24-04-02 04:28:46.500 - INFO: Train epoch 352: [83200/94637 (88%)] Step: [2082276] | Lr: 0.000100 | Loss: 1.4837 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 91.37
24-04-02 04:29:20.739 - INFO: Train epoch 352: [84800/94637 (90%)] Step: [2082376] | Lr: 0.000100 | Loss: 1.3092 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 82.02
24-04-02 04:29:53.744 - INFO: Train epoch 352: [86400/94637 (91%)] Step: [2082476] | Lr: 0.000100 | Loss: 1.0377 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 86.41
24-04-02 04:30:29.887 - INFO: Train epoch 352: [88000/94637 (93%)] Step: [2082576] | Lr: 0.000100 | Loss: 1.1056 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 81.65
24-04-02 04:31:03.800 - INFO: Train epoch 352: [89600/94637 (95%)] Step: [2082676] | Lr: 0.000100 | Loss: 0.9996 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 82.69
24-04-02 04:31:37.699 - INFO: Train epoch 352: [91200/94637 (96%)] Step: [2082776] | Lr: 0.000100 | Loss: 1.2683 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 85.75
24-04-02 04:32:12.310 - INFO: Train epoch 352: [92800/94637 (98%)] Step: [2082876] | Lr: 0.000100 | Loss: 1.4901 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 81.21
24-04-02 04:32:45.598 - INFO: Train epoch 352: [94400/94637 (100%)] Step: [2082976] | Lr: 0.000100 | Loss: 1.6657 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 79.37
24-04-02 04:33:02.257 - INFO: Learning rate: 0.0001
24-04-02 04:33:03.116 - INFO: Train epoch 353: [    0/94637 (0%)] Step: [2082991] | Lr: 0.000100 | Loss: 0.8671 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 79.04
24-04-02 04:33:36.692 - INFO: Train epoch 353: [ 1600/94637 (2%)] Step: [2083091] | Lr: 0.000100 | Loss: 1.0204 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 81.83
24-04-02 04:34:11.541 - INFO: Train epoch 353: [ 3200/94637 (3%)] Step: [2083191] | Lr: 0.000100 | Loss: 1.0852 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 83.51
24-04-02 04:34:46.076 - INFO: Train epoch 353: [ 4800/94637 (5%)] Step: [2083291] | Lr: 0.000100 | Loss: 1.0494 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 79.50
24-04-02 04:35:20.310 - INFO: Train epoch 353: [ 6400/94637 (7%)] Step: [2083391] | Lr: 0.000100 | Loss: 1.3884 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 85.74
24-04-02 04:35:54.424 - INFO: Train epoch 353: [ 8000/94637 (8%)] Step: [2083491] | Lr: 0.000100 | Loss: 0.8359 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 80.19
24-04-02 04:36:29.098 - INFO: Train epoch 353: [ 9600/94637 (10%)] Step: [2083591] | Lr: 0.000100 | Loss: 1.0472 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 85.33
24-04-02 04:37:03.628 - INFO: Train epoch 353: [11200/94637 (12%)] Step: [2083691] | Lr: 0.000100 | Loss: 1.1276 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 84.43
24-04-02 04:37:38.771 - INFO: Train epoch 353: [12800/94637 (14%)] Step: [2083791] | Lr: 0.000100 | Loss: 1.4114 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 83.26
24-04-02 04:38:14.173 - INFO: Train epoch 353: [14400/94637 (15%)] Step: [2083891] | Lr: 0.000100 | Loss: 1.6781 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 83.75
24-04-02 04:38:48.279 - INFO: Train epoch 353: [16000/94637 (17%)] Step: [2083991] | Lr: 0.000100 | Loss: 1.4544 | MSE loss: 0.0004 | Bpp loss: 0.83 | Aux loss: 86.39
24-04-02 04:39:23.346 - INFO: Train epoch 353: [17600/94637 (19%)] Step: [2084091] | Lr: 0.000100 | Loss: 1.2729 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 86.57
24-04-02 04:39:58.885 - INFO: Train epoch 353: [19200/94637 (20%)] Step: [2084191] | Lr: 0.000100 | Loss: 0.7062 | MSE loss: 0.0001 | Bpp loss: 0.49 | Aux loss: 83.20
24-04-02 04:40:33.710 - INFO: Train epoch 353: [20800/94637 (22%)] Step: [2084291] | Lr: 0.000100 | Loss: 2.4583 | MSE loss: 0.0007 | Bpp loss: 1.40 | Aux loss: 84.68
24-04-02 04:41:09.115 - INFO: Train epoch 353: [22400/94637 (24%)] Step: [2084391] | Lr: 0.000100 | Loss: 1.4531 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 88.33
24-04-02 04:41:44.428 - INFO: Train epoch 353: [24000/94637 (25%)] Step: [2084491] | Lr: 0.000100 | Loss: 1.1250 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 81.29
24-04-02 04:42:19.340 - INFO: Train epoch 353: [25600/94637 (27%)] Step: [2084591] | Lr: 0.000100 | Loss: 1.5138 | MSE loss: 0.0003 | Bpp loss: 0.98 | Aux loss: 86.44
24-04-02 04:42:55.154 - INFO: Train epoch 353: [27200/94637 (29%)] Step: [2084691] | Lr: 0.000100 | Loss: 1.4979 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 87.69
24-04-02 04:43:30.571 - INFO: Train epoch 353: [28800/94637 (30%)] Step: [2084791] | Lr: 0.000100 | Loss: 1.2728 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 90.89
24-04-02 04:44:06.013 - INFO: Train epoch 353: [30400/94637 (32%)] Step: [2084891] | Lr: 0.000100 | Loss: 1.4665 | MSE loss: 0.0004 | Bpp loss: 0.86 | Aux loss: 81.76
24-04-02 04:44:41.851 - INFO: Train epoch 353: [32000/94637 (34%)] Step: [2084991] | Lr: 0.000100 | Loss: 1.2643 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 86.42
24-04-02 04:45:19.733 - INFO: Train epoch 353: [33600/94637 (36%)] Step: [2085091] | Lr: 0.000100 | Loss: 1.1468 | MSE loss: 0.0002 | Bpp loss: 0.74 | Aux loss: 91.86
24-04-02 04:45:54.884 - INFO: Train epoch 353: [35200/94637 (37%)] Step: [2085191] | Lr: 0.000100 | Loss: 1.3954 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 88.57
24-04-02 04:46:30.557 - INFO: Train epoch 353: [36800/94637 (39%)] Step: [2085291] | Lr: 0.000100 | Loss: 1.0211 | MSE loss: 0.0002 | Bpp loss: 0.68 | Aux loss: 85.73
24-04-02 04:47:06.245 - INFO: Train epoch 353: [38400/94637 (41%)] Step: [2085391] | Lr: 0.000100 | Loss: 1.0409 | MSE loss: 0.0003 | Bpp loss: 0.62 | Aux loss: 90.40
24-04-02 04:47:41.826 - INFO: Train epoch 353: [40000/94637 (42%)] Step: [2085491] | Lr: 0.000100 | Loss: 1.4059 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 86.27
24-04-02 04:48:16.586 - INFO: Train epoch 353: [41600/94637 (44%)] Step: [2085591] | Lr: 0.000100 | Loss: 1.3659 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 82.10
24-04-02 04:48:50.858 - INFO: Train epoch 353: [43200/94637 (46%)] Step: [2085691] | Lr: 0.000100 | Loss: 1.0376 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 83.74
24-04-02 04:49:24.834 - INFO: Train epoch 353: [44800/94637 (47%)] Step: [2085791] | Lr: 0.000100 | Loss: 1.4504 | MSE loss: 0.0004 | Bpp loss: 0.87 | Aux loss: 85.92
24-04-02 04:49:58.227 - INFO: Train epoch 353: [46400/94637 (49%)] Step: [2085891] | Lr: 0.000100 | Loss: 1.6468 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 90.69
24-04-02 04:50:32.902 - INFO: Train epoch 353: [48000/94637 (51%)] Step: [2085991] | Lr: 0.000100 | Loss: 1.0325 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 87.33
24-04-02 04:51:07.402 - INFO: Train epoch 353: [49600/94637 (52%)] Step: [2086091] | Lr: 0.000100 | Loss: 1.2270 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 92.62
24-04-02 04:51:41.926 - INFO: Train epoch 353: [51200/94637 (54%)] Step: [2086191] | Lr: 0.000100 | Loss: 1.2144 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 83.03
24-04-02 04:52:16.348 - INFO: Train epoch 353: [52800/94637 (56%)] Step: [2086291] | Lr: 0.000100 | Loss: 1.7773 | MSE loss: 0.0005 | Bpp loss: 0.99 | Aux loss: 86.98
24-04-02 04:52:51.388 - INFO: Train epoch 353: [54400/94637 (57%)] Step: [2086391] | Lr: 0.000100 | Loss: 1.0083 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 86.22
24-04-02 04:53:26.785 - INFO: Train epoch 353: [56000/94637 (59%)] Step: [2086491] | Lr: 0.000100 | Loss: 1.0305 | MSE loss: 0.0003 | Bpp loss: 0.61 | Aux loss: 88.62
24-04-02 04:54:02.333 - INFO: Train epoch 353: [57600/94637 (61%)] Step: [2086591] | Lr: 0.000100 | Loss: 1.5804 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 88.20
24-04-02 04:54:36.343 - INFO: Train epoch 353: [59200/94637 (63%)] Step: [2086691] | Lr: 0.000100 | Loss: 1.1143 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 90.42
24-04-02 04:55:11.522 - INFO: Train epoch 353: [60800/94637 (64%)] Step: [2086791] | Lr: 0.000100 | Loss: 1.6400 | MSE loss: 0.0005 | Bpp loss: 0.90 | Aux loss: 88.80
24-04-02 04:55:46.533 - INFO: Train epoch 353: [62400/94637 (66%)] Step: [2086891] | Lr: 0.000100 | Loss: 1.3458 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 87.64
24-04-02 04:56:21.284 - INFO: Train epoch 353: [64000/94637 (68%)] Step: [2086991] | Lr: 0.000100 | Loss: 1.4463 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 87.61
24-04-02 04:56:57.174 - INFO: Train epoch 353: [65600/94637 (69%)] Step: [2087091] | Lr: 0.000100 | Loss: 1.6685 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 86.28
24-04-02 04:57:31.570 - INFO: Train epoch 353: [67200/94637 (71%)] Step: [2087191] | Lr: 0.000100 | Loss: 2.3513 | MSE loss: 0.0008 | Bpp loss: 1.07 | Aux loss: 87.21
24-04-02 04:58:06.907 - INFO: Train epoch 353: [68800/94637 (73%)] Step: [2087291] | Lr: 0.000100 | Loss: 1.5573 | MSE loss: 0.0004 | Bpp loss: 0.92 | Aux loss: 78.80
24-04-02 04:58:41.206 - INFO: Train epoch 353: [70400/94637 (74%)] Step: [2087391] | Lr: 0.000100 | Loss: 1.6811 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 79.69
24-04-02 04:59:15.820 - INFO: Train epoch 353: [72000/94637 (76%)] Step: [2087491] | Lr: 0.000100 | Loss: 1.4387 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 80.96
24-04-02 04:59:53.413 - INFO: Train epoch 353: [73600/94637 (78%)] Step: [2087591] | Lr: 0.000100 | Loss: 1.0137 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 81.72
24-04-02 05:00:28.629 - INFO: Train epoch 353: [75200/94637 (79%)] Step: [2087691] | Lr: 0.000100 | Loss: 1.2665 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 79.04
24-04-02 05:01:03.334 - INFO: Train epoch 353: [76800/94637 (81%)] Step: [2087791] | Lr: 0.000100 | Loss: 1.4467 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 100.79
24-04-02 05:01:38.017 - INFO: Train epoch 353: [78400/94637 (83%)] Step: [2087891] | Lr: 0.000100 | Loss: 1.7854 | MSE loss: 0.0004 | Bpp loss: 1.14 | Aux loss: 81.64
24-04-02 05:02:13.563 - INFO: Train epoch 353: [80000/94637 (85%)] Step: [2087991] | Lr: 0.000100 | Loss: 1.0273 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 79.07
24-04-02 05:02:49.318 - INFO: Train epoch 353: [81600/94637 (86%)] Step: [2088091] | Lr: 0.000100 | Loss: 1.7823 | MSE loss: 0.0005 | Bpp loss: 1.04 | Aux loss: 78.69
24-04-02 05:03:24.230 - INFO: Train epoch 353: [83200/94637 (88%)] Step: [2088191] | Lr: 0.000100 | Loss: 1.0601 | MSE loss: 0.0002 | Bpp loss: 0.67 | Aux loss: 79.34
24-04-02 05:03:59.531 - INFO: Train epoch 353: [84800/94637 (90%)] Step: [2088291] | Lr: 0.000100 | Loss: 1.5180 | MSE loss: 0.0003 | Bpp loss: 0.99 | Aux loss: 78.78
24-04-02 05:04:33.801 - INFO: Train epoch 353: [86400/94637 (91%)] Step: [2088391] | Lr: 0.000100 | Loss: 0.5968 | MSE loss: 0.0001 | Bpp loss: 0.43 | Aux loss: 75.72
24-04-02 05:05:08.873 - INFO: Train epoch 353: [88000/94637 (93%)] Step: [2088491] | Lr: 0.000100 | Loss: 1.0077 | MSE loss: 0.0003 | Bpp loss: 0.56 | Aux loss: 79.19
24-04-02 05:05:43.388 - INFO: Train epoch 353: [89600/94637 (95%)] Step: [2088591] | Lr: 0.000100 | Loss: 1.4525 | MSE loss: 0.0004 | Bpp loss: 0.81 | Aux loss: 79.04
24-04-02 05:06:17.728 - INFO: Train epoch 353: [91200/94637 (96%)] Step: [2088691] | Lr: 0.000100 | Loss: 1.0182 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 78.52
24-04-02 05:06:51.305 - INFO: Train epoch 353: [92800/94637 (98%)] Step: [2088791] | Lr: 0.000100 | Loss: 1.4702 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 78.51
24-04-02 05:07:25.605 - INFO: Train epoch 353: [94400/94637 (100%)] Step: [2088891] | Lr: 0.000100 | Loss: 1.1314 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 80.84
24-04-02 05:07:48.320 - INFO: Learning rate: 0.0001
24-04-02 05:07:49.299 - INFO: Train epoch 354: [    0/94637 (0%)] Step: [2088906] | Lr: 0.000100 | Loss: 1.1446 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 76.66
24-04-02 05:08:24.548 - INFO: Train epoch 354: [ 1600/94637 (2%)] Step: [2089006] | Lr: 0.000100 | Loss: 1.0476 | MSE loss: 0.0003 | Bpp loss: 0.63 | Aux loss: 80.18
24-04-02 05:08:59.381 - INFO: Train epoch 354: [ 3200/94637 (3%)] Step: [2089106] | Lr: 0.000100 | Loss: 1.1010 | MSE loss: 0.0003 | Bpp loss: 0.64 | Aux loss: 80.43
24-04-02 05:09:34.500 - INFO: Train epoch 354: [ 4800/94637 (5%)] Step: [2089206] | Lr: 0.000100 | Loss: 1.7863 | MSE loss: 0.0004 | Bpp loss: 1.11 | Aux loss: 84.01
24-04-02 05:10:09.844 - INFO: Train epoch 354: [ 6400/94637 (7%)] Step: [2089306] | Lr: 0.000100 | Loss: 0.8592 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 76.83
24-04-02 05:10:44.216 - INFO: Train epoch 354: [ 8000/94637 (8%)] Step: [2089406] | Lr: 0.000100 | Loss: 0.9276 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 76.00
24-04-02 05:11:19.413 - INFO: Train epoch 354: [ 9600/94637 (10%)] Step: [2089506] | Lr: 0.000100 | Loss: 1.7142 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 86.79
24-04-02 05:11:54.809 - INFO: Train epoch 354: [11200/94637 (12%)] Step: [2089606] | Lr: 0.000100 | Loss: 0.7902 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 80.70
24-04-02 05:12:29.847 - INFO: Train epoch 354: [12800/94637 (14%)] Step: [2089706] | Lr: 0.000100 | Loss: 1.4239 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 80.12
24-04-02 05:13:05.224 - INFO: Train epoch 354: [14400/94637 (15%)] Step: [2089806] | Lr: 0.000100 | Loss: 1.4662 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 83.39
24-04-02 05:13:40.337 - INFO: Train epoch 354: [16000/94637 (17%)] Step: [2089906] | Lr: 0.000100 | Loss: 0.8852 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 83.50
24-04-02 05:14:17.534 - INFO: Train epoch 354: [17600/94637 (19%)] Step: [2090006] | Lr: 0.000100 | Loss: 1.2856 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 83.00
24-04-02 05:14:53.306 - INFO: Train epoch 354: [19200/94637 (20%)] Step: [2090106] | Lr: 0.000100 | Loss: 1.2523 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 88.40
24-04-02 05:15:27.654 - INFO: Train epoch 354: [20800/94637 (22%)] Step: [2090206] | Lr: 0.000100 | Loss: 0.8678 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 79.17
24-04-02 05:16:02.272 - INFO: Train epoch 354: [22400/94637 (24%)] Step: [2090306] | Lr: 0.000100 | Loss: 1.4066 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 88.47
24-04-02 05:16:36.467 - INFO: Train epoch 354: [24000/94637 (25%)] Step: [2090406] | Lr: 0.000100 | Loss: 1.0969 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 86.92
24-04-02 05:17:12.203 - INFO: Train epoch 354: [25600/94637 (27%)] Step: [2090506] | Lr: 0.000100 | Loss: 0.9826 | MSE loss: 0.0003 | Bpp loss: 0.56 | Aux loss: 87.83
24-04-02 05:17:47.998 - INFO: Train epoch 354: [27200/94637 (29%)] Step: [2090606] | Lr: 0.000100 | Loss: 1.1033 | MSE loss: 0.0002 | Bpp loss: 0.72 | Aux loss: 79.92
24-04-02 05:18:23.079 - INFO: Train epoch 354: [28800/94637 (30%)] Step: [2090706] | Lr: 0.000100 | Loss: 1.2052 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 82.92
24-04-02 05:18:58.697 - INFO: Train epoch 354: [30400/94637 (32%)] Step: [2090806] | Lr: 0.000100 | Loss: 0.8773 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 85.06
24-04-02 05:19:33.777 - INFO: Train epoch 354: [32000/94637 (34%)] Step: [2090906] | Lr: 0.000100 | Loss: 1.0912 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 84.20
24-04-02 05:20:08.975 - INFO: Train epoch 354: [33600/94637 (36%)] Step: [2091006] | Lr: 0.000100 | Loss: 0.9198 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 81.01
24-04-02 05:20:44.312 - INFO: Train epoch 354: [35200/94637 (37%)] Step: [2091106] | Lr: 0.000100 | Loss: 1.3600 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 83.29
24-04-02 05:21:19.222 - INFO: Train epoch 354: [36800/94637 (39%)] Step: [2091206] | Lr: 0.000100 | Loss: 0.9181 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 83.19
24-04-02 05:21:54.073 - INFO: Train epoch 354: [38400/94637 (41%)] Step: [2091306] | Lr: 0.000100 | Loss: 1.2447 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 84.57
24-04-02 05:22:29.800 - INFO: Train epoch 354: [40000/94637 (42%)] Step: [2091406] | Lr: 0.000100 | Loss: 0.7864 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 84.64
24-04-02 05:23:04.715 - INFO: Train epoch 354: [41600/94637 (44%)] Step: [2091506] | Lr: 0.000100 | Loss: 1.2199 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 84.30
24-04-02 05:23:39.971 - INFO: Train epoch 354: [43200/94637 (46%)] Step: [2091606] | Lr: 0.000100 | Loss: 1.3434 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 90.99
24-04-02 05:24:15.171 - INFO: Train epoch 354: [44800/94637 (47%)] Step: [2091706] | Lr: 0.000100 | Loss: 0.9188 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 85.58
24-04-02 05:24:50.164 - INFO: Train epoch 354: [46400/94637 (49%)] Step: [2091806] | Lr: 0.000100 | Loss: 1.2576 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 90.26
24-04-02 05:25:25.557 - INFO: Train epoch 354: [48000/94637 (51%)] Step: [2091906] | Lr: 0.000100 | Loss: 2.6539 | MSE loss: 0.0008 | Bpp loss: 1.36 | Aux loss: 83.45
24-04-02 05:26:00.213 - INFO: Train epoch 354: [49600/94637 (52%)] Step: [2092006] | Lr: 0.000100 | Loss: 1.2637 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 86.27
24-04-02 05:26:35.767 - INFO: Train epoch 354: [51200/94637 (54%)] Step: [2092106] | Lr: 0.000100 | Loss: 2.0554 | MSE loss: 0.0004 | Bpp loss: 1.33 | Aux loss: 84.35
24-04-02 05:27:11.440 - INFO: Train epoch 354: [52800/94637 (56%)] Step: [2092206] | Lr: 0.000100 | Loss: 1.9726 | MSE loss: 0.0005 | Bpp loss: 1.16 | Aux loss: 88.03
24-04-02 05:27:47.098 - INFO: Train epoch 354: [54400/94637 (57%)] Step: [2092306] | Lr: 0.000100 | Loss: 1.8336 | MSE loss: 0.0006 | Bpp loss: 0.91 | Aux loss: 88.73
24-04-02 05:28:22.555 - INFO: Train epoch 354: [56000/94637 (59%)] Step: [2092406] | Lr: 0.000100 | Loss: 1.5426 | MSE loss: 0.0003 | Bpp loss: 1.00 | Aux loss: 89.19
24-04-02 05:28:59.212 - INFO: Train epoch 354: [57600/94637 (61%)] Step: [2092506] | Lr: 0.000100 | Loss: 1.4077 | MSE loss: 0.0004 | Bpp loss: 0.82 | Aux loss: 90.68
24-04-02 05:29:34.558 - INFO: Train epoch 354: [59200/94637 (63%)] Step: [2092606] | Lr: 0.000100 | Loss: 1.5788 | MSE loss: 0.0004 | Bpp loss: 1.00 | Aux loss: 85.45
24-04-02 05:30:09.618 - INFO: Train epoch 354: [60800/94637 (64%)] Step: [2092706] | Lr: 0.000100 | Loss: 1.4191 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 85.16
24-04-02 05:30:44.630 - INFO: Train epoch 354: [62400/94637 (66%)] Step: [2092806] | Lr: 0.000100 | Loss: 1.4901 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 84.48
24-04-02 05:31:19.176 - INFO: Train epoch 354: [64000/94637 (68%)] Step: [2092906] | Lr: 0.000100 | Loss: 1.5095 | MSE loss: 0.0003 | Bpp loss: 0.98 | Aux loss: 86.16
24-04-02 05:31:54.346 - INFO: Train epoch 354: [65600/94637 (69%)] Step: [2093006] | Lr: 0.000100 | Loss: 1.1588 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 89.21
24-04-02 05:32:28.287 - INFO: Train epoch 354: [67200/94637 (71%)] Step: [2093106] | Lr: 0.000100 | Loss: 1.1220 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 88.16
24-04-02 05:33:02.138 - INFO: Train epoch 354: [68800/94637 (73%)] Step: [2093206] | Lr: 0.000100 | Loss: 1.8621 | MSE loss: 0.0005 | Bpp loss: 1.01 | Aux loss: 88.66
24-04-02 05:33:35.668 - INFO: Train epoch 354: [70400/94637 (74%)] Step: [2093306] | Lr: 0.000100 | Loss: 1.2017 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 84.59
24-04-02 05:34:10.212 - INFO: Train epoch 354: [72000/94637 (76%)] Step: [2093406] | Lr: 0.000100 | Loss: 1.1763 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 94.14
24-04-02 05:34:45.833 - INFO: Train epoch 354: [73600/94637 (78%)] Step: [2093506] | Lr: 0.000100 | Loss: 1.2389 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 83.73
24-04-02 05:35:21.142 - INFO: Train epoch 354: [75200/94637 (79%)] Step: [2093606] | Lr: 0.000100 | Loss: 1.0393 | MSE loss: 0.0002 | Bpp loss: 0.67 | Aux loss: 84.07
24-04-02 05:35:57.064 - INFO: Train epoch 354: [76800/94637 (81%)] Step: [2093706] | Lr: 0.000100 | Loss: 1.8957 | MSE loss: 0.0005 | Bpp loss: 1.04 | Aux loss: 85.55
24-04-02 05:36:32.861 - INFO: Train epoch 354: [78400/94637 (83%)] Step: [2093806] | Lr: 0.000100 | Loss: 0.6768 | MSE loss: 0.0001 | Bpp loss: 0.46 | Aux loss: 87.18
24-04-02 05:37:08.671 - INFO: Train epoch 354: [80000/94637 (85%)] Step: [2093906] | Lr: 0.000100 | Loss: 1.1989 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 87.80
24-04-02 05:37:44.220 - INFO: Train epoch 354: [81600/94637 (86%)] Step: [2094006] | Lr: 0.000100 | Loss: 1.2707 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 86.06
24-04-02 05:38:18.910 - INFO: Train epoch 354: [83200/94637 (88%)] Step: [2094106] | Lr: 0.000100 | Loss: 1.3664 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 84.21
24-04-02 05:38:53.954 - INFO: Train epoch 354: [84800/94637 (90%)] Step: [2094206] | Lr: 0.000100 | Loss: 1.7143 | MSE loss: 0.0005 | Bpp loss: 0.98 | Aux loss: 85.32
24-04-02 05:39:28.248 - INFO: Train epoch 354: [86400/94637 (91%)] Step: [2094306] | Lr: 0.000100 | Loss: 1.8051 | MSE loss: 0.0005 | Bpp loss: 1.06 | Aux loss: 85.33
24-04-02 05:40:03.059 - INFO: Train epoch 354: [88000/94637 (93%)] Step: [2094406] | Lr: 0.000100 | Loss: 1.4217 | MSE loss: 0.0004 | Bpp loss: 0.82 | Aux loss: 89.81
24-04-02 05:40:37.454 - INFO: Train epoch 354: [89600/94637 (95%)] Step: [2094506] | Lr: 0.000100 | Loss: 0.9742 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 84.97
24-04-02 05:41:12.052 - INFO: Train epoch 354: [91200/94637 (96%)] Step: [2094606] | Lr: 0.000100 | Loss: 1.2857 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.27
24-04-02 05:41:47.834 - INFO: Train epoch 354: [92800/94637 (98%)] Step: [2094706] | Lr: 0.000100 | Loss: 1.3893 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 83.18
24-04-02 05:42:22.590 - INFO: Train epoch 354: [94400/94637 (100%)] Step: [2094806] | Lr: 0.000100 | Loss: 1.5849 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 81.61
24-04-02 05:42:39.625 - INFO: Learning rate: 0.0001
24-04-02 05:42:40.482 - INFO: Train epoch 355: [    0/94637 (0%)] Step: [2094821] | Lr: 0.000100 | Loss: 1.2251 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 79.86
24-04-02 05:43:13.803 - INFO: Train epoch 355: [ 1600/94637 (2%)] Step: [2094921] | Lr: 0.000100 | Loss: 1.8736 | MSE loss: 0.0005 | Bpp loss: 1.11 | Aux loss: 91.04
24-04-02 05:43:50.006 - INFO: Train epoch 355: [ 3200/94637 (3%)] Step: [2095021] | Lr: 0.000100 | Loss: 1.0720 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 85.50
24-04-02 05:44:25.196 - INFO: Train epoch 355: [ 4800/94637 (5%)] Step: [2095121] | Lr: 0.000100 | Loss: 1.8063 | MSE loss: 0.0005 | Bpp loss: 1.02 | Aux loss: 85.10
24-04-02 05:44:59.908 - INFO: Train epoch 355: [ 6400/94637 (7%)] Step: [2095221] | Lr: 0.000100 | Loss: 1.3988 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 85.38
24-04-02 05:45:35.022 - INFO: Train epoch 355: [ 8000/94637 (8%)] Step: [2095321] | Lr: 0.000100 | Loss: 1.1983 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.44
24-04-02 05:46:10.390 - INFO: Train epoch 355: [ 9600/94637 (10%)] Step: [2095421] | Lr: 0.000100 | Loss: 1.3613 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 83.99
24-04-02 05:46:45.175 - INFO: Train epoch 355: [11200/94637 (12%)] Step: [2095521] | Lr: 0.000100 | Loss: 1.0612 | MSE loss: 0.0003 | Bpp loss: 0.64 | Aux loss: 84.84
24-04-02 05:47:20.065 - INFO: Train epoch 355: [12800/94637 (14%)] Step: [2095621] | Lr: 0.000100 | Loss: 1.4093 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 85.81
24-04-02 05:47:54.359 - INFO: Train epoch 355: [14400/94637 (15%)] Step: [2095721] | Lr: 0.000100 | Loss: 1.1976 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 83.17
24-04-02 05:48:29.141 - INFO: Train epoch 355: [16000/94637 (17%)] Step: [2095821] | Lr: 0.000100 | Loss: 0.9548 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 86.56
24-04-02 05:49:04.074 - INFO: Train epoch 355: [17600/94637 (19%)] Step: [2095921] | Lr: 0.000100 | Loss: 1.6107 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 94.19
24-04-02 05:49:39.324 - INFO: Train epoch 355: [19200/94637 (20%)] Step: [2096021] | Lr: 0.000100 | Loss: 0.9757 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 86.54
24-04-02 05:50:14.452 - INFO: Train epoch 355: [20800/94637 (22%)] Step: [2096121] | Lr: 0.000100 | Loss: 1.3955 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 88.29
24-04-02 05:50:48.020 - INFO: Train epoch 355: [22400/94637 (24%)] Step: [2096221] | Lr: 0.000100 | Loss: 1.9108 | MSE loss: 0.0005 | Bpp loss: 1.09 | Aux loss: 86.59
24-04-02 05:51:22.831 - INFO: Train epoch 355: [24000/94637 (25%)] Step: [2096321] | Lr: 0.000100 | Loss: 0.8722 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 87.71
24-04-02 05:51:57.362 - INFO: Train epoch 355: [25600/94637 (27%)] Step: [2096421] | Lr: 0.000100 | Loss: 0.8775 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 91.85
24-04-02 05:52:32.481 - INFO: Train epoch 355: [27200/94637 (29%)] Step: [2096521] | Lr: 0.000100 | Loss: 1.4424 | MSE loss: 0.0003 | Bpp loss: 0.96 | Aux loss: 80.89
24-04-02 05:53:07.426 - INFO: Train epoch 355: [28800/94637 (30%)] Step: [2096621] | Lr: 0.000100 | Loss: 1.2317 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 85.44
24-04-02 05:53:42.366 - INFO: Train epoch 355: [30400/94637 (32%)] Step: [2096721] | Lr: 0.000100 | Loss: 1.2971 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 83.33
24-04-02 05:54:16.436 - INFO: Train epoch 355: [32000/94637 (34%)] Step: [2096821] | Lr: 0.000100 | Loss: 1.2101 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 80.58
24-04-02 05:54:50.988 - INFO: Train epoch 355: [33600/94637 (36%)] Step: [2096921] | Lr: 0.000100 | Loss: 1.4677 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 86.84
24-04-02 05:55:25.047 - INFO: Train epoch 355: [35200/94637 (37%)] Step: [2097021] | Lr: 0.000100 | Loss: 1.0007 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 87.63
24-04-02 05:55:59.493 - INFO: Train epoch 355: [36800/94637 (39%)] Step: [2097121] | Lr: 0.000100 | Loss: 1.2637 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 90.31
24-04-02 05:56:35.197 - INFO: Train epoch 355: [38400/94637 (41%)] Step: [2097221] | Lr: 0.000100 | Loss: 1.2962 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 93.64
24-04-02 05:57:09.957 - INFO: Train epoch 355: [40000/94637 (42%)] Step: [2097321] | Lr: 0.000100 | Loss: 1.0007 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 89.08
24-04-02 05:57:45.315 - INFO: Train epoch 355: [41600/94637 (44%)] Step: [2097421] | Lr: 0.000100 | Loss: 1.4809 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 85.04
24-04-02 05:58:22.211 - INFO: Train epoch 355: [43200/94637 (46%)] Step: [2097521] | Lr: 0.000100 | Loss: 1.4216 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 79.91
24-04-02 05:58:57.457 - INFO: Train epoch 355: [44800/94637 (47%)] Step: [2097621] | Lr: 0.000100 | Loss: 1.0826 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 84.39
24-04-02 05:59:31.631 - INFO: Train epoch 355: [46400/94637 (49%)] Step: [2097721] | Lr: 0.000100 | Loss: 1.0115 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 80.14
24-04-02 06:00:05.294 - INFO: Train epoch 355: [48000/94637 (51%)] Step: [2097821] | Lr: 0.000100 | Loss: 0.9736 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 80.45
24-04-02 06:00:40.469 - INFO: Train epoch 355: [49600/94637 (52%)] Step: [2097921] | Lr: 0.000100 | Loss: 1.4439 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 80.18
24-04-02 06:01:16.259 - INFO: Train epoch 355: [51200/94637 (54%)] Step: [2098021] | Lr: 0.000100 | Loss: 1.7507 | MSE loss: 0.0004 | Bpp loss: 1.06 | Aux loss: 97.47
24-04-02 06:01:50.592 - INFO: Train epoch 355: [52800/94637 (56%)] Step: [2098121] | Lr: 0.000100 | Loss: 1.8390 | MSE loss: 0.0004 | Bpp loss: 1.12 | Aux loss: 84.25
24-04-02 06:02:24.430 - INFO: Train epoch 355: [54400/94637 (57%)] Step: [2098221] | Lr: 0.000100 | Loss: 0.7633 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 80.59
24-04-02 06:02:59.585 - INFO: Train epoch 355: [56000/94637 (59%)] Step: [2098321] | Lr: 0.000100 | Loss: 0.9191 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 78.78
24-04-02 06:03:33.252 - INFO: Train epoch 355: [57600/94637 (61%)] Step: [2098421] | Lr: 0.000100 | Loss: 1.3696 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 82.88
24-04-02 06:04:07.472 - INFO: Train epoch 355: [59200/94637 (63%)] Step: [2098521] | Lr: 0.000100 | Loss: 1.2617 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 82.24
24-04-02 06:04:42.183 - INFO: Train epoch 355: [60800/94637 (64%)] Step: [2098621] | Lr: 0.000100 | Loss: 1.4430 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 84.01
24-04-02 06:05:16.795 - INFO: Train epoch 355: [62400/94637 (66%)] Step: [2098721] | Lr: 0.000100 | Loss: 1.1448 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 83.02
24-04-02 06:05:51.690 - INFO: Train epoch 355: [64000/94637 (68%)] Step: [2098821] | Lr: 0.000100 | Loss: 1.4285 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 78.16
24-04-02 06:06:26.356 - INFO: Train epoch 355: [65600/94637 (69%)] Step: [2098921] | Lr: 0.000100 | Loss: 0.9165 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 91.92
24-04-02 06:07:03.063 - INFO: Train epoch 355: [67200/94637 (71%)] Step: [2099021] | Lr: 0.000100 | Loss: 1.3246 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 82.29
24-04-02 06:07:37.976 - INFO: Train epoch 355: [68800/94637 (73%)] Step: [2099121] | Lr: 0.000100 | Loss: 1.6131 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 83.39
24-04-02 06:08:13.456 - INFO: Train epoch 355: [70400/94637 (74%)] Step: [2099221] | Lr: 0.000100 | Loss: 1.2736 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 79.62
24-04-02 06:08:49.175 - INFO: Train epoch 355: [72000/94637 (76%)] Step: [2099321] | Lr: 0.000100 | Loss: 1.6399 | MSE loss: 0.0004 | Bpp loss: 0.92 | Aux loss: 85.83
24-04-02 06:09:24.484 - INFO: Train epoch 355: [73600/94637 (78%)] Step: [2099421] | Lr: 0.000100 | Loss: 1.2956 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 84.85
24-04-02 06:09:59.673 - INFO: Train epoch 355: [75200/94637 (79%)] Step: [2099521] | Lr: 0.000100 | Loss: 0.8067 | MSE loss: 0.0002 | Bpp loss: 0.47 | Aux loss: 84.25
24-04-02 06:10:34.278 - INFO: Train epoch 355: [76800/94637 (81%)] Step: [2099621] | Lr: 0.000100 | Loss: 1.2079 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.27
24-04-02 06:11:08.652 - INFO: Train epoch 355: [78400/94637 (83%)] Step: [2099721] | Lr: 0.000100 | Loss: 0.8066 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 81.00
24-04-02 06:11:42.981 - INFO: Train epoch 355: [80000/94637 (85%)] Step: [2099821] | Lr: 0.000100 | Loss: 1.0964 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 82.88
24-04-02 06:12:18.255 - INFO: Train epoch 355: [81600/94637 (86%)] Step: [2099921] | Lr: 0.000100 | Loss: 1.4556 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 81.91
24-04-02 06:12:55.378 - INFO: Train epoch 355: [83200/94637 (88%)] Step: [2100021] | Lr: 0.000100 | Loss: 1.1431 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 83.69
24-04-02 06:13:28.389 - INFO: Train epoch 355: [84800/94637 (90%)] Step: [2100121] | Lr: 0.000100 | Loss: 1.2773 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 83.54
24-04-02 06:14:02.176 - INFO: Train epoch 355: [86400/94637 (91%)] Step: [2100221] | Lr: 0.000100 | Loss: 1.1550 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 82.39
24-04-02 06:14:37.286 - INFO: Train epoch 355: [88000/94637 (93%)] Step: [2100321] | Lr: 0.000100 | Loss: 1.4812 | MSE loss: 0.0004 | Bpp loss: 0.86 | Aux loss: 76.65
24-04-02 06:15:12.848 - INFO: Train epoch 355: [89600/94637 (95%)] Step: [2100421] | Lr: 0.000100 | Loss: 1.3363 | MSE loss: 0.0004 | Bpp loss: 0.75 | Aux loss: 87.47
24-04-02 06:15:48.330 - INFO: Train epoch 355: [91200/94637 (96%)] Step: [2100521] | Lr: 0.000100 | Loss: 0.7008 | MSE loss: 0.0001 | Bpp loss: 0.48 | Aux loss: 83.68
24-04-02 06:16:23.597 - INFO: Train epoch 355: [92800/94637 (98%)] Step: [2100621] | Lr: 0.000100 | Loss: 0.9684 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 80.46
24-04-02 06:16:58.627 - INFO: Train epoch 355: [94400/94637 (100%)] Step: [2100721] | Lr: 0.000100 | Loss: 0.9548 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 79.69
24-04-02 06:17:15.407 - INFO: Learning rate: 0.0001
24-04-02 06:17:16.223 - INFO: Train epoch 356: [    0/94637 (0%)] Step: [2100736] | Lr: 0.000100 | Loss: 1.5100 | MSE loss: 0.0004 | Bpp loss: 0.93 | Aux loss: 82.80
24-04-02 06:17:52.336 - INFO: Train epoch 356: [ 1600/94637 (2%)] Step: [2100836] | Lr: 0.000100 | Loss: 1.2167 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 85.22
24-04-02 06:18:28.361 - INFO: Train epoch 356: [ 3200/94637 (3%)] Step: [2100936] | Lr: 0.000100 | Loss: 1.3260 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 81.20
24-04-02 06:19:04.019 - INFO: Train epoch 356: [ 4800/94637 (5%)] Step: [2101036] | Lr: 0.000100 | Loss: 1.0389 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 84.98
24-04-02 06:19:40.244 - INFO: Train epoch 356: [ 6400/94637 (7%)] Step: [2101136] | Lr: 0.000100 | Loss: 1.0038 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 86.16
24-04-02 06:20:15.361 - INFO: Train epoch 356: [ 8000/94637 (8%)] Step: [2101236] | Lr: 0.000100 | Loss: 0.7681 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 87.56
24-04-02 06:20:50.523 - INFO: Train epoch 356: [ 9600/94637 (10%)] Step: [2101336] | Lr: 0.000100 | Loss: 1.2188 | MSE loss: 0.0002 | Bpp loss: 0.82 | Aux loss: 84.10
24-04-02 06:21:26.655 - INFO: Train epoch 356: [11200/94637 (12%)] Step: [2101436] | Lr: 0.000100 | Loss: 0.9318 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 84.94
24-04-02 06:22:02.387 - INFO: Train epoch 356: [12800/94637 (14%)] Step: [2101536] | Lr: 0.000100 | Loss: 0.6873 | MSE loss: 0.0001 | Bpp loss: 0.46 | Aux loss: 85.86
24-04-02 06:22:37.420 - INFO: Train epoch 356: [14400/94637 (15%)] Step: [2101636] | Lr: 0.000100 | Loss: 1.4002 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 82.49
24-04-02 06:23:12.499 - INFO: Train epoch 356: [16000/94637 (17%)] Step: [2101736] | Lr: 0.000100 | Loss: 1.2880 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 86.59
24-04-02 06:23:47.219 - INFO: Train epoch 356: [17600/94637 (19%)] Step: [2101836] | Lr: 0.000100 | Loss: 1.1240 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 88.71
24-04-02 06:24:22.522 - INFO: Train epoch 356: [19200/94637 (20%)] Step: [2101936] | Lr: 0.000100 | Loss: 1.3440 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 85.58
24-04-02 06:24:57.921 - INFO: Train epoch 356: [20800/94637 (22%)] Step: [2102036] | Lr: 0.000100 | Loss: 1.7536 | MSE loss: 0.0006 | Bpp loss: 0.71 | Aux loss: 80.30
24-04-02 06:25:33.646 - INFO: Train epoch 356: [22400/94637 (24%)] Step: [2102136] | Lr: 0.000100 | Loss: 1.1607 | MSE loss: 0.0002 | Bpp loss: 0.76 | Aux loss: 85.60
24-04-02 06:26:08.485 - INFO: Train epoch 356: [24000/94637 (25%)] Step: [2102236] | Lr: 0.000100 | Loss: 1.2210 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 88.47
24-04-02 06:26:43.453 - INFO: Train epoch 356: [25600/94637 (27%)] Step: [2102336] | Lr: 0.000100 | Loss: 1.3169 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 87.93
24-04-02 06:27:17.753 - INFO: Train epoch 356: [27200/94637 (29%)] Step: [2102436] | Lr: 0.000100 | Loss: 1.1365 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 84.90
24-04-02 06:27:54.291 - INFO: Train epoch 356: [28800/94637 (30%)] Step: [2102536] | Lr: 0.000100 | Loss: 0.9480 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 87.49
24-04-02 06:28:29.473 - INFO: Train epoch 356: [30400/94637 (32%)] Step: [2102636] | Lr: 0.000100 | Loss: 1.4565 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 90.77
24-04-02 06:29:04.087 - INFO: Train epoch 356: [32000/94637 (34%)] Step: [2102736] | Lr: 0.000100 | Loss: 0.9862 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 86.62
24-04-02 06:29:38.642 - INFO: Train epoch 356: [33600/94637 (36%)] Step: [2102836] | Lr: 0.000100 | Loss: 1.6431 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 88.60
24-04-02 06:30:13.385 - INFO: Train epoch 356: [35200/94637 (37%)] Step: [2102936] | Lr: 0.000100 | Loss: 1.0167 | MSE loss: 0.0003 | Bpp loss: 0.60 | Aux loss: 85.38
24-04-02 06:30:47.600 - INFO: Train epoch 356: [36800/94637 (39%)] Step: [2103036] | Lr: 0.000100 | Loss: 0.9477 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 93.31
24-04-02 06:31:22.012 - INFO: Train epoch 356: [38400/94637 (41%)] Step: [2103136] | Lr: 0.000100 | Loss: 1.0146 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 86.68
24-04-02 06:31:57.321 - INFO: Train epoch 356: [40000/94637 (42%)] Step: [2103236] | Lr: 0.000100 | Loss: 1.1789 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 86.07
24-04-02 06:32:32.650 - INFO: Train epoch 356: [41600/94637 (44%)] Step: [2103336] | Lr: 0.000100 | Loss: 2.6966 | MSE loss: 0.0007 | Bpp loss: 1.55 | Aux loss: 92.26
24-04-02 06:33:06.940 - INFO: Train epoch 356: [43200/94637 (46%)] Step: [2103436] | Lr: 0.000100 | Loss: 1.1223 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 87.89
24-04-02 06:33:42.332 - INFO: Train epoch 356: [44800/94637 (47%)] Step: [2103536] | Lr: 0.000100 | Loss: 1.3695 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 90.26
24-04-02 06:34:18.247 - INFO: Train epoch 356: [46400/94637 (49%)] Step: [2103636] | Lr: 0.000100 | Loss: 1.2964 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 82.26
24-04-02 06:34:53.032 - INFO: Train epoch 356: [48000/94637 (51%)] Step: [2103736] | Lr: 0.000100 | Loss: 1.3188 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 87.36
24-04-02 06:35:28.237 - INFO: Train epoch 356: [49600/94637 (52%)] Step: [2103836] | Lr: 0.000100 | Loss: 1.5262 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 82.79
24-04-02 06:36:03.410 - INFO: Train epoch 356: [51200/94637 (54%)] Step: [2103936] | Lr: 0.000100 | Loss: 1.8943 | MSE loss: 0.0004 | Bpp loss: 1.20 | Aux loss: 89.95
24-04-02 06:36:38.664 - INFO: Train epoch 356: [52800/94637 (56%)] Step: [2104036] | Lr: 0.000100 | Loss: 1.0960 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 86.33
24-04-02 06:37:13.752 - INFO: Train epoch 356: [54400/94637 (57%)] Step: [2104136] | Lr: 0.000100 | Loss: 1.2344 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 89.20
24-04-02 06:37:49.486 - INFO: Train epoch 356: [56000/94637 (59%)] Step: [2104236] | Lr: 0.000100 | Loss: 0.7640 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 87.68
24-04-02 06:38:24.828 - INFO: Train epoch 356: [57600/94637 (61%)] Step: [2104336] | Lr: 0.000100 | Loss: 1.1933 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 82.48
24-04-02 06:38:59.321 - INFO: Train epoch 356: [59200/94637 (63%)] Step: [2104436] | Lr: 0.000100 | Loss: 0.9760 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 84.49
24-04-02 06:39:33.912 - INFO: Train epoch 356: [60800/94637 (64%)] Step: [2104536] | Lr: 0.000100 | Loss: 2.1202 | MSE loss: 0.0005 | Bpp loss: 1.25 | Aux loss: 87.96
24-04-02 06:40:08.569 - INFO: Train epoch 356: [62400/94637 (66%)] Step: [2104636] | Lr: 0.000100 | Loss: 1.3705 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 93.00
24-04-02 06:40:42.503 - INFO: Train epoch 356: [64000/94637 (68%)] Step: [2104736] | Lr: 0.000100 | Loss: 1.0016 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 83.16
24-04-02 06:41:17.251 - INFO: Train epoch 356: [65600/94637 (69%)] Step: [2104836] | Lr: 0.000100 | Loss: 1.2633 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 82.30
24-04-02 06:41:53.627 - INFO: Train epoch 356: [67200/94637 (71%)] Step: [2104936] | Lr: 0.000100 | Loss: 1.0434 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 89.48
24-04-02 06:42:31.391 - INFO: Train epoch 356: [68800/94637 (73%)] Step: [2105036] | Lr: 0.000100 | Loss: 0.8247 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 86.25
24-04-02 06:43:06.620 - INFO: Train epoch 356: [70400/94637 (74%)] Step: [2105136] | Lr: 0.000100 | Loss: 0.8149 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 82.29
24-04-02 06:43:40.992 - INFO: Train epoch 356: [72000/94637 (76%)] Step: [2105236] | Lr: 0.000100 | Loss: 1.1990 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 89.10
24-04-02 06:44:15.595 - INFO: Train epoch 356: [73600/94637 (78%)] Step: [2105336] | Lr: 0.000100 | Loss: 1.0510 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 82.70
24-04-02 06:44:50.606 - INFO: Train epoch 356: [75200/94637 (79%)] Step: [2105436] | Lr: 0.000100 | Loss: 1.2388 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 90.74
24-04-02 06:45:25.244 - INFO: Train epoch 356: [76800/94637 (81%)] Step: [2105536] | Lr: 0.000100 | Loss: 1.2075 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 92.21
24-04-02 06:46:00.693 - INFO: Train epoch 356: [78400/94637 (83%)] Step: [2105636] | Lr: 0.000100 | Loss: 1.3837 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 89.76
24-04-02 06:46:35.912 - INFO: Train epoch 356: [80000/94637 (85%)] Step: [2105736] | Lr: 0.000100 | Loss: 1.1926 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 87.53
24-04-02 06:47:10.761 - INFO: Train epoch 356: [81600/94637 (86%)] Step: [2105836] | Lr: 0.000100 | Loss: 1.2060 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 88.73
24-04-02 06:47:45.855 - INFO: Train epoch 356: [83200/94637 (88%)] Step: [2105936] | Lr: 0.000100 | Loss: 1.7761 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 88.26
24-04-02 06:48:21.527 - INFO: Train epoch 356: [84800/94637 (90%)] Step: [2106036] | Lr: 0.000100 | Loss: 1.8845 | MSE loss: 0.0005 | Bpp loss: 1.00 | Aux loss: 88.24
24-04-02 06:48:57.195 - INFO: Train epoch 356: [86400/94637 (91%)] Step: [2106136] | Lr: 0.000100 | Loss: 1.0998 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 80.82
24-04-02 06:49:32.261 - INFO: Train epoch 356: [88000/94637 (93%)] Step: [2106236] | Lr: 0.000100 | Loss: 1.6118 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 89.97
24-04-02 06:50:07.813 - INFO: Train epoch 356: [89600/94637 (95%)] Step: [2106336] | Lr: 0.000100 | Loss: 0.9909 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 96.89
24-04-02 06:50:43.446 - INFO: Train epoch 356: [91200/94637 (96%)] Step: [2106436] | Lr: 0.000100 | Loss: 1.0580 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 83.76
24-04-02 06:51:18.484 - INFO: Train epoch 356: [92800/94637 (98%)] Step: [2106536] | Lr: 0.000100 | Loss: 1.3769 | MSE loss: 0.0004 | Bpp loss: 0.80 | Aux loss: 94.53
24-04-02 06:51:53.690 - INFO: Train epoch 356: [94400/94637 (100%)] Step: [2106636] | Lr: 0.000100 | Loss: 1.6320 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 94.27
24-04-02 06:52:10.409 - INFO: Learning rate: 0.0001
24-04-02 06:52:11.196 - INFO: Train epoch 357: [    0/94637 (0%)] Step: [2106651] | Lr: 0.000100 | Loss: 1.4493 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 82.85
24-04-02 06:52:45.935 - INFO: Train epoch 357: [ 1600/94637 (2%)] Step: [2106751] | Lr: 0.000100 | Loss: 0.9593 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 91.32
24-04-02 06:53:19.912 - INFO: Train epoch 357: [ 3200/94637 (3%)] Step: [2106851] | Lr: 0.000100 | Loss: 1.8121 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 90.42
24-04-02 06:53:55.223 - INFO: Train epoch 357: [ 4800/94637 (5%)] Step: [2106951] | Lr: 0.000100 | Loss: 0.8654 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 88.78
24-04-02 06:54:31.522 - INFO: Train epoch 357: [ 6400/94637 (7%)] Step: [2107051] | Lr: 0.000100 | Loss: 1.2647 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 83.90
24-04-02 06:55:06.511 - INFO: Train epoch 357: [ 8000/94637 (8%)] Step: [2107151] | Lr: 0.000100 | Loss: 1.1659 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 87.13
24-04-02 06:55:42.331 - INFO: Train epoch 357: [ 9600/94637 (10%)] Step: [2107251] | Lr: 0.000100 | Loss: 1.7687 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 84.43
24-04-02 06:56:18.430 - INFO: Train epoch 357: [11200/94637 (12%)] Step: [2107351] | Lr: 0.000100 | Loss: 1.3825 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 89.65
24-04-02 06:56:53.380 - INFO: Train epoch 357: [12800/94637 (14%)] Step: [2107451] | Lr: 0.000100 | Loss: 0.8242 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 93.33
24-04-02 06:57:30.804 - INFO: Train epoch 357: [14400/94637 (15%)] Step: [2107551] | Lr: 0.000100 | Loss: 1.1146 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 88.80
24-04-02 06:58:06.337 - INFO: Train epoch 357: [16000/94637 (17%)] Step: [2107651] | Lr: 0.000100 | Loss: 1.1117 | MSE loss: 0.0003 | Bpp loss: 0.61 | Aux loss: 83.40
24-04-02 06:58:42.920 - INFO: Train epoch 357: [17600/94637 (19%)] Step: [2107751] | Lr: 0.000100 | Loss: 1.4415 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 90.18
24-04-02 06:59:19.281 - INFO: Train epoch 357: [19200/94637 (20%)] Step: [2107851] | Lr: 0.000100 | Loss: 1.1648 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 85.87
24-04-02 06:59:54.400 - INFO: Train epoch 357: [20800/94637 (22%)] Step: [2107951] | Lr: 0.000100 | Loss: 1.2752 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 94.26
24-04-02 07:00:29.987 - INFO: Train epoch 357: [22400/94637 (24%)] Step: [2108051] | Lr: 0.000100 | Loss: 1.6467 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 91.04
24-04-02 07:01:04.997 - INFO: Train epoch 357: [24000/94637 (25%)] Step: [2108151] | Lr: 0.000100 | Loss: 1.6835 | MSE loss: 0.0005 | Bpp loss: 0.94 | Aux loss: 86.46
24-04-02 07:01:40.499 - INFO: Train epoch 357: [25600/94637 (27%)] Step: [2108251] | Lr: 0.000100 | Loss: 0.8995 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 86.79
24-04-02 07:02:15.444 - INFO: Train epoch 357: [27200/94637 (29%)] Step: [2108351] | Lr: 0.000100 | Loss: 2.2214 | MSE loss: 0.0006 | Bpp loss: 1.18 | Aux loss: 84.32
24-04-02 07:02:50.404 - INFO: Train epoch 357: [28800/94637 (30%)] Step: [2108451] | Lr: 0.000100 | Loss: 1.0685 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 87.45
24-04-02 07:03:26.272 - INFO: Train epoch 357: [30400/94637 (32%)] Step: [2108551] | Lr: 0.000100 | Loss: 1.5842 | MSE loss: 0.0005 | Bpp loss: 0.83 | Aux loss: 90.39
24-04-02 07:04:02.725 - INFO: Train epoch 357: [32000/94637 (34%)] Step: [2108651] | Lr: 0.000100 | Loss: 1.0073 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 89.50
24-04-02 07:04:37.929 - INFO: Train epoch 357: [33600/94637 (36%)] Step: [2108751] | Lr: 0.000100 | Loss: 1.6183 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 85.47
24-04-02 07:05:13.115 - INFO: Train epoch 357: [35200/94637 (37%)] Step: [2108851] | Lr: 0.000100 | Loss: 1.4256 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 86.68
24-04-02 07:05:48.182 - INFO: Train epoch 357: [36800/94637 (39%)] Step: [2108951] | Lr: 0.000100 | Loss: 1.7011 | MSE loss: 0.0005 | Bpp loss: 0.93 | Aux loss: 88.37
24-04-02 07:06:23.381 - INFO: Train epoch 357: [38400/94637 (41%)] Step: [2109051] | Lr: 0.000100 | Loss: 0.8999 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 94.47
24-04-02 07:06:58.280 - INFO: Train epoch 357: [40000/94637 (42%)] Step: [2109151] | Lr: 0.000100 | Loss: 1.5488 | MSE loss: 0.0004 | Bpp loss: 0.86 | Aux loss: 89.75
24-04-02 07:07:32.184 - INFO: Train epoch 357: [41600/94637 (44%)] Step: [2109251] | Lr: 0.000100 | Loss: 1.6822 | MSE loss: 0.0004 | Bpp loss: 1.08 | Aux loss: 94.36
24-04-02 07:08:06.398 - INFO: Train epoch 357: [43200/94637 (46%)] Step: [2109351] | Lr: 0.000100 | Loss: 1.2333 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 93.04
24-04-02 07:08:40.138 - INFO: Train epoch 357: [44800/94637 (47%)] Step: [2109451] | Lr: 0.000100 | Loss: 1.2354 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 84.53
24-04-02 07:09:14.787 - INFO: Train epoch 357: [46400/94637 (49%)] Step: [2109551] | Lr: 0.000100 | Loss: 1.1901 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 83.90
24-04-02 07:09:49.031 - INFO: Train epoch 357: [48000/94637 (51%)] Step: [2109651] | Lr: 0.000100 | Loss: 1.5766 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 88.20
24-04-02 07:10:22.570 - INFO: Train epoch 357: [49600/94637 (52%)] Step: [2109751] | Lr: 0.000100 | Loss: 1.2665 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 89.68
24-04-02 07:10:56.578 - INFO: Train epoch 357: [51200/94637 (54%)] Step: [2109851] | Lr: 0.000100 | Loss: 0.9291 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 88.97
24-04-02 07:11:30.907 - INFO: Train epoch 357: [52800/94637 (56%)] Step: [2109951] | Lr: 0.000100 | Loss: 0.6750 | MSE loss: 0.0002 | Bpp loss: 0.43 | Aux loss: 89.52
24-04-02 07:12:08.387 - INFO: Train epoch 357: [54400/94637 (57%)] Step: [2110051] | Lr: 0.000100 | Loss: 1.6143 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 85.84
24-04-02 07:12:43.994 - INFO: Train epoch 357: [56000/94637 (59%)] Step: [2110151] | Lr: 0.000100 | Loss: 1.1158 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 89.72
24-04-02 07:13:20.253 - INFO: Train epoch 357: [57600/94637 (61%)] Step: [2110251] | Lr: 0.000100 | Loss: 1.3486 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 89.06
24-04-02 07:13:56.325 - INFO: Train epoch 357: [59200/94637 (63%)] Step: [2110351] | Lr: 0.000100 | Loss: 1.1711 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 94.86
24-04-02 07:14:31.894 - INFO: Train epoch 357: [60800/94637 (64%)] Step: [2110451] | Lr: 0.000100 | Loss: 1.1893 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.86
24-04-02 07:15:07.903 - INFO: Train epoch 357: [62400/94637 (66%)] Step: [2110551] | Lr: 0.000100 | Loss: 1.3976 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 91.85
24-04-02 07:15:42.977 - INFO: Train epoch 357: [64000/94637 (68%)] Step: [2110651] | Lr: 0.000100 | Loss: 1.2796 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.43
24-04-02 07:16:18.978 - INFO: Train epoch 357: [65600/94637 (69%)] Step: [2110751] | Lr: 0.000100 | Loss: 0.8870 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 92.77
24-04-02 07:16:55.005 - INFO: Train epoch 357: [67200/94637 (71%)] Step: [2110851] | Lr: 0.000100 | Loss: 1.6523 | MSE loss: 0.0005 | Bpp loss: 0.86 | Aux loss: 88.77
24-04-02 07:17:30.656 - INFO: Train epoch 357: [68800/94637 (73%)] Step: [2110951] | Lr: 0.000100 | Loss: 0.8601 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 91.45
24-04-02 07:18:05.669 - INFO: Train epoch 357: [70400/94637 (74%)] Step: [2111051] | Lr: 0.000100 | Loss: 0.8845 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 92.36
24-04-02 07:18:40.605 - INFO: Train epoch 357: [72000/94637 (76%)] Step: [2111151] | Lr: 0.000100 | Loss: 2.1005 | MSE loss: 0.0006 | Bpp loss: 1.15 | Aux loss: 87.23
24-04-02 07:19:17.304 - INFO: Train epoch 357: [73600/94637 (78%)] Step: [2111251] | Lr: 0.000100 | Loss: 1.5335 | MSE loss: 0.0003 | Bpp loss: 0.99 | Aux loss: 87.95
24-04-02 07:19:52.081 - INFO: Train epoch 357: [75200/94637 (79%)] Step: [2111351] | Lr: 0.000100 | Loss: 1.0616 | MSE loss: 0.0003 | Bpp loss: 0.57 | Aux loss: 93.00
24-04-02 07:20:27.122 - INFO: Train epoch 357: [76800/94637 (81%)] Step: [2111451] | Lr: 0.000100 | Loss: 1.2086 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 87.75
24-04-02 07:21:01.929 - INFO: Train epoch 357: [78400/94637 (83%)] Step: [2111551] | Lr: 0.000100 | Loss: 1.3267 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 92.33
24-04-02 07:21:37.243 - INFO: Train epoch 357: [80000/94637 (85%)] Step: [2111651] | Lr: 0.000100 | Loss: 1.4070 | MSE loss: 0.0004 | Bpp loss: 0.81 | Aux loss: 91.68
24-04-02 07:22:12.335 - INFO: Train epoch 357: [81600/94637 (86%)] Step: [2111751] | Lr: 0.000100 | Loss: 1.2275 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 91.69
24-04-02 07:22:47.745 - INFO: Train epoch 357: [83200/94637 (88%)] Step: [2111851] | Lr: 0.000100 | Loss: 1.0799 | MSE loss: 0.0003 | Bpp loss: 0.63 | Aux loss: 89.44
24-04-02 07:23:23.469 - INFO: Train epoch 357: [84800/94637 (90%)] Step: [2111951] | Lr: 0.000100 | Loss: 1.1215 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 84.42
24-04-02 07:23:58.771 - INFO: Train epoch 357: [86400/94637 (91%)] Step: [2112051] | Lr: 0.000100 | Loss: 0.8582 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 84.55
24-04-02 07:24:33.971 - INFO: Train epoch 357: [88000/94637 (93%)] Step: [2112151] | Lr: 0.000100 | Loss: 1.4924 | MSE loss: 0.0003 | Bpp loss: 0.96 | Aux loss: 92.79
24-04-02 07:25:09.203 - INFO: Train epoch 357: [89600/94637 (95%)] Step: [2112251] | Lr: 0.000100 | Loss: 0.8139 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 89.59
24-04-02 07:25:44.980 - INFO: Train epoch 357: [91200/94637 (96%)] Step: [2112351] | Lr: 0.000100 | Loss: 0.9717 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 87.67
24-04-02 07:26:20.228 - INFO: Train epoch 357: [92800/94637 (98%)] Step: [2112451] | Lr: 0.000100 | Loss: 1.4211 | MSE loss: 0.0004 | Bpp loss: 0.82 | Aux loss: 90.45
24-04-02 07:26:58.373 - INFO: Train epoch 357: [94400/94637 (100%)] Step: [2112551] | Lr: 0.000100 | Loss: 1.7322 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 87.92
24-04-02 07:27:20.584 - INFO: Learning rate: 0.0001
24-04-02 07:27:21.403 - INFO: Train epoch 358: [    0/94637 (0%)] Step: [2112566] | Lr: 0.000100 | Loss: 1.6639 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 83.79
24-04-02 07:27:55.342 - INFO: Train epoch 358: [ 1600/94637 (2%)] Step: [2112666] | Lr: 0.000100 | Loss: 0.8285 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 95.17
24-04-02 07:28:29.137 - INFO: Train epoch 358: [ 3200/94637 (3%)] Step: [2112766] | Lr: 0.000100 | Loss: 1.1578 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 85.73
24-04-02 07:29:03.283 - INFO: Train epoch 358: [ 4800/94637 (5%)] Step: [2112866] | Lr: 0.000100 | Loss: 1.2320 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 90.28
24-04-02 07:29:38.374 - INFO: Train epoch 358: [ 6400/94637 (7%)] Step: [2112966] | Lr: 0.000100 | Loss: 1.1483 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 93.59
24-04-02 07:30:13.308 - INFO: Train epoch 358: [ 8000/94637 (8%)] Step: [2113066] | Lr: 0.000100 | Loss: 1.7208 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 87.81
24-04-02 07:30:48.057 - INFO: Train epoch 358: [ 9600/94637 (10%)] Step: [2113166] | Lr: 0.000100 | Loss: 1.0724 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 91.06
24-04-02 07:31:23.073 - INFO: Train epoch 358: [11200/94637 (12%)] Step: [2113266] | Lr: 0.000100 | Loss: 1.4262 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 88.50
24-04-02 07:31:57.949 - INFO: Train epoch 358: [12800/94637 (14%)] Step: [2113366] | Lr: 0.000100 | Loss: 1.1163 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 93.26
24-04-02 07:32:32.586 - INFO: Train epoch 358: [14400/94637 (15%)] Step: [2113466] | Lr: 0.000100 | Loss: 0.8601 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 84.39
24-04-02 07:33:07.318 - INFO: Train epoch 358: [16000/94637 (17%)] Step: [2113566] | Lr: 0.000100 | Loss: 1.3609 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 84.79
24-04-02 07:33:41.836 - INFO: Train epoch 358: [17600/94637 (19%)] Step: [2113666] | Lr: 0.000100 | Loss: 1.2998 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 92.10
24-04-02 07:34:15.657 - INFO: Train epoch 358: [19200/94637 (20%)] Step: [2113766] | Lr: 0.000100 | Loss: 1.2333 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 91.12
24-04-02 07:34:50.784 - INFO: Train epoch 358: [20800/94637 (22%)] Step: [2113866] | Lr: 0.000100 | Loss: 1.7705 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 89.68
24-04-02 07:35:25.913 - INFO: Train epoch 358: [22400/94637 (24%)] Step: [2113966] | Lr: 0.000100 | Loss: 1.9853 | MSE loss: 0.0005 | Bpp loss: 1.22 | Aux loss: 89.79
24-04-02 07:36:00.649 - INFO: Train epoch 358: [24000/94637 (25%)] Step: [2114066] | Lr: 0.000100 | Loss: 1.3251 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 88.31
24-04-02 07:36:36.141 - INFO: Train epoch 358: [25600/94637 (27%)] Step: [2114166] | Lr: 0.000100 | Loss: 1.0513 | MSE loss: 0.0003 | Bpp loss: 0.61 | Aux loss: 91.80
24-04-02 07:37:11.185 - INFO: Train epoch 358: [27200/94637 (29%)] Step: [2114266] | Lr: 0.000100 | Loss: 1.5390 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 80.13
24-04-02 07:37:45.575 - INFO: Train epoch 358: [28800/94637 (30%)] Step: [2114366] | Lr: 0.000100 | Loss: 1.7475 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 83.86
24-04-02 07:38:20.948 - INFO: Train epoch 358: [30400/94637 (32%)] Step: [2114466] | Lr: 0.000100 | Loss: 1.5881 | MSE loss: 0.0004 | Bpp loss: 0.91 | Aux loss: 90.79
24-04-02 07:38:56.414 - INFO: Train epoch 358: [32000/94637 (34%)] Step: [2114566] | Lr: 0.000100 | Loss: 0.8343 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 86.08
24-04-02 07:39:30.755 - INFO: Train epoch 358: [33600/94637 (36%)] Step: [2114666] | Lr: 0.000100 | Loss: 2.0267 | MSE loss: 0.0005 | Bpp loss: 1.23 | Aux loss: 93.63
24-04-02 07:40:05.713 - INFO: Train epoch 358: [35200/94637 (37%)] Step: [2114766] | Lr: 0.000100 | Loss: 1.3331 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 90.57
24-04-02 07:40:40.412 - INFO: Train epoch 358: [36800/94637 (39%)] Step: [2114866] | Lr: 0.000100 | Loss: 1.0022 | MSE loss: 0.0003 | Bpp loss: 0.52 | Aux loss: 88.40
24-04-02 07:41:14.953 - INFO: Train epoch 358: [38400/94637 (41%)] Step: [2114966] | Lr: 0.000100 | Loss: 1.5327 | MSE loss: 0.0003 | Bpp loss: 1.00 | Aux loss: 88.77
24-04-02 07:41:52.014 - INFO: Train epoch 358: [40000/94637 (42%)] Step: [2115066] | Lr: 0.000100 | Loss: 1.2729 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 95.20
24-04-02 07:42:26.502 - INFO: Train epoch 358: [41600/94637 (44%)] Step: [2115166] | Lr: 0.000100 | Loss: 1.3762 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 89.40
24-04-02 07:43:00.650 - INFO: Train epoch 358: [43200/94637 (46%)] Step: [2115266] | Lr: 0.000100 | Loss: 1.2147 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 81.30
24-04-02 07:43:35.860 - INFO: Train epoch 358: [44800/94637 (47%)] Step: [2115366] | Lr: 0.000100 | Loss: 1.8447 | MSE loss: 0.0005 | Bpp loss: 1.10 | Aux loss: 89.90
24-04-02 07:44:10.091 - INFO: Train epoch 358: [46400/94637 (49%)] Step: [2115466] | Lr: 0.000100 | Loss: 1.1861 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 90.40
24-04-02 07:44:44.945 - INFO: Train epoch 358: [48000/94637 (51%)] Step: [2115566] | Lr: 0.000100 | Loss: 0.9271 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 87.26
24-04-02 07:45:19.692 - INFO: Train epoch 358: [49600/94637 (52%)] Step: [2115666] | Lr: 0.000100 | Loss: 1.4168 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 94.51
24-04-02 07:45:54.888 - INFO: Train epoch 358: [51200/94637 (54%)] Step: [2115766] | Lr: 0.000100 | Loss: 1.1053 | MSE loss: 0.0002 | Bpp loss: 0.74 | Aux loss: 93.74
24-04-02 07:46:29.654 - INFO: Train epoch 358: [52800/94637 (56%)] Step: [2115866] | Lr: 0.000100 | Loss: 0.9191 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 93.07
24-04-02 07:47:03.842 - INFO: Train epoch 358: [54400/94637 (57%)] Step: [2115966] | Lr: 0.000100 | Loss: 1.1380 | MSE loss: 0.0002 | Bpp loss: 0.77 | Aux loss: 84.22
24-04-02 07:47:38.569 - INFO: Train epoch 358: [56000/94637 (59%)] Step: [2116066] | Lr: 0.000100 | Loss: 0.8953 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 90.49
24-04-02 07:48:12.261 - INFO: Train epoch 358: [57600/94637 (61%)] Step: [2116166] | Lr: 0.000100 | Loss: 0.7276 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 86.48
24-04-02 07:48:47.017 - INFO: Train epoch 358: [59200/94637 (63%)] Step: [2116266] | Lr: 0.000100 | Loss: 1.7138 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 88.32
24-04-02 07:49:22.441 - INFO: Train epoch 358: [60800/94637 (64%)] Step: [2116366] | Lr: 0.000100 | Loss: 0.7209 | MSE loss: 0.0001 | Bpp loss: 0.48 | Aux loss: 99.27
24-04-02 07:49:56.859 - INFO: Train epoch 358: [62400/94637 (66%)] Step: [2116466] | Lr: 0.000100 | Loss: 1.3231 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 89.42
24-04-02 07:50:32.405 - INFO: Train epoch 358: [64000/94637 (68%)] Step: [2116566] | Lr: 0.000100 | Loss: 1.3760 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 93.43
24-04-02 07:51:07.191 - INFO: Train epoch 358: [65600/94637 (69%)] Step: [2116666] | Lr: 0.000100 | Loss: 1.5686 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 86.30
24-04-02 07:51:42.476 - INFO: Train epoch 358: [67200/94637 (71%)] Step: [2116766] | Lr: 0.000100 | Loss: 1.5823 | MSE loss: 0.0004 | Bpp loss: 0.98 | Aux loss: 93.12
24-04-02 07:52:17.634 - INFO: Train epoch 358: [68800/94637 (73%)] Step: [2116866] | Lr: 0.000100 | Loss: 1.4648 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 91.51
24-04-02 07:52:53.047 - INFO: Train epoch 358: [70400/94637 (74%)] Step: [2116966] | Lr: 0.000100 | Loss: 1.3118 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 89.98
24-04-02 07:53:27.611 - INFO: Train epoch 358: [72000/94637 (76%)] Step: [2117066] | Lr: 0.000100 | Loss: 1.0969 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 89.98
24-04-02 07:54:01.902 - INFO: Train epoch 358: [73600/94637 (78%)] Step: [2117166] | Lr: 0.000100 | Loss: 1.5446 | MSE loss: 0.0004 | Bpp loss: 0.95 | Aux loss: 85.31
24-04-02 07:54:36.111 - INFO: Train epoch 358: [75200/94637 (79%)] Step: [2117266] | Lr: 0.000100 | Loss: 1.2269 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 95.26
24-04-02 07:55:10.875 - INFO: Train epoch 358: [76800/94637 (81%)] Step: [2117366] | Lr: 0.000100 | Loss: 1.7284 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 89.12
24-04-02 07:55:45.662 - INFO: Train epoch 358: [78400/94637 (83%)] Step: [2117466] | Lr: 0.000100 | Loss: 0.9739 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 90.45
24-04-02 07:56:22.366 - INFO: Train epoch 358: [80000/94637 (85%)] Step: [2117566] | Lr: 0.000100 | Loss: 1.6974 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 86.62
24-04-02 07:56:56.461 - INFO: Train epoch 358: [81600/94637 (86%)] Step: [2117666] | Lr: 0.000100 | Loss: 1.6458 | MSE loss: 0.0004 | Bpp loss: 0.95 | Aux loss: 91.23
24-04-02 07:57:31.017 - INFO: Train epoch 358: [83200/94637 (88%)] Step: [2117766] | Lr: 0.000100 | Loss: 1.9246 | MSE loss: 0.0006 | Bpp loss: 1.03 | Aux loss: 87.17
24-04-02 07:58:05.266 - INFO: Train epoch 358: [84800/94637 (90%)] Step: [2117866] | Lr: 0.000100 | Loss: 1.2082 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 92.87
24-04-02 07:58:39.793 - INFO: Train epoch 358: [86400/94637 (91%)] Step: [2117966] | Lr: 0.000100 | Loss: 1.1468 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 85.41
24-04-02 07:59:14.867 - INFO: Train epoch 358: [88000/94637 (93%)] Step: [2118066] | Lr: 0.000100 | Loss: 1.5067 | MSE loss: 0.0003 | Bpp loss: 0.96 | Aux loss: 91.01
24-04-02 07:59:50.470 - INFO: Train epoch 358: [89600/94637 (95%)] Step: [2118166] | Lr: 0.000100 | Loss: 1.1559 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 85.36
24-04-02 08:00:25.841 - INFO: Train epoch 358: [91200/94637 (96%)] Step: [2118266] | Lr: 0.000100 | Loss: 1.3588 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 89.93
24-04-02 08:00:59.379 - INFO: Train epoch 358: [92800/94637 (98%)] Step: [2118366] | Lr: 0.000100 | Loss: 0.8403 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 89.04
24-04-02 08:01:33.513 - INFO: Train epoch 358: [94400/94637 (100%)] Step: [2118466] | Lr: 0.000100 | Loss: 1.2259 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.88
24-04-02 08:01:50.194 - INFO: Learning rate: 0.0001
24-04-02 08:01:51.053 - INFO: Train epoch 359: [    0/94637 (0%)] Step: [2118481] | Lr: 0.000100 | Loss: 1.4782 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 85.94
24-04-02 08:02:25.861 - INFO: Train epoch 359: [ 1600/94637 (2%)] Step: [2118581] | Lr: 0.000100 | Loss: 1.4575 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 88.14
24-04-02 08:03:00.202 - INFO: Train epoch 359: [ 3200/94637 (3%)] Step: [2118681] | Lr: 0.000100 | Loss: 1.7339 | MSE loss: 0.0005 | Bpp loss: 0.95 | Aux loss: 84.40
24-04-02 08:03:33.823 - INFO: Train epoch 359: [ 4800/94637 (5%)] Step: [2118781] | Lr: 0.000100 | Loss: 1.3283 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 84.61
24-04-02 08:04:07.862 - INFO: Train epoch 359: [ 6400/94637 (7%)] Step: [2118881] | Lr: 0.000100 | Loss: 0.9042 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 90.73
24-04-02 08:04:41.459 - INFO: Train epoch 359: [ 8000/94637 (8%)] Step: [2118981] | Lr: 0.000100 | Loss: 1.6114 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 89.92
24-04-02 08:05:15.312 - INFO: Train epoch 359: [ 9600/94637 (10%)] Step: [2119081] | Lr: 0.000100 | Loss: 1.2913 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 88.64
24-04-02 08:05:50.273 - INFO: Train epoch 359: [11200/94637 (12%)] Step: [2119181] | Lr: 0.000100 | Loss: 0.7258 | MSE loss: 0.0001 | Bpp loss: 0.49 | Aux loss: 95.13
24-04-02 08:06:23.443 - INFO: Train epoch 359: [12800/94637 (14%)] Step: [2119281] | Lr: 0.000100 | Loss: 1.4649 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 95.40
24-04-02 08:06:57.245 - INFO: Train epoch 359: [14400/94637 (15%)] Step: [2119381] | Lr: 0.000100 | Loss: 1.9827 | MSE loss: 0.0004 | Bpp loss: 1.25 | Aux loss: 90.39
24-04-02 08:07:31.632 - INFO: Train epoch 359: [16000/94637 (17%)] Step: [2119481] | Lr: 0.000100 | Loss: 1.3461 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 90.31
24-04-02 08:08:05.980 - INFO: Train epoch 359: [17600/94637 (19%)] Step: [2119581] | Lr: 0.000100 | Loss: 1.0555 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 83.59
24-04-02 08:08:40.041 - INFO: Train epoch 359: [19200/94637 (20%)] Step: [2119681] | Lr: 0.000100 | Loss: 1.3624 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 94.76
24-04-02 08:09:15.212 - INFO: Train epoch 359: [20800/94637 (22%)] Step: [2119781] | Lr: 0.000100 | Loss: 1.1298 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 87.69
24-04-02 08:09:50.288 - INFO: Train epoch 359: [22400/94637 (24%)] Step: [2119881] | Lr: 0.000100 | Loss: 1.0669 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 90.96
24-04-02 08:10:25.276 - INFO: Train epoch 359: [24000/94637 (25%)] Step: [2119981] | Lr: 0.000100 | Loss: 1.3512 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 83.95
24-04-02 08:11:02.542 - INFO: Train epoch 359: [25600/94637 (27%)] Step: [2120081] | Lr: 0.000100 | Loss: 1.2303 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 90.09
24-04-02 08:11:37.972 - INFO: Train epoch 359: [27200/94637 (29%)] Step: [2120181] | Lr: 0.000100 | Loss: 0.8520 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 92.77
24-04-02 08:12:12.664 - INFO: Train epoch 359: [28800/94637 (30%)] Step: [2120281] | Lr: 0.000100 | Loss: 0.7695 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 86.04
24-04-02 08:12:47.888 - INFO: Train epoch 359: [30400/94637 (32%)] Step: [2120381] | Lr: 0.000100 | Loss: 1.2966 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 94.88
24-04-02 08:13:22.831 - INFO: Train epoch 359: [32000/94637 (34%)] Step: [2120481] | Lr: 0.000100 | Loss: 0.8698 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 89.65
24-04-02 08:13:56.325 - INFO: Train epoch 359: [33600/94637 (36%)] Step: [2120581] | Lr: 0.000100 | Loss: 2.0472 | MSE loss: 0.0004 | Bpp loss: 1.33 | Aux loss: 88.61
24-04-02 08:14:29.769 - INFO: Train epoch 359: [35200/94637 (37%)] Step: [2120681] | Lr: 0.000100 | Loss: 1.2138 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 91.75
24-04-02 08:15:04.084 - INFO: Train epoch 359: [36800/94637 (39%)] Step: [2120781] | Lr: 0.000100 | Loss: 1.0721 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 86.94
24-04-02 08:15:39.840 - INFO: Train epoch 359: [38400/94637 (41%)] Step: [2120881] | Lr: 0.000100 | Loss: 1.1561 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 88.92
24-04-02 08:16:15.135 - INFO: Train epoch 359: [40000/94637 (42%)] Step: [2120981] | Lr: 0.000100 | Loss: 1.5518 | MSE loss: 0.0004 | Bpp loss: 0.97 | Aux loss: 93.47
24-04-02 08:16:50.728 - INFO: Train epoch 359: [41600/94637 (44%)] Step: [2121081] | Lr: 0.000100 | Loss: 1.1814 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 94.53
24-04-02 08:17:25.903 - INFO: Train epoch 359: [43200/94637 (46%)] Step: [2121181] | Lr: 0.000100 | Loss: 1.3476 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 85.00
24-04-02 08:18:00.711 - INFO: Train epoch 359: [44800/94637 (47%)] Step: [2121281] | Lr: 0.000100 | Loss: 1.3027 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 94.11
24-04-02 08:18:36.879 - INFO: Train epoch 359: [46400/94637 (49%)] Step: [2121381] | Lr: 0.000100 | Loss: 0.8957 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 90.50
24-04-02 08:19:11.522 - INFO: Train epoch 359: [48000/94637 (51%)] Step: [2121481] | Lr: 0.000100 | Loss: 1.1550 | MSE loss: 0.0002 | Bpp loss: 0.75 | Aux loss: 89.94
24-04-02 08:19:46.617 - INFO: Train epoch 359: [49600/94637 (52%)] Step: [2121581] | Lr: 0.000100 | Loss: 1.5459 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 92.52
24-04-02 08:20:21.856 - INFO: Train epoch 359: [51200/94637 (54%)] Step: [2121681] | Lr: 0.000100 | Loss: 1.5948 | MSE loss: 0.0004 | Bpp loss: 0.98 | Aux loss: 87.12
24-04-02 08:20:57.246 - INFO: Train epoch 359: [52800/94637 (56%)] Step: [2121781] | Lr: 0.000100 | Loss: 1.4922 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 93.04
24-04-02 08:21:32.722 - INFO: Train epoch 359: [54400/94637 (57%)] Step: [2121881] | Lr: 0.000100 | Loss: 0.9445 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 90.83
24-04-02 08:22:07.937 - INFO: Train epoch 359: [56000/94637 (59%)] Step: [2121981] | Lr: 0.000100 | Loss: 1.7908 | MSE loss: 0.0004 | Bpp loss: 1.09 | Aux loss: 90.34
24-04-02 08:22:42.462 - INFO: Train epoch 359: [57600/94637 (61%)] Step: [2122081] | Lr: 0.000100 | Loss: 0.8851 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 92.17
24-04-02 08:23:17.251 - INFO: Train epoch 359: [59200/94637 (63%)] Step: [2122181] | Lr: 0.000100 | Loss: 0.8853 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 96.18
24-04-02 08:23:53.108 - INFO: Train epoch 359: [60800/94637 (64%)] Step: [2122281] | Lr: 0.000100 | Loss: 1.0090 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 90.14
24-04-02 08:24:27.868 - INFO: Train epoch 359: [62400/94637 (66%)] Step: [2122381] | Lr: 0.000100 | Loss: 1.4891 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 97.15
24-04-02 08:25:02.643 - INFO: Train epoch 359: [64000/94637 (68%)] Step: [2122481] | Lr: 0.000100 | Loss: 1.1533 | MSE loss: 0.0002 | Bpp loss: 0.76 | Aux loss: 89.03
24-04-02 08:25:39.870 - INFO: Train epoch 359: [65600/94637 (69%)] Step: [2122581] | Lr: 0.000100 | Loss: 0.9881 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 93.67
24-04-02 08:26:15.496 - INFO: Train epoch 359: [67200/94637 (71%)] Step: [2122681] | Lr: 0.000100 | Loss: 1.1629 | MSE loss: 0.0002 | Bpp loss: 0.76 | Aux loss: 87.71
24-04-02 08:26:50.633 - INFO: Train epoch 359: [68800/94637 (73%)] Step: [2122781] | Lr: 0.000100 | Loss: 1.2174 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 90.03
24-04-02 08:27:25.749 - INFO: Train epoch 359: [70400/94637 (74%)] Step: [2122881] | Lr: 0.000100 | Loss: 1.4540 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 92.22
24-04-02 08:28:01.373 - INFO: Train epoch 359: [72000/94637 (76%)] Step: [2122981] | Lr: 0.000100 | Loss: 1.3766 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 90.61
24-04-02 08:28:35.995 - INFO: Train epoch 359: [73600/94637 (78%)] Step: [2123081] | Lr: 0.000100 | Loss: 0.9948 | MSE loss: 0.0003 | Bpp loss: 0.58 | Aux loss: 91.11
24-04-02 08:29:10.522 - INFO: Train epoch 359: [75200/94637 (79%)] Step: [2123181] | Lr: 0.000100 | Loss: 1.3168 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.72
24-04-02 08:29:44.848 - INFO: Train epoch 359: [76800/94637 (81%)] Step: [2123281] | Lr: 0.000100 | Loss: 1.0862 | MSE loss: 0.0003 | Bpp loss: 0.59 | Aux loss: 91.59
24-04-02 08:30:18.538 - INFO: Train epoch 359: [78400/94637 (83%)] Step: [2123381] | Lr: 0.000100 | Loss: 1.7068 | MSE loss: 0.0004 | Bpp loss: 0.99 | Aux loss: 92.54
24-04-02 08:30:53.499 - INFO: Train epoch 359: [80000/94637 (85%)] Step: [2123481] | Lr: 0.000100 | Loss: 1.2177 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 94.20
24-04-02 08:31:27.862 - INFO: Train epoch 359: [81600/94637 (86%)] Step: [2123581] | Lr: 0.000100 | Loss: 1.3344 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 89.87
24-04-02 08:32:02.247 - INFO: Train epoch 359: [83200/94637 (88%)] Step: [2123681] | Lr: 0.000100 | Loss: 1.9989 | MSE loss: 0.0004 | Bpp loss: 1.27 | Aux loss: 99.73
24-04-02 08:32:36.180 - INFO: Train epoch 359: [84800/94637 (90%)] Step: [2123781] | Lr: 0.000100 | Loss: 1.3019 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 90.04
24-04-02 08:33:10.956 - INFO: Train epoch 359: [86400/94637 (91%)] Step: [2123881] | Lr: 0.000100 | Loss: 1.0910 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 95.15
24-04-02 08:33:45.882 - INFO: Train epoch 359: [88000/94637 (93%)] Step: [2123981] | Lr: 0.000100 | Loss: 1.9114 | MSE loss: 0.0005 | Bpp loss: 1.07 | Aux loss: 86.51
24-04-02 08:34:20.191 - INFO: Train epoch 359: [89600/94637 (95%)] Step: [2124081] | Lr: 0.000100 | Loss: 1.7882 | MSE loss: 0.0005 | Bpp loss: 1.02 | Aux loss: 85.16
24-04-02 08:34:55.025 - INFO: Train epoch 359: [91200/94637 (96%)] Step: [2124181] | Lr: 0.000100 | Loss: 1.0884 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 95.16
24-04-02 08:35:29.950 - INFO: Train epoch 359: [92800/94637 (98%)] Step: [2124281] | Lr: 0.000100 | Loss: 1.2610 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 87.45
24-04-02 08:36:04.863 - INFO: Train epoch 359: [94400/94637 (100%)] Step: [2124381] | Lr: 0.000100 | Loss: 0.8897 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 93.29
24-04-02 08:36:21.467 - INFO: Learning rate: 0.0001
24-04-02 08:36:22.246 - INFO: Train epoch 360: [    0/94637 (0%)] Step: [2124396] | Lr: 0.000100 | Loss: 1.9127 | MSE loss: 0.0005 | Bpp loss: 1.13 | Aux loss: 94.77
24-04-02 08:36:56.166 - INFO: Train epoch 360: [ 1600/94637 (2%)] Step: [2124496] | Lr: 0.000100 | Loss: 0.6793 | MSE loss: 0.0001 | Bpp loss: 0.45 | Aux loss: 91.55
24-04-02 08:37:30.588 - INFO: Train epoch 360: [ 3200/94637 (3%)] Step: [2124596] | Lr: 0.000100 | Loss: 1.2111 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 90.92
24-04-02 08:38:05.756 - INFO: Train epoch 360: [ 4800/94637 (5%)] Step: [2124696] | Lr: 0.000100 | Loss: 0.9683 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 97.36
24-04-02 08:38:41.543 - INFO: Train epoch 360: [ 6400/94637 (7%)] Step: [2124796] | Lr: 0.000100 | Loss: 1.1880 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 94.78
24-04-02 08:39:15.738 - INFO: Train epoch 360: [ 8000/94637 (8%)] Step: [2124896] | Lr: 0.000100 | Loss: 2.6531 | MSE loss: 0.0007 | Bpp loss: 1.57 | Aux loss: 89.62
24-04-02 08:39:50.510 - INFO: Train epoch 360: [ 9600/94637 (10%)] Step: [2124996] | Lr: 0.000100 | Loss: 0.9617 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 86.83
24-04-02 08:40:27.970 - INFO: Train epoch 360: [11200/94637 (12%)] Step: [2125096] | Lr: 0.000100 | Loss: 1.0115 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 88.43
24-04-02 08:41:03.571 - INFO: Train epoch 360: [12800/94637 (14%)] Step: [2125196] | Lr: 0.000100 | Loss: 1.2721 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 98.06
24-04-02 08:41:38.491 - INFO: Train epoch 360: [14400/94637 (15%)] Step: [2125296] | Lr: 0.000100 | Loss: 1.2543 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 87.18
24-04-02 08:42:13.011 - INFO: Train epoch 360: [16000/94637 (17%)] Step: [2125396] | Lr: 0.000100 | Loss: 1.0927 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 88.40
24-04-02 08:42:46.525 - INFO: Train epoch 360: [17600/94637 (19%)] Step: [2125496] | Lr: 0.000100 | Loss: 1.1108 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 94.19
24-04-02 08:43:20.872 - INFO: Train epoch 360: [19200/94637 (20%)] Step: [2125596] | Lr: 0.000100 | Loss: 1.4135 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 89.99
24-04-02 08:43:55.395 - INFO: Train epoch 360: [20800/94637 (22%)] Step: [2125696] | Lr: 0.000100 | Loss: 0.6280 | MSE loss: 0.0001 | Bpp loss: 0.39 | Aux loss: 89.05
24-04-02 08:44:29.660 - INFO: Train epoch 360: [22400/94637 (24%)] Step: [2125796] | Lr: 0.000100 | Loss: 0.9347 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 92.60
24-04-02 08:45:04.831 - INFO: Train epoch 360: [24000/94637 (25%)] Step: [2125896] | Lr: 0.000100 | Loss: 0.9905 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 88.34
24-04-02 08:45:39.602 - INFO: Train epoch 360: [25600/94637 (27%)] Step: [2125996] | Lr: 0.000100 | Loss: 0.9226 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 88.16
24-04-02 08:46:15.827 - INFO: Train epoch 360: [27200/94637 (29%)] Step: [2126096] | Lr: 0.000100 | Loss: 1.1340 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 90.26
24-04-02 08:46:50.904 - INFO: Train epoch 360: [28800/94637 (30%)] Step: [2126196] | Lr: 0.000100 | Loss: 0.9283 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 89.00
24-04-02 08:47:25.387 - INFO: Train epoch 360: [30400/94637 (32%)] Step: [2126296] | Lr: 0.000100 | Loss: 1.3265 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 98.16
24-04-02 08:48:01.008 - INFO: Train epoch 360: [32000/94637 (34%)] Step: [2126396] | Lr: 0.000100 | Loss: 1.3566 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 90.69
24-04-02 08:48:35.281 - INFO: Train epoch 360: [33600/94637 (36%)] Step: [2126496] | Lr: 0.000100 | Loss: 1.3373 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 92.40
24-04-02 08:49:10.064 - INFO: Train epoch 360: [35200/94637 (37%)] Step: [2126596] | Lr: 0.000100 | Loss: 1.8086 | MSE loss: 0.0004 | Bpp loss: 1.14 | Aux loss: 95.71
24-04-02 08:49:44.461 - INFO: Train epoch 360: [36800/94637 (39%)] Step: [2126696] | Lr: 0.000100 | Loss: 0.9707 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 87.82
24-04-02 08:50:20.308 - INFO: Train epoch 360: [38400/94637 (41%)] Step: [2126796] | Lr: 0.000100 | Loss: 1.3578 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 84.12
24-04-02 08:50:55.066 - INFO: Train epoch 360: [40000/94637 (42%)] Step: [2126896] | Lr: 0.000100 | Loss: 1.0712 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 91.41
24-04-02 08:51:28.950 - INFO: Train epoch 360: [41600/94637 (44%)] Step: [2126996] | Lr: 0.000100 | Loss: 0.9724 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 89.10
24-04-02 08:52:02.944 - INFO: Train epoch 360: [43200/94637 (46%)] Step: [2127096] | Lr: 0.000100 | Loss: 0.6952 | MSE loss: 0.0001 | Bpp loss: 0.45 | Aux loss: 86.25
24-04-02 08:52:36.650 - INFO: Train epoch 360: [44800/94637 (47%)] Step: [2127196] | Lr: 0.000100 | Loss: 0.8394 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 96.30
24-04-02 08:53:10.429 - INFO: Train epoch 360: [46400/94637 (49%)] Step: [2127296] | Lr: 0.000100 | Loss: 1.6531 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 86.09
24-04-02 08:53:44.044 - INFO: Train epoch 360: [48000/94637 (51%)] Step: [2127396] | Lr: 0.000100 | Loss: 2.0547 | MSE loss: 0.0007 | Bpp loss: 0.89 | Aux loss: 87.54
24-04-02 08:54:18.769 - INFO: Train epoch 360: [49600/94637 (52%)] Step: [2127496] | Lr: 0.000100 | Loss: 2.1418 | MSE loss: 0.0006 | Bpp loss: 1.17 | Aux loss: 86.49
24-04-02 08:54:55.243 - INFO: Train epoch 360: [51200/94637 (54%)] Step: [2127596] | Lr: 0.000100 | Loss: 1.3032 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 92.02
24-04-02 08:55:29.617 - INFO: Train epoch 360: [52800/94637 (56%)] Step: [2127696] | Lr: 0.000100 | Loss: 0.7281 | MSE loss: 0.0002 | Bpp loss: 0.46 | Aux loss: 87.17
24-04-02 08:56:05.026 - INFO: Train epoch 360: [54400/94637 (57%)] Step: [2127796] | Lr: 0.000100 | Loss: 1.7050 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 89.59
24-04-02 08:56:40.053 - INFO: Train epoch 360: [56000/94637 (59%)] Step: [2127896] | Lr: 0.000100 | Loss: 1.2201 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 89.59
24-04-02 08:57:15.770 - INFO: Train epoch 360: [57600/94637 (61%)] Step: [2127996] | Lr: 0.000100 | Loss: 1.6265 | MSE loss: 0.0003 | Bpp loss: 1.14 | Aux loss: 83.58
24-04-02 08:57:51.080 - INFO: Train epoch 360: [59200/94637 (63%)] Step: [2128096] | Lr: 0.000100 | Loss: 1.9716 | MSE loss: 0.0005 | Bpp loss: 1.17 | Aux loss: 93.09
24-04-02 08:58:27.027 - INFO: Train epoch 360: [60800/94637 (64%)] Step: [2128196] | Lr: 0.000100 | Loss: 1.6680 | MSE loss: 0.0004 | Bpp loss: 1.07 | Aux loss: 91.40
24-04-02 08:59:02.584 - INFO: Train epoch 360: [62400/94637 (66%)] Step: [2128296] | Lr: 0.000100 | Loss: 0.8611 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 84.41
24-04-02 08:59:37.598 - INFO: Train epoch 360: [64000/94637 (68%)] Step: [2128396] | Lr: 0.000100 | Loss: 1.4192 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 80.94
24-04-02 09:00:13.258 - INFO: Train epoch 360: [65600/94637 (69%)] Step: [2128496] | Lr: 0.000100 | Loss: 1.3763 | MSE loss: 0.0004 | Bpp loss: 0.80 | Aux loss: 76.15
24-04-02 09:00:48.132 - INFO: Train epoch 360: [67200/94637 (71%)] Step: [2128596] | Lr: 0.000100 | Loss: 1.3729 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 84.68
24-04-02 09:01:23.703 - INFO: Train epoch 360: [68800/94637 (73%)] Step: [2128696] | Lr: 0.000100 | Loss: 1.9950 | MSE loss: 0.0005 | Bpp loss: 1.24 | Aux loss: 79.02
24-04-02 09:01:58.475 - INFO: Train epoch 360: [70400/94637 (74%)] Step: [2128796] | Lr: 0.000100 | Loss: 1.5085 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 83.46
24-04-02 09:02:34.251 - INFO: Train epoch 360: [72000/94637 (76%)] Step: [2128896] | Lr: 0.000100 | Loss: 1.0059 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 84.27
24-04-02 09:03:09.260 - INFO: Train epoch 360: [73600/94637 (78%)] Step: [2128996] | Lr: 0.000100 | Loss: 0.8619 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 86.36
24-04-02 09:03:44.759 - INFO: Train epoch 360: [75200/94637 (79%)] Step: [2129096] | Lr: 0.000100 | Loss: 1.4196 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 85.91
24-04-02 09:04:19.336 - INFO: Train epoch 360: [76800/94637 (81%)] Step: [2129196] | Lr: 0.000100 | Loss: 1.2510 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 76.33
24-04-02 09:04:54.568 - INFO: Train epoch 360: [78400/94637 (83%)] Step: [2129296] | Lr: 0.000100 | Loss: 1.9093 | MSE loss: 0.0004 | Bpp loss: 1.18 | Aux loss: 88.05
24-04-02 09:05:29.946 - INFO: Train epoch 360: [80000/94637 (85%)] Step: [2129396] | Lr: 0.000100 | Loss: 1.4667 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 82.64
24-04-02 09:06:04.361 - INFO: Train epoch 360: [81600/94637 (86%)] Step: [2129496] | Lr: 0.000100 | Loss: 1.2531 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 85.75
24-04-02 09:06:40.525 - INFO: Train epoch 360: [83200/94637 (88%)] Step: [2129596] | Lr: 0.000100 | Loss: 2.3791 | MSE loss: 0.0007 | Bpp loss: 1.30 | Aux loss: 84.58
24-04-02 09:07:15.484 - INFO: Train epoch 360: [84800/94637 (90%)] Step: [2129696] | Lr: 0.000100 | Loss: 2.4567 | MSE loss: 0.0007 | Bpp loss: 1.31 | Aux loss: 81.34
24-04-02 09:07:49.432 - INFO: Train epoch 360: [86400/94637 (91%)] Step: [2129796] | Lr: 0.000100 | Loss: 1.2501 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.53
24-04-02 09:08:24.326 - INFO: Train epoch 360: [88000/94637 (93%)] Step: [2129896] | Lr: 0.000100 | Loss: 1.1539 | MSE loss: 0.0002 | Bpp loss: 0.76 | Aux loss: 81.40
24-04-02 09:08:58.522 - INFO: Train epoch 360: [89600/94637 (95%)] Step: [2129996] | Lr: 0.000100 | Loss: 1.2626 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 87.49
24-04-02 09:09:35.746 - INFO: Train epoch 360: [91200/94637 (96%)] Step: [2130096] | Lr: 0.000100 | Loss: 1.0384 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 83.23
24-04-02 09:10:11.288 - INFO: Train epoch 360: [92800/94637 (98%)] Step: [2130196] | Lr: 0.000100 | Loss: 1.5077 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 82.22
24-04-02 09:10:46.882 - INFO: Train epoch 360: [94400/94637 (100%)] Step: [2130296] | Lr: 0.000100 | Loss: 1.2135 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 86.25
24-04-02 09:11:03.479 - INFO: Learning rate: 0.0001
24-04-02 09:11:05.029 - INFO: Train epoch 361: [    0/94637 (0%)] Step: [2130311] | Lr: 0.000100 | Loss: 1.2762 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 85.54
24-04-02 09:11:39.650 - INFO: Train epoch 361: [ 1600/94637 (2%)] Step: [2130411] | Lr: 0.000100 | Loss: 1.0588 | MSE loss: 0.0002 | Bpp loss: 0.68 | Aux loss: 84.78
24-04-02 09:12:14.280 - INFO: Train epoch 361: [ 3200/94637 (3%)] Step: [2130511] | Lr: 0.000100 | Loss: 1.1498 | MSE loss: 0.0002 | Bpp loss: 0.77 | Aux loss: 88.45
24-04-02 09:12:48.406 - INFO: Train epoch 361: [ 4800/94637 (5%)] Step: [2130611] | Lr: 0.000100 | Loss: 1.2669 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 76.89
24-04-02 09:13:23.033 - INFO: Train epoch 361: [ 6400/94637 (7%)] Step: [2130711] | Lr: 0.000100 | Loss: 0.8137 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 77.87
24-04-02 09:13:56.789 - INFO: Train epoch 361: [ 8000/94637 (8%)] Step: [2130811] | Lr: 0.000100 | Loss: 1.3000 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 82.17
24-04-02 09:14:31.809 - INFO: Train epoch 361: [ 9600/94637 (10%)] Step: [2130911] | Lr: 0.000100 | Loss: 1.8996 | MSE loss: 0.0005 | Bpp loss: 1.16 | Aux loss: 80.70
24-04-02 09:15:05.876 - INFO: Train epoch 361: [11200/94637 (12%)] Step: [2131011] | Lr: 0.000100 | Loss: 1.8139 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 75.33
24-04-02 09:15:40.422 - INFO: Train epoch 361: [12800/94637 (14%)] Step: [2131111] | Lr: 0.000100 | Loss: 1.4211 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 81.22
24-04-02 09:16:16.104 - INFO: Train epoch 361: [14400/94637 (15%)] Step: [2131211] | Lr: 0.000100 | Loss: 1.1167 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 75.23
24-04-02 09:16:50.511 - INFO: Train epoch 361: [16000/94637 (17%)] Step: [2131311] | Lr: 0.000100 | Loss: 1.3452 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 77.56
24-04-02 09:17:25.291 - INFO: Train epoch 361: [17600/94637 (19%)] Step: [2131411] | Lr: 0.000100 | Loss: 0.9125 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 78.11
24-04-02 09:18:00.341 - INFO: Train epoch 361: [19200/94637 (20%)] Step: [2131511] | Lr: 0.000100 | Loss: 1.3836 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 79.62
24-04-02 09:18:35.633 - INFO: Train epoch 361: [20800/94637 (22%)] Step: [2131611] | Lr: 0.000100 | Loss: 1.8073 | MSE loss: 0.0005 | Bpp loss: 1.03 | Aux loss: 72.12
24-04-02 09:19:11.085 - INFO: Train epoch 361: [22400/94637 (24%)] Step: [2131711] | Lr: 0.000100 | Loss: 0.9036 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 77.42
24-04-02 09:19:45.822 - INFO: Train epoch 361: [24000/94637 (25%)] Step: [2131811] | Lr: 0.000100 | Loss: 1.0638 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 79.43
24-04-02 09:20:20.905 - INFO: Train epoch 361: [25600/94637 (27%)] Step: [2131911] | Lr: 0.000100 | Loss: 1.2834 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 75.62
24-04-02 09:20:55.148 - INFO: Train epoch 361: [27200/94637 (29%)] Step: [2132011] | Lr: 0.000100 | Loss: 1.2396 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 80.35
24-04-02 09:21:29.711 - INFO: Train epoch 361: [28800/94637 (30%)] Step: [2132111] | Lr: 0.000100 | Loss: 1.4077 | MSE loss: 0.0003 | Bpp loss: 0.86 | Aux loss: 76.89
24-04-02 09:22:04.427 - INFO: Train epoch 361: [30400/94637 (32%)] Step: [2132211] | Lr: 0.000100 | Loss: 1.3271 | MSE loss: 0.0003 | Bpp loss: 0.80 | Aux loss: 80.16
24-04-02 09:22:38.737 - INFO: Train epoch 361: [32000/94637 (34%)] Step: [2132311] | Lr: 0.000100 | Loss: 1.4657 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 75.58
24-04-02 09:23:13.059 - INFO: Train epoch 361: [33600/94637 (36%)] Step: [2132411] | Lr: 0.000100 | Loss: 1.4064 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 87.38
24-04-02 09:23:49.719 - INFO: Train epoch 361: [35200/94637 (37%)] Step: [2132511] | Lr: 0.000100 | Loss: 1.2699 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 81.74
24-04-02 09:24:23.975 - INFO: Train epoch 361: [36800/94637 (39%)] Step: [2132611] | Lr: 0.000100 | Loss: 1.0568 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 83.97
24-04-02 09:24:57.571 - INFO: Train epoch 361: [38400/94637 (41%)] Step: [2132711] | Lr: 0.000100 | Loss: 1.6551 | MSE loss: 0.0004 | Bpp loss: 1.02 | Aux loss: 85.51
24-04-02 09:25:32.657 - INFO: Train epoch 361: [40000/94637 (42%)] Step: [2132811] | Lr: 0.000100 | Loss: 2.1049 | MSE loss: 0.0005 | Bpp loss: 1.30 | Aux loss: 83.04
24-04-02 09:26:06.858 - INFO: Train epoch 361: [41600/94637 (44%)] Step: [2132911] | Lr: 0.000100 | Loss: 1.7068 | MSE loss: 0.0004 | Bpp loss: 1.09 | Aux loss: 76.30
24-04-02 09:26:41.578 - INFO: Train epoch 361: [43200/94637 (46%)] Step: [2133011] | Lr: 0.000100 | Loss: 0.9925 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 82.41
24-04-02 09:27:15.807 - INFO: Train epoch 361: [44800/94637 (47%)] Step: [2133111] | Lr: 0.000100 | Loss: 1.1889 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 83.84
24-04-02 09:27:50.197 - INFO: Train epoch 361: [46400/94637 (49%)] Step: [2133211] | Lr: 0.000100 | Loss: 1.3117 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 80.38
24-04-02 09:28:25.469 - INFO: Train epoch 361: [48000/94637 (51%)] Step: [2133311] | Lr: 0.000100 | Loss: 1.0280 | MSE loss: 0.0003 | Bpp loss: 0.60 | Aux loss: 83.00
24-04-02 09:28:59.819 - INFO: Train epoch 361: [49600/94637 (52%)] Step: [2133411] | Lr: 0.000100 | Loss: 1.0442 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 82.06
24-04-02 09:29:33.780 - INFO: Train epoch 361: [51200/94637 (54%)] Step: [2133511] | Lr: 0.000100 | Loss: 0.9477 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 78.78
24-04-02 09:30:07.874 - INFO: Train epoch 361: [52800/94637 (56%)] Step: [2133611] | Lr: 0.000100 | Loss: 0.7224 | MSE loss: 0.0001 | Bpp loss: 0.48 | Aux loss: 89.39
24-04-02 09:30:42.061 - INFO: Train epoch 361: [54400/94637 (57%)] Step: [2133711] | Lr: 0.000100 | Loss: 0.9590 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 89.65
24-04-02 09:31:16.693 - INFO: Train epoch 361: [56000/94637 (59%)] Step: [2133811] | Lr: 0.000100 | Loss: 1.3241 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 81.05
24-04-02 09:31:50.991 - INFO: Train epoch 361: [57600/94637 (61%)] Step: [2133911] | Lr: 0.000100 | Loss: 1.1236 | MSE loss: 0.0003 | Bpp loss: 0.68 | Aux loss: 84.36
24-04-02 09:32:25.344 - INFO: Train epoch 361: [59200/94637 (63%)] Step: [2134011] | Lr: 0.000100 | Loss: 0.9068 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 81.70
24-04-02 09:32:58.923 - INFO: Train epoch 361: [60800/94637 (64%)] Step: [2134111] | Lr: 0.000100 | Loss: 2.0836 | MSE loss: 0.0005 | Bpp loss: 1.34 | Aux loss: 81.43
24-04-02 09:33:33.676 - INFO: Train epoch 361: [62400/94637 (66%)] Step: [2134211] | Lr: 0.000100 | Loss: 0.7700 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 82.81
24-04-02 09:34:08.423 - INFO: Train epoch 361: [64000/94637 (68%)] Step: [2134311] | Lr: 0.000100 | Loss: 1.3634 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 85.95
24-04-02 09:34:43.407 - INFO: Train epoch 361: [65600/94637 (69%)] Step: [2134411] | Lr: 0.000100 | Loss: 0.7831 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 82.57
24-04-02 09:35:17.592 - INFO: Train epoch 361: [67200/94637 (71%)] Step: [2134511] | Lr: 0.000100 | Loss: 1.1827 | MSE loss: 0.0002 | Bpp loss: 0.79 | Aux loss: 80.90
24-04-02 09:35:52.765 - INFO: Train epoch 361: [68800/94637 (73%)] Step: [2134611] | Lr: 0.000100 | Loss: 1.6600 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 82.22
24-04-02 09:36:27.386 - INFO: Train epoch 361: [70400/94637 (74%)] Step: [2134711] | Lr: 0.000100 | Loss: 1.8560 | MSE loss: 0.0005 | Bpp loss: 1.04 | Aux loss: 83.87
24-04-02 09:37:01.757 - INFO: Train epoch 361: [72000/94637 (76%)] Step: [2134811] | Lr: 0.000100 | Loss: 1.5479 | MSE loss: 0.0004 | Bpp loss: 0.95 | Aux loss: 80.61
24-04-02 09:37:36.511 - INFO: Train epoch 361: [73600/94637 (78%)] Step: [2134911] | Lr: 0.000100 | Loss: 1.2522 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 83.26
24-04-02 09:38:12.937 - INFO: Train epoch 361: [75200/94637 (79%)] Step: [2135011] | Lr: 0.000100 | Loss: 0.9572 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 83.22
24-04-02 09:38:47.827 - INFO: Train epoch 361: [76800/94637 (81%)] Step: [2135111] | Lr: 0.000100 | Loss: 1.1231 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 81.62
24-04-02 09:39:21.998 - INFO: Train epoch 361: [78400/94637 (83%)] Step: [2135211] | Lr: 0.000100 | Loss: 1.1684 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 88.86
24-04-02 09:39:55.547 - INFO: Train epoch 361: [80000/94637 (85%)] Step: [2135311] | Lr: 0.000100 | Loss: 1.0034 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 81.68
24-04-02 09:40:29.323 - INFO: Train epoch 361: [81600/94637 (86%)] Step: [2135411] | Lr: 0.000100 | Loss: 1.4626 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 83.06
24-04-02 09:41:02.357 - INFO: Train epoch 361: [83200/94637 (88%)] Step: [2135511] | Lr: 0.000100 | Loss: 1.2361 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 85.50
24-04-02 09:41:36.499 - INFO: Train epoch 361: [84800/94637 (90%)] Step: [2135611] | Lr: 0.000100 | Loss: 0.6562 | MSE loss: 0.0001 | Bpp loss: 0.43 | Aux loss: 80.09
24-04-02 09:42:10.882 - INFO: Train epoch 361: [86400/94637 (91%)] Step: [2135711] | Lr: 0.000100 | Loss: 1.6973 | MSE loss: 0.0004 | Bpp loss: 1.00 | Aux loss: 84.67
24-04-02 09:42:45.697 - INFO: Train epoch 361: [88000/94637 (93%)] Step: [2135811] | Lr: 0.000100 | Loss: 0.7882 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 81.94
24-04-02 09:43:20.250 - INFO: Train epoch 361: [89600/94637 (95%)] Step: [2135911] | Lr: 0.000100 | Loss: 0.6255 | MSE loss: 0.0001 | Bpp loss: 0.39 | Aux loss: 87.04
24-04-02 09:43:55.767 - INFO: Train epoch 361: [91200/94637 (96%)] Step: [2136011] | Lr: 0.000100 | Loss: 1.0293 | MSE loss: 0.0003 | Bpp loss: 0.62 | Aux loss: 84.50
24-04-02 09:44:30.445 - INFO: Train epoch 361: [92800/94637 (98%)] Step: [2136111] | Lr: 0.000100 | Loss: 1.4819 | MSE loss: 0.0004 | Bpp loss: 0.91 | Aux loss: 82.76
24-04-02 09:45:05.396 - INFO: Train epoch 361: [94400/94637 (100%)] Step: [2136211] | Lr: 0.000100 | Loss: 1.4384 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 79.07
24-04-02 09:45:22.200 - INFO: Learning rate: 0.0001
24-04-02 09:45:23.481 - INFO: Train epoch 362: [    0/94637 (0%)] Step: [2136226] | Lr: 0.000100 | Loss: 0.8302 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 84.91
24-04-02 09:45:58.101 - INFO: Train epoch 362: [ 1600/94637 (2%)] Step: [2136326] | Lr: 0.000100 | Loss: 0.8468 | MSE loss: 0.0002 | Bpp loss: 0.51 | Aux loss: 86.46
24-04-02 09:46:32.303 - INFO: Train epoch 362: [ 3200/94637 (3%)] Step: [2136426] | Lr: 0.000100 | Loss: 1.3399 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 84.92
24-04-02 09:47:07.945 - INFO: Train epoch 362: [ 4800/94637 (5%)] Step: [2136526] | Lr: 0.000100 | Loss: 0.8880 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 86.47
24-04-02 09:47:43.194 - INFO: Train epoch 362: [ 6400/94637 (7%)] Step: [2136626] | Lr: 0.000100 | Loss: 1.5323 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 85.70
24-04-02 09:48:18.637 - INFO: Train epoch 362: [ 8000/94637 (8%)] Step: [2136726] | Lr: 0.000100 | Loss: 1.4416 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 88.51
24-04-02 09:48:52.938 - INFO: Train epoch 362: [ 9600/94637 (10%)] Step: [2136826] | Lr: 0.000100 | Loss: 1.9028 | MSE loss: 0.0005 | Bpp loss: 1.17 | Aux loss: 82.75
24-04-02 09:49:27.345 - INFO: Train epoch 362: [11200/94637 (12%)] Step: [2136926] | Lr: 0.000100 | Loss: 1.4226 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 86.36
24-04-02 09:50:01.584 - INFO: Train epoch 362: [12800/94637 (14%)] Step: [2137026] | Lr: 0.000100 | Loss: 0.5933 | MSE loss: 0.0001 | Bpp loss: 0.39 | Aux loss: 85.59
24-04-02 09:50:36.812 - INFO: Train epoch 362: [14400/94637 (15%)] Step: [2137126] | Lr: 0.000100 | Loss: 1.7355 | MSE loss: 0.0005 | Bpp loss: 0.88 | Aux loss: 83.73
24-04-02 09:51:11.149 - INFO: Train epoch 362: [16000/94637 (17%)] Step: [2137226] | Lr: 0.000100 | Loss: 1.3111 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 85.63
24-04-02 09:51:44.867 - INFO: Train epoch 362: [17600/94637 (19%)] Step: [2137326] | Lr: 0.000100 | Loss: 1.6056 | MSE loss: 0.0004 | Bpp loss: 1.03 | Aux loss: 82.30
24-04-02 09:52:18.796 - INFO: Train epoch 362: [19200/94637 (20%)] Step: [2137426] | Lr: 0.000100 | Loss: 1.9924 | MSE loss: 0.0005 | Bpp loss: 1.19 | Aux loss: 80.90
24-04-02 09:52:54.875 - INFO: Train epoch 362: [20800/94637 (22%)] Step: [2137526] | Lr: 0.000100 | Loss: 0.9060 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 88.02
24-04-02 09:53:28.468 - INFO: Train epoch 362: [22400/94637 (24%)] Step: [2137626] | Lr: 0.000100 | Loss: 1.1475 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 87.05
24-04-02 09:54:03.109 - INFO: Train epoch 362: [24000/94637 (25%)] Step: [2137726] | Lr: 0.000100 | Loss: 1.4601 | MSE loss: 0.0004 | Bpp loss: 0.84 | Aux loss: 85.97
24-04-02 09:54:37.652 - INFO: Train epoch 362: [25600/94637 (27%)] Step: [2137826] | Lr: 0.000100 | Loss: 1.9661 | MSE loss: 0.0005 | Bpp loss: 1.19 | Aux loss: 82.39
24-04-02 09:55:12.574 - INFO: Train epoch 362: [27200/94637 (29%)] Step: [2137926] | Lr: 0.000100 | Loss: 1.2024 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.72
24-04-02 09:55:47.457 - INFO: Train epoch 362: [28800/94637 (30%)] Step: [2138026] | Lr: 0.000100 | Loss: 1.8335 | MSE loss: 0.0005 | Bpp loss: 1.09 | Aux loss: 78.03
24-04-02 09:56:22.821 - INFO: Train epoch 362: [30400/94637 (32%)] Step: [2138126] | Lr: 0.000100 | Loss: 1.3662 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 82.70
24-04-02 09:56:57.635 - INFO: Train epoch 362: [32000/94637 (34%)] Step: [2138226] | Lr: 0.000100 | Loss: 1.5708 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 88.90
24-04-02 09:57:34.310 - INFO: Train epoch 362: [33600/94637 (36%)] Step: [2138326] | Lr: 0.000100 | Loss: 0.8686 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 83.29
24-04-02 09:58:09.135 - INFO: Train epoch 362: [35200/94637 (37%)] Step: [2138426] | Lr: 0.000100 | Loss: 1.3749 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 84.77
24-04-02 09:58:43.071 - INFO: Train epoch 362: [36800/94637 (39%)] Step: [2138526] | Lr: 0.000100 | Loss: 1.0895 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 87.34
24-04-02 09:59:18.348 - INFO: Train epoch 362: [38400/94637 (41%)] Step: [2138626] | Lr: 0.000100 | Loss: 0.8673 | MSE loss: 0.0002 | Bpp loss: 0.53 | Aux loss: 80.14
24-04-02 09:59:53.997 - INFO: Train epoch 362: [40000/94637 (42%)] Step: [2138726] | Lr: 0.000100 | Loss: 1.4495 | MSE loss: 0.0004 | Bpp loss: 0.80 | Aux loss: 80.20
24-04-02 10:00:28.497 - INFO: Train epoch 362: [41600/94637 (44%)] Step: [2138826] | Lr: 0.000100 | Loss: 0.9997 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 84.29
24-04-02 10:01:03.434 - INFO: Train epoch 362: [43200/94637 (46%)] Step: [2138926] | Lr: 0.000100 | Loss: 1.1894 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 91.49
24-04-02 10:01:37.485 - INFO: Train epoch 362: [44800/94637 (47%)] Step: [2139026] | Lr: 0.000100 | Loss: 1.4577 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 90.97
24-04-02 10:02:12.506 - INFO: Train epoch 362: [46400/94637 (49%)] Step: [2139126] | Lr: 0.000100 | Loss: 1.2229 | MSE loss: 0.0003 | Bpp loss: 0.78 | Aux loss: 81.02
24-04-02 10:02:47.386 - INFO: Train epoch 362: [48000/94637 (51%)] Step: [2139226] | Lr: 0.000100 | Loss: 1.1091 | MSE loss: 0.0002 | Bpp loss: 0.72 | Aux loss: 86.12
24-04-02 10:03:22.215 - INFO: Train epoch 362: [49600/94637 (52%)] Step: [2139326] | Lr: 0.000100 | Loss: 1.3324 | MSE loss: 0.0003 | Bpp loss: 0.85 | Aux loss: 79.60
24-04-02 10:03:56.916 - INFO: Train epoch 362: [51200/94637 (54%)] Step: [2139426] | Lr: 0.000100 | Loss: 1.5026 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 73.74
24-04-02 10:04:31.316 - INFO: Train epoch 362: [52800/94637 (56%)] Step: [2139526] | Lr: 0.000100 | Loss: 0.9822 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 76.86
24-04-02 10:05:06.349 - INFO: Train epoch 362: [54400/94637 (57%)] Step: [2139626] | Lr: 0.000100 | Loss: 1.2200 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 80.32
24-04-02 10:05:42.035 - INFO: Train epoch 362: [56000/94637 (59%)] Step: [2139726] | Lr: 0.000100 | Loss: 1.5764 | MSE loss: 0.0004 | Bpp loss: 0.94 | Aux loss: 82.53
24-04-02 10:06:17.252 - INFO: Train epoch 362: [57600/94637 (61%)] Step: [2139826] | Lr: 0.000100 | Loss: 1.2076 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 80.68
24-04-02 10:06:53.345 - INFO: Train epoch 362: [59200/94637 (63%)] Step: [2139926] | Lr: 0.000100 | Loss: 1.5086 | MSE loss: 0.0004 | Bpp loss: 0.92 | Aux loss: 85.62
24-04-02 10:07:30.518 - INFO: Train epoch 362: [60800/94637 (64%)] Step: [2140026] | Lr: 0.000100 | Loss: 1.0746 | MSE loss: 0.0002 | Bpp loss: 0.71 | Aux loss: 83.57
24-04-02 10:08:05.039 - INFO: Train epoch 362: [62400/94637 (66%)] Step: [2140126] | Lr: 0.000100 | Loss: 1.1592 | MSE loss: 0.0003 | Bpp loss: 0.63 | Aux loss: 83.18
24-04-02 10:08:40.071 - INFO: Train epoch 362: [64000/94637 (68%)] Step: [2140226] | Lr: 0.000100 | Loss: 1.3473 | MSE loss: 0.0003 | Bpp loss: 0.87 | Aux loss: 85.50
24-04-02 10:09:15.417 - INFO: Train epoch 362: [65600/94637 (69%)] Step: [2140326] | Lr: 0.000100 | Loss: 1.9318 | MSE loss: 0.0005 | Bpp loss: 1.13 | Aux loss: 82.12
24-04-02 10:09:49.498 - INFO: Train epoch 362: [67200/94637 (71%)] Step: [2140426] | Lr: 0.000100 | Loss: 1.6292 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 80.39
24-04-02 10:10:23.311 - INFO: Train epoch 362: [68800/94637 (73%)] Step: [2140526] | Lr: 0.000100 | Loss: 1.0175 | MSE loss: 0.0002 | Bpp loss: 0.67 | Aux loss: 88.70
24-04-02 10:10:57.724 - INFO: Train epoch 362: [70400/94637 (74%)] Step: [2140626] | Lr: 0.000100 | Loss: 1.4631 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 84.38
24-04-02 10:11:32.082 - INFO: Train epoch 362: [72000/94637 (76%)] Step: [2140726] | Lr: 0.000100 | Loss: 1.6076 | MSE loss: 0.0004 | Bpp loss: 1.01 | Aux loss: 79.68
24-04-02 10:12:06.756 - INFO: Train epoch 362: [73600/94637 (78%)] Step: [2140826] | Lr: 0.000100 | Loss: 1.0065 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 80.12
24-04-02 10:12:41.194 - INFO: Train epoch 362: [75200/94637 (79%)] Step: [2140926] | Lr: 0.000100 | Loss: 0.9705 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 85.83
24-04-02 10:13:15.374 - INFO: Train epoch 362: [76800/94637 (81%)] Step: [2141026] | Lr: 0.000100 | Loss: 0.9755 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 80.94
24-04-02 10:13:49.556 - INFO: Train epoch 362: [78400/94637 (83%)] Step: [2141126] | Lr: 0.000100 | Loss: 1.4099 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 80.81
24-04-02 10:14:23.809 - INFO: Train epoch 362: [80000/94637 (85%)] Step: [2141226] | Lr: 0.000100 | Loss: 1.0911 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 81.26
24-04-02 10:14:58.277 - INFO: Train epoch 362: [81600/94637 (86%)] Step: [2141326] | Lr: 0.000100 | Loss: 0.9972 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 79.48
24-04-02 10:15:33.432 - INFO: Train epoch 362: [83200/94637 (88%)] Step: [2141426] | Lr: 0.000100 | Loss: 1.3109 | MSE loss: 0.0003 | Bpp loss: 0.81 | Aux loss: 87.15
24-04-02 10:16:07.952 - INFO: Train epoch 362: [84800/94637 (90%)] Step: [2141526] | Lr: 0.000100 | Loss: 1.1505 | MSE loss: 0.0002 | Bpp loss: 0.75 | Aux loss: 80.10
24-04-02 10:16:42.344 - INFO: Train epoch 362: [86400/94637 (91%)] Step: [2141626] | Lr: 0.000100 | Loss: 1.4239 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 80.45
24-04-02 10:17:17.936 - INFO: Train epoch 362: [88000/94637 (93%)] Step: [2141726] | Lr: 0.000100 | Loss: 1.6465 | MSE loss: 0.0003 | Bpp loss: 1.09 | Aux loss: 82.70
24-04-02 10:17:52.713 - INFO: Train epoch 362: [89600/94637 (95%)] Step: [2141826] | Lr: 0.000100 | Loss: 1.0250 | MSE loss: 0.0003 | Bpp loss: 0.62 | Aux loss: 87.91
24-04-02 10:18:27.377 - INFO: Train epoch 362: [91200/94637 (96%)] Step: [2141926] | Lr: 0.000100 | Loss: 1.5061 | MSE loss: 0.0004 | Bpp loss: 0.94 | Aux loss: 82.97
24-04-02 10:19:01.118 - INFO: Train epoch 362: [92800/94637 (98%)] Step: [2142026] | Lr: 0.000100 | Loss: 1.2728 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.84
24-04-02 10:19:35.107 - INFO: Train epoch 362: [94400/94637 (100%)] Step: [2142126] | Lr: 0.000100 | Loss: 1.0058 | MSE loss: 0.0002 | Bpp loss: 0.60 | Aux loss: 89.80
24-04-02 10:19:51.558 - INFO: Learning rate: 0.0001
24-04-02 10:19:52.423 - INFO: Train epoch 363: [    0/94637 (0%)] Step: [2142141] | Lr: 0.000100 | Loss: 1.3292 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 83.53
24-04-02 10:20:28.089 - INFO: Train epoch 363: [ 1600/94637 (2%)] Step: [2142241] | Lr: 0.000100 | Loss: 1.3982 | MSE loss: 0.0004 | Bpp loss: 0.80 | Aux loss: 80.48
24-04-02 10:21:03.092 - INFO: Train epoch 363: [ 3200/94637 (3%)] Step: [2142341] | Lr: 0.000100 | Loss: 1.0046 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 80.01
24-04-02 10:21:38.234 - INFO: Train epoch 363: [ 4800/94637 (5%)] Step: [2142441] | Lr: 0.000100 | Loss: 0.6662 | MSE loss: 0.0001 | Bpp loss: 0.45 | Aux loss: 77.14
24-04-02 10:22:14.813 - INFO: Train epoch 363: [ 6400/94637 (7%)] Step: [2142541] | Lr: 0.000100 | Loss: 1.2065 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 81.99
24-04-02 10:22:49.338 - INFO: Train epoch 363: [ 8000/94637 (8%)] Step: [2142641] | Lr: 0.000100 | Loss: 1.1506 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 81.42
24-04-02 10:23:24.118 - INFO: Train epoch 363: [ 9600/94637 (10%)] Step: [2142741] | Lr: 0.000100 | Loss: 1.4535 | MSE loss: 0.0004 | Bpp loss: 0.85 | Aux loss: 80.77
24-04-02 10:23:59.167 - INFO: Train epoch 363: [11200/94637 (12%)] Step: [2142841] | Lr: 0.000100 | Loss: 1.0289 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 87.54
24-04-02 10:24:33.907 - INFO: Train epoch 363: [12800/94637 (14%)] Step: [2142941] | Lr: 0.000100 | Loss: 0.8240 | MSE loss: 0.0002 | Bpp loss: 0.52 | Aux loss: 79.36
24-04-02 10:25:07.373 - INFO: Train epoch 363: [14400/94637 (15%)] Step: [2143041] | Lr: 0.000100 | Loss: 1.4829 | MSE loss: 0.0003 | Bpp loss: 0.93 | Aux loss: 89.64
24-04-02 10:25:42.224 - INFO: Train epoch 363: [16000/94637 (17%)] Step: [2143141] | Lr: 0.000100 | Loss: 0.7696 | MSE loss: 0.0002 | Bpp loss: 0.44 | Aux loss: 82.32
24-04-02 10:26:16.343 - INFO: Train epoch 363: [17600/94637 (19%)] Step: [2143241] | Lr: 0.000100 | Loss: 0.9613 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 80.35
24-04-02 10:26:50.663 - INFO: Train epoch 363: [19200/94637 (20%)] Step: [2143341] | Lr: 0.000100 | Loss: 1.0077 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 79.77
24-04-02 10:27:26.031 - INFO: Train epoch 363: [20800/94637 (22%)] Step: [2143441] | Lr: 0.000100 | Loss: 1.2655 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 79.77
24-04-02 10:28:01.037 - INFO: Train epoch 363: [22400/94637 (24%)] Step: [2143541] | Lr: 0.000100 | Loss: 1.1233 | MSE loss: 0.0003 | Bpp loss: 0.65 | Aux loss: 76.83
24-04-02 10:28:34.822 - INFO: Train epoch 363: [24000/94637 (25%)] Step: [2143641] | Lr: 0.000100 | Loss: 1.7813 | MSE loss: 0.0004 | Bpp loss: 1.08 | Aux loss: 78.46
24-04-02 10:29:08.805 - INFO: Train epoch 363: [25600/94637 (27%)] Step: [2143741] | Lr: 0.000100 | Loss: 0.9198 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 76.52
24-04-02 10:29:43.163 - INFO: Train epoch 363: [27200/94637 (29%)] Step: [2143841] | Lr: 0.000100 | Loss: 0.9198 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 75.68
24-04-02 10:30:17.096 - INFO: Train epoch 363: [28800/94637 (30%)] Step: [2143941] | Lr: 0.000100 | Loss: 0.8771 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 89.82
24-04-02 10:30:52.171 - INFO: Train epoch 363: [30400/94637 (32%)] Step: [2144041] | Lr: 0.000100 | Loss: 0.7656 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 82.50
24-04-02 10:31:26.494 - INFO: Train epoch 363: [32000/94637 (34%)] Step: [2144141] | Lr: 0.000100 | Loss: 0.9955 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 79.06
24-04-02 10:32:00.650 - INFO: Train epoch 363: [33600/94637 (36%)] Step: [2144241] | Lr: 0.000100 | Loss: 0.9880 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 77.74
24-04-02 10:32:35.339 - INFO: Train epoch 363: [35200/94637 (37%)] Step: [2144341] | Lr: 0.000100 | Loss: 1.5166 | MSE loss: 0.0004 | Bpp loss: 0.89 | Aux loss: 81.30
24-04-02 10:33:10.099 - INFO: Train epoch 363: [36800/94637 (39%)] Step: [2144441] | Lr: 0.000100 | Loss: 1.6209 | MSE loss: 0.0004 | Bpp loss: 0.98 | Aux loss: 82.41
24-04-02 10:33:44.941 - INFO: Train epoch 363: [38400/94637 (41%)] Step: [2144541] | Lr: 0.000100 | Loss: 0.8967 | MSE loss: 0.0002 | Bpp loss: 0.56 | Aux loss: 78.72
24-04-02 10:34:19.323 - INFO: Train epoch 363: [40000/94637 (42%)] Step: [2144641] | Lr: 0.000100 | Loss: 1.1917 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 80.43
24-04-02 10:34:54.272 - INFO: Train epoch 363: [41600/94637 (44%)] Step: [2144741] | Lr: 0.000100 | Loss: 1.4908 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 88.43
24-04-02 10:35:28.811 - INFO: Train epoch 363: [43200/94637 (46%)] Step: [2144841] | Lr: 0.000100 | Loss: 1.7847 | MSE loss: 0.0004 | Bpp loss: 1.10 | Aux loss: 84.83
24-04-02 10:36:03.367 - INFO: Train epoch 363: [44800/94637 (47%)] Step: [2144941] | Lr: 0.000100 | Loss: 1.5060 | MSE loss: 0.0004 | Bpp loss: 0.93 | Aux loss: 90.69
24-04-02 10:36:41.352 - INFO: Train epoch 363: [46400/94637 (49%)] Step: [2145041] | Lr: 0.000100 | Loss: 1.3288 | MSE loss: 0.0003 | Bpp loss: 0.84 | Aux loss: 86.36
24-04-02 10:37:16.795 - INFO: Train epoch 363: [48000/94637 (51%)] Step: [2145141] | Lr: 0.000100 | Loss: 1.5170 | MSE loss: 0.0003 | Bpp loss: 0.98 | Aux loss: 81.52
24-04-02 10:37:52.105 - INFO: Train epoch 363: [49600/94637 (52%)] Step: [2145241] | Lr: 0.000100 | Loss: 1.2289 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 85.16
24-04-02 10:38:27.103 - INFO: Train epoch 363: [51200/94637 (54%)] Step: [2145341] | Lr: 0.000100 | Loss: 1.0930 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 83.12
24-04-02 10:39:01.510 - INFO: Train epoch 363: [52800/94637 (56%)] Step: [2145441] | Lr: 0.000100 | Loss: 1.6970 | MSE loss: 0.0004 | Bpp loss: 1.04 | Aux loss: 85.01
24-04-02 10:39:36.327 - INFO: Train epoch 363: [54400/94637 (57%)] Step: [2145541] | Lr: 0.000100 | Loss: 1.4424 | MSE loss: 0.0003 | Bpp loss: 0.90 | Aux loss: 88.49
24-04-02 10:40:12.047 - INFO: Train epoch 363: [56000/94637 (59%)] Step: [2145641] | Lr: 0.000100 | Loss: 0.8011 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 78.37
24-04-02 10:40:47.158 - INFO: Train epoch 363: [57600/94637 (61%)] Step: [2145741] | Lr: 0.000100 | Loss: 1.6219 | MSE loss: 0.0004 | Bpp loss: 0.92 | Aux loss: 78.73
24-04-02 10:41:22.150 - INFO: Train epoch 363: [59200/94637 (63%)] Step: [2145841] | Lr: 0.000100 | Loss: 1.4660 | MSE loss: 0.0003 | Bpp loss: 0.92 | Aux loss: 85.00
24-04-02 10:41:56.790 - INFO: Train epoch 363: [60800/94637 (64%)] Step: [2145941] | Lr: 0.000100 | Loss: 0.9737 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 81.31
24-04-02 10:42:31.671 - INFO: Train epoch 363: [62400/94637 (66%)] Step: [2146041] | Lr: 0.000100 | Loss: 0.8989 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 88.40
24-04-02 10:43:06.232 - INFO: Train epoch 363: [64000/94637 (68%)] Step: [2146141] | Lr: 0.000100 | Loss: 1.4894 | MSE loss: 0.0004 | Bpp loss: 0.88 | Aux loss: 80.74
24-04-02 10:43:40.421 - INFO: Train epoch 363: [65600/94637 (69%)] Step: [2146241] | Lr: 0.000100 | Loss: 1.1111 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 78.16
24-04-02 10:44:15.728 - INFO: Train epoch 363: [67200/94637 (71%)] Step: [2146341] | Lr: 0.000100 | Loss: 1.1847 | MSE loss: 0.0003 | Bpp loss: 0.74 | Aux loss: 78.84
24-04-02 10:44:50.232 - INFO: Train epoch 363: [68800/94637 (73%)] Step: [2146441] | Lr: 0.000100 | Loss: 0.9192 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 76.39
24-04-02 10:45:24.311 - INFO: Train epoch 363: [70400/94637 (74%)] Step: [2146541] | Lr: 0.000100 | Loss: 1.2400 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 73.40
24-04-02 10:45:58.344 - INFO: Train epoch 363: [72000/94637 (76%)] Step: [2146641] | Lr: 0.000100 | Loss: 1.1067 | MSE loss: 0.0002 | Bpp loss: 0.70 | Aux loss: 85.35
24-04-02 10:46:32.626 - INFO: Train epoch 363: [73600/94637 (78%)] Step: [2146741] | Lr: 0.000100 | Loss: 1.6911 | MSE loss: 0.0004 | Bpp loss: 1.09 | Aux loss: 75.20
24-04-02 10:47:06.725 - INFO: Train epoch 363: [75200/94637 (79%)] Step: [2146841] | Lr: 0.000100 | Loss: 0.8647 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 81.41
24-04-02 10:47:41.823 - INFO: Train epoch 363: [76800/94637 (81%)] Step: [2146941] | Lr: 0.000100 | Loss: 1.1561 | MSE loss: 0.0003 | Bpp loss: 0.70 | Aux loss: 77.50
24-04-02 10:48:17.116 - INFO: Train epoch 363: [78400/94637 (83%)] Step: [2147041] | Lr: 0.000100 | Loss: 1.2785 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 73.66
24-04-02 10:48:52.190 - INFO: Train epoch 363: [80000/94637 (85%)] Step: [2147141] | Lr: 0.000100 | Loss: 0.6092 | MSE loss: 0.0001 | Bpp loss: 0.38 | Aux loss: 76.83
24-04-02 10:49:27.186 - INFO: Train epoch 363: [81600/94637 (86%)] Step: [2147241] | Lr: 0.000100 | Loss: 1.4511 | MSE loss: 0.0003 | Bpp loss: 0.89 | Aux loss: 81.45
24-04-02 10:50:01.686 - INFO: Train epoch 363: [83200/94637 (88%)] Step: [2147341] | Lr: 0.000100 | Loss: 1.0083 | MSE loss: 0.0002 | Bpp loss: 0.65 | Aux loss: 80.74
24-04-02 10:50:36.524 - INFO: Train epoch 363: [84800/94637 (90%)] Step: [2147441] | Lr: 0.000100 | Loss: 1.4988 | MSE loss: 0.0003 | Bpp loss: 0.95 | Aux loss: 79.37
24-04-02 10:51:13.977 - INFO: Train epoch 363: [86400/94637 (91%)] Step: [2147541] | Lr: 0.000100 | Loss: 1.5050 | MSE loss: 0.0004 | Bpp loss: 0.92 | Aux loss: 81.20
24-04-02 10:51:48.688 - INFO: Train epoch 363: [88000/94637 (93%)] Step: [2147641] | Lr: 0.000100 | Loss: 1.1927 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 78.80
24-04-02 10:52:23.415 - INFO: Train epoch 363: [89600/94637 (95%)] Step: [2147741] | Lr: 0.000100 | Loss: 1.1485 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 83.11
24-04-02 10:52:58.090 - INFO: Train epoch 363: [91200/94637 (96%)] Step: [2147841] | Lr: 0.000100 | Loss: 1.1306 | MSE loss: 0.0002 | Bpp loss: 0.73 | Aux loss: 82.60
24-04-02 10:53:32.825 - INFO: Train epoch 363: [92800/94637 (98%)] Step: [2147941] | Lr: 0.000100 | Loss: 1.0852 | MSE loss: 0.0002 | Bpp loss: 0.69 | Aux loss: 81.28
24-04-02 10:54:06.989 - INFO: Train epoch 363: [94400/94637 (100%)] Step: [2148041] | Lr: 0.000100 | Loss: 0.7893 | MSE loss: 0.0002 | Bpp loss: 0.49 | Aux loss: 81.53
24-04-02 10:54:23.517 - INFO: Learning rate: 0.0001
24-04-02 10:54:24.293 - INFO: Train epoch 364: [    0/94637 (0%)] Step: [2148056] | Lr: 0.000100 | Loss: 0.9789 | MSE loss: 0.0002 | Bpp loss: 0.61 | Aux loss: 81.75
24-04-02 10:54:58.857 - INFO: Train epoch 364: [ 1600/94637 (2%)] Step: [2148156] | Lr: 0.000100 | Loss: 1.3568 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 81.24
24-04-02 10:55:33.020 - INFO: Train epoch 364: [ 3200/94637 (3%)] Step: [2148256] | Lr: 0.000100 | Loss: 1.0201 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 83.98
24-04-02 10:56:08.104 - INFO: Train epoch 364: [ 4800/94637 (5%)] Step: [2148356] | Lr: 0.000100 | Loss: 2.4438 | MSE loss: 0.0006 | Bpp loss: 1.40 | Aux loss: 77.42
24-04-02 10:56:44.065 - INFO: Train epoch 364: [ 6400/94637 (7%)] Step: [2148456] | Lr: 0.000100 | Loss: 0.8918 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 77.46
24-04-02 10:57:18.763 - INFO: Train epoch 364: [ 8000/94637 (8%)] Step: [2148556] | Lr: 0.000100 | Loss: 0.6759 | MSE loss: 0.0001 | Bpp loss: 0.46 | Aux loss: 80.78
24-04-02 10:57:54.246 - INFO: Train epoch 364: [ 9600/94637 (10%)] Step: [2148656] | Lr: 0.000100 | Loss: 1.7886 | MSE loss: 0.0005 | Bpp loss: 1.06 | Aux loss: 80.65
24-04-02 10:58:29.566 - INFO: Train epoch 364: [11200/94637 (12%)] Step: [2148756] | Lr: 0.000100 | Loss: 0.9548 | MSE loss: 0.0002 | Bpp loss: 0.63 | Aux loss: 74.77
24-04-02 10:59:03.468 - INFO: Train epoch 364: [12800/94637 (14%)] Step: [2148856] | Lr: 0.000100 | Loss: 1.3475 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 88.06
24-04-02 10:59:37.699 - INFO: Train epoch 364: [14400/94637 (15%)] Step: [2148956] | Lr: 0.000100 | Loss: 1.0025 | MSE loss: 0.0002 | Bpp loss: 0.66 | Aux loss: 83.64
24-04-02 11:00:12.792 - INFO: Train epoch 364: [16000/94637 (17%)] Step: [2149056] | Lr: 0.000100 | Loss: 1.3601 | MSE loss: 0.0004 | Bpp loss: 0.77 | Aux loss: 77.64
24-04-02 11:00:47.717 - INFO: Train epoch 364: [17600/94637 (19%)] Step: [2149156] | Lr: 0.000100 | Loss: 1.5596 | MSE loss: 0.0004 | Bpp loss: 0.93 | Aux loss: 79.91
24-04-02 11:01:23.018 - INFO: Train epoch 364: [19200/94637 (20%)] Step: [2149256] | Lr: 0.000100 | Loss: 0.7175 | MSE loss: 0.0002 | Bpp loss: 0.46 | Aux loss: 85.65
24-04-02 11:01:58.075 - INFO: Train epoch 364: [20800/94637 (22%)] Step: [2149356] | Lr: 0.000100 | Loss: 1.6810 | MSE loss: 0.0004 | Bpp loss: 1.05 | Aux loss: 78.99
24-04-02 11:02:32.582 - INFO: Train epoch 364: [22400/94637 (24%)] Step: [2149456] | Lr: 0.000100 | Loss: 1.4886 | MSE loss: 0.0004 | Bpp loss: 0.91 | Aux loss: 86.53
24-04-02 11:03:08.658 - INFO: Train epoch 364: [24000/94637 (25%)] Step: [2149556] | Lr: 0.000100 | Loss: 1.3898 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 82.55
24-04-02 11:03:43.545 - INFO: Train epoch 364: [25600/94637 (27%)] Step: [2149656] | Lr: 0.000100 | Loss: 1.1539 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 84.42
24-04-02 11:04:19.678 - INFO: Train epoch 364: [27200/94637 (29%)] Step: [2149756] | Lr: 0.000100 | Loss: 1.7608 | MSE loss: 0.0004 | Bpp loss: 1.08 | Aux loss: 83.70
24-04-02 11:04:55.258 - INFO: Train epoch 364: [28800/94637 (30%)] Step: [2149856] | Lr: 0.000100 | Loss: 0.9582 | MSE loss: 0.0003 | Bpp loss: 0.55 | Aux loss: 84.01
24-04-02 11:05:30.485 - INFO: Train epoch 364: [30400/94637 (32%)] Step: [2149956] | Lr: 0.000100 | Loss: 0.7534 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 82.51
24-04-02 11:06:08.434 - INFO: Train epoch 364: [32000/94637 (34%)] Step: [2150056] | Lr: 0.000100 | Loss: 0.8246 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 81.78
24-04-02 11:06:43.606 - INFO: Train epoch 364: [33600/94637 (36%)] Step: [2150156] | Lr: 0.000100 | Loss: 1.1971 | MSE loss: 0.0002 | Bpp loss: 0.80 | Aux loss: 85.28
24-04-02 11:07:19.880 - INFO: Train epoch 364: [35200/94637 (37%)] Step: [2150256] | Lr: 0.000100 | Loss: 1.1535 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 81.22
24-04-02 11:07:54.812 - INFO: Train epoch 364: [36800/94637 (39%)] Step: [2150356] | Lr: 0.000100 | Loss: 1.3071 | MSE loss: 0.0003 | Bpp loss: 0.83 | Aux loss: 86.18
24-04-02 11:08:30.675 - INFO: Train epoch 364: [38400/94637 (41%)] Step: [2150456] | Lr: 0.000100 | Loss: 0.9246 | MSE loss: 0.0002 | Bpp loss: 0.58 | Aux loss: 80.09
24-04-02 11:09:06.210 - INFO: Train epoch 364: [40000/94637 (42%)] Step: [2150556] | Lr: 0.000100 | Loss: 1.2255 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 86.97
24-04-02 11:09:42.199 - INFO: Train epoch 364: [41600/94637 (44%)] Step: [2150656] | Lr: 0.000100 | Loss: 0.9245 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 78.09
24-04-02 11:10:17.310 - INFO: Train epoch 364: [43200/94637 (46%)] Step: [2150756] | Lr: 0.000100 | Loss: 1.1822 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 83.24
24-04-02 11:10:52.910 - INFO: Train epoch 364: [44800/94637 (47%)] Step: [2150856] | Lr: 0.000100 | Loss: 1.1483 | MSE loss: 0.0003 | Bpp loss: 0.71 | Aux loss: 82.82
24-04-02 11:11:28.449 - INFO: Train epoch 364: [46400/94637 (49%)] Step: [2150956] | Lr: 0.000100 | Loss: 1.1184 | MSE loss: 0.0003 | Bpp loss: 0.67 | Aux loss: 82.48
24-04-02 11:12:03.937 - INFO: Train epoch 364: [48000/94637 (51%)] Step: [2151056] | Lr: 0.000100 | Loss: 1.8201 | MSE loss: 0.0005 | Bpp loss: 0.99 | Aux loss: 82.34
24-04-02 11:12:39.684 - INFO: Train epoch 364: [49600/94637 (52%)] Step: [2151156] | Lr: 0.000100 | Loss: 1.2404 | MSE loss: 0.0003 | Bpp loss: 0.77 | Aux loss: 81.56
24-04-02 11:13:12.900 - INFO: Train epoch 364: [51200/94637 (54%)] Step: [2151256] | Lr: 0.000100 | Loss: 1.5949 | MSE loss: 0.0004 | Bpp loss: 0.96 | Aux loss: 86.04
24-04-02 11:13:47.561 - INFO: Train epoch 364: [52800/94637 (56%)] Step: [2151356] | Lr: 0.000100 | Loss: 0.7494 | MSE loss: 0.0002 | Bpp loss: 0.50 | Aux loss: 88.05
24-04-02 11:14:21.326 - INFO: Train epoch 364: [54400/94637 (57%)] Step: [2151456] | Lr: 0.000100 | Loss: 0.7742 | MSE loss: 0.0002 | Bpp loss: 0.48 | Aux loss: 89.33
24-04-02 11:14:55.268 - INFO: Train epoch 364: [56000/94637 (59%)] Step: [2151556] | Lr: 0.000100 | Loss: 0.9296 | MSE loss: 0.0002 | Bpp loss: 0.54 | Aux loss: 80.27
24-04-02 11:15:30.766 - INFO: Train epoch 364: [57600/94637 (61%)] Step: [2151656] | Lr: 0.000100 | Loss: 1.9359 | MSE loss: 0.0006 | Bpp loss: 0.96 | Aux loss: 85.59
24-04-02 11:16:05.263 - INFO: Train epoch 364: [59200/94637 (63%)] Step: [2151756] | Lr: 0.000100 | Loss: 1.2021 | MSE loss: 0.0003 | Bpp loss: 0.73 | Aux loss: 85.75
24-04-02 11:16:39.910 - INFO: Train epoch 364: [60800/94637 (64%)] Step: [2151856] | Lr: 0.000100 | Loss: 0.8813 | MSE loss: 0.0002 | Bpp loss: 0.57 | Aux loss: 83.58
24-04-02 11:17:14.239 - INFO: Train epoch 364: [62400/94637 (66%)] Step: [2151956] | Lr: 0.000100 | Loss: 0.6439 | MSE loss: 0.0001 | Bpp loss: 0.42 | Aux loss: 89.07
24-04-02 11:17:48.549 - INFO: Train epoch 364: [64000/94637 (68%)] Step: [2152056] | Lr: 0.000100 | Loss: 0.9888 | MSE loss: 0.0002 | Bpp loss: 0.62 | Aux loss: 83.74
24-04-02 11:18:22.608 - INFO: Train epoch 364: [65600/94637 (69%)] Step: [2152156] | Lr: 0.000100 | Loss: 1.2788 | MSE loss: 0.0003 | Bpp loss: 0.82 | Aux loss: 85.55
24-04-02 11:18:57.701 - INFO: Train epoch 364: [67200/94637 (71%)] Step: [2152256] | Lr: 0.000100 | Loss: 1.1391 | MSE loss: 0.0003 | Bpp loss: 0.66 | Aux loss: 85.58
24-04-02 11:19:32.610 - INFO: Train epoch 364: [68800/94637 (73%)] Step: [2152356] | Lr: 0.000100 | Loss: 1.1105 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 78.34
24-04-02 11:20:07.087 - INFO: Train epoch 364: [70400/94637 (74%)] Step: [2152456] | Lr: 0.000100 | Loss: 1.1668 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 77.19
24-04-02 11:20:42.747 - INFO: Train epoch 364: [72000/94637 (76%)] Step: [2152556] | Lr: 0.000100 | Loss: 1.1849 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 81.58
24-04-02 11:21:16.055 - INFO: Train epoch 364: [73600/94637 (78%)] Step: [2152656] | Lr: 0.000100 | Loss: 0.9854 | MSE loss: 0.0002 | Bpp loss: 0.64 | Aux loss: 80.19
24-04-02 11:21:51.102 - INFO: Train epoch 364: [75200/94637 (79%)] Step: [2152756] | Lr: 0.000100 | Loss: 1.1293 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 80.48
24-04-02 11:22:25.266 - INFO: Train epoch 364: [76800/94637 (81%)] Step: [2152856] | Lr: 0.000100 | Loss: 1.4856 | MSE loss: 0.0003 | Bpp loss: 0.94 | Aux loss: 75.99
24-04-02 11:22:59.210 - INFO: Train epoch 364: [78400/94637 (83%)] Step: [2152956] | Lr: 0.000100 | Loss: 1.4055 | MSE loss: 0.0004 | Bpp loss: 0.80 | Aux loss: 79.50
24-04-02 11:23:33.916 - INFO: Train epoch 364: [80000/94637 (85%)] Step: [2153056] | Lr: 0.000100 | Loss: 1.2803 | MSE loss: 0.0003 | Bpp loss: 0.75 | Aux loss: 79.11
24-04-02 11:24:08.981 - INFO: Train epoch 364: [81600/94637 (86%)] Step: [2153156] | Lr: 0.000100 | Loss: 1.0687 | MSE loss: 0.0003 | Bpp loss: 0.60 | Aux loss: 84.28
24-04-02 11:24:45.008 - INFO: Train epoch 364: [83200/94637 (88%)] Step: [2153256] | Lr: 0.000100 | Loss: 1.0058 | MSE loss: 0.0002 | Bpp loss: 0.67 | Aux loss: 82.21
24-04-02 11:25:19.623 - INFO: Train epoch 364: [84800/94637 (90%)] Step: [2153356] | Lr: 0.000100 | Loss: 1.1394 | MSE loss: 0.0003 | Bpp loss: 0.69 | Aux loss: 85.97
24-04-02 11:25:53.994 - INFO: Train epoch 364: [86400/94637 (91%)] Step: [2153456] | Lr: 0.000100 | Loss: 1.5593 | MSE loss: 0.0004 | Bpp loss: 0.97 | Aux loss: 84.41
24-04-02 11:26:28.883 - INFO: Train epoch 364: [88000/94637 (93%)] Step: [2153556] | Lr: 0.000100 | Loss: 1.3969 | MSE loss: 0.0003 | Bpp loss: 0.88 | Aux loss: 81.38
24-04-02 11:27:03.976 - INFO: Train epoch 364: [89600/94637 (95%)] Step: [2153656] | Lr: 0.000100 | Loss: 1.5879 | MSE loss: 0.0004 | Bpp loss: 1.00 | Aux loss: 86.43
24-04-02 11:27:39.333 - INFO: Train epoch 364: [91200/94637 (96%)] Step: [2153756] | Lr: 0.000100 | Loss: 1.0509 | MSE loss: 0.0002 | Bpp loss: 0.68 | Aux loss: 81.53
24-04-02 11:28:14.310 - INFO: Train epoch 364: [92800/94637 (98%)] Step: [2153856] | Lr: 0.000100 | Loss: 1.2179 | MSE loss: 0.0003 | Bpp loss: 0.72 | Aux loss: 78.50
24-04-02 11:28:49.884 - INFO: Train epoch 364: [94400/94637 (100%)] Step: [2153956] | Lr: 0.000100 | Loss: 0.8852 | MSE loss: 0.0002 | Bpp loss: 0.55 | Aux loss: 86.03
24-04-02 11:29:06.742 - INFO: Learning rate: 0.0001
24-04-02 11:29:07.634 - INFO: Train epoch 365: [    0/94637 (0%)] Step: [2153971] | Lr: 0.000100 | Loss: 1.5875 | MSE loss: 0.0004 | Bpp loss: 0.90 | Aux loss: 82.86
24-04-02 11:29:43.245 - INFO: Train epoch 365: [ 1600/94637 (2%)] Step: [2154071] | Lr: 0.000100 | Loss: 0.9160 | MSE loss: 0.0002 | Bpp loss: 0.59 | Aux loss: 81.68
24-04-02 11:30:18.406 - INFO: Train epoch 365: [ 3200/94637 (3%)] Step: [2154171] | Lr: 0.000100 | Loss: 1.2927 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 84.38
24-04-02 11:30:52.323 - INFO: Train epoch 365: [ 4800/94637 (5%)] Step: [2154271] | Lr: 0.000100 | Loss: 1.3040 | MSE loss: 0.0003 | Bpp loss: 0.76 | Aux loss: 84.29
24-04-02 11:31:27.530 - INFO: Train epoch 365: [ 6400/94637 (7%)] Step: [2154371] | Lr: 0.000100 | Loss: 1.2342 | MSE loss: 0.0003 | Bpp loss: 0.79 | Aux loss: 79.70
24-04-02 11:32:02.834 - INFO: Train epoch 365: [ 8000/94637 (8%)] Step: [2154471] | Lr: 0.000100 | Loss: 1.4198 | MSE loss: 0.0003 | Bpp loss: 0.91 | Aux loss: 79.93
